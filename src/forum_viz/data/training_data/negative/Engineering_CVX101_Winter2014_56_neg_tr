0:<I wasn't even able to download it. From the download page it takes me to the product presentation, and I have no idea what to do next. I feel very stupid and discouraged :(>
1:<Just noticed that the solutions require an ID :(>
2:<It's possible for $A$ to have rank $n$ and for $Ax=b$ to be an inconsistent system of equations if $A$ has $m$ rows and $m>n$.  $x=(A^{T}A)^{-1}A^{T}b$ does give the unique least squares solution in this case.>
3:<Re Question 1: note that it says the sum of each of the components of a 3-dimensional point, and that is a real number. So there's no typo.

Re Question 2: I'm still trying to figure out what it means and how to solve it :-(>
4:<Let us take the case of more than one line. consider points $x1=(0,0)$,$x2=(1,0)$ and $x3=(0,1)$. Lines through the above points forms triangle. Now consider the combinations according to $x=(1-	heta)x2+	heta x3$, when $	heta=0.5$, the resultant point will be $x=(0.5, 0.5)$. 
The question is whether $x$ is affine set or convex set or both.
Is it the inequality in $Ax leq b$ that differentiates  affine and convex sets.>
5:<We are told the following:

We define $(x)_+=max{0,x} 	ext{and } (x)_?=max{0,?x}, 	ext{so } x=(x)_+?(x)_?$.

My question is what if $x in R^n$? Are we supposed to interpret the definition of $(x)_-$ and $(x)_+$ as applying componentwise? That is to say, let $x = [-1, 5]$. Then, $(x)_- = [1, 0]$ and $(x)_+ = [0, 5]$

From the fact that $x=(x)_+?(x)_?$, I think it should be defined componentwise for vectors but I wanted to give a heads up and check with my colleagues and classmates just in case.

Update: I think they use bold max, i.e. $mathbf{max}$, which indicates a componentwise vector operation.>
6:<You should try to rewrite $1^T(x)_-leq(1/2)1^T(x)_+$ as $1^T(x)_--(1/2)1^T(x)_+leq0$. Split the term $1^T(x)_-$ and remember that $x=(x)_+-(x)_-$. The goal is to find the equation defines a convex set such as a hyperplane, halfspace, cone, etc.>
7:<Hi all,

In 3.2.1 subsection-"Operations that preserve convexity", to verify that convexity is preserved under nonnegative scaling and addition there is the following equation:

epi(wf) =[ I 0 ;0 w ] epi(f)

It should be straightforward to understand. But I can not see it. Could anyone please explain why this equation holds? I will appreciate a lot your kind help!

Specifically, what I already know is the definition of epigraph. But It doesn't make sense to me the product of the matrix [I 0 ;0 w ] and epi(f).

Best,
<nameRedac_anon_screen_name_redacted>>
8:<Convex Functions -> Basic problems and examples -> Problem 2
QUESTION 2  (1/1 points)
We define (x)+=max{0,x} and (x)?=max{0,?x}, so x=(x)+?(x)?.

Is {x?Rn?1T(x)??(1/2)1T(x)+} a convex set?>
9:<Hi all,

The exercise 3.3 on the text book is about finding out the convexity/concavity of the inverse function of a increasing and convex function f.  

>Suppose f : R ? R is increasing and convex
on its domain (a, b). Let g denote its inverse, i.e., the function with domain (f (a), f (b))
and g(f (x)) = x for a < x < b. What can you say about convexity or concavity of g?"

It is not hard to figure out g is concave. But I can not understand the epigraph/hypograph version of illustration of this problem. The solution is as follows:

>hypo(g)={(y,t)|t<=g(y)}
>>={(y,t)|f(t)<=f(g(y))}

>>={(y,t)|f(t)<=y}

>>=[0 1; 1 0]epi(f)

why the last two steps happen? I can not understand. I will appreciate a lot any help from you! thanks!>
10:<Hi yshklarov,

Your comment helps a lot! I was confused by the direction or shape of the epigraph. If dom(f)=[a,b], then it should be unable to do the last step. But since here dom(f)=R, it make great sense to do the last step. 

Thanks for your help again!>
11:<Any idea how the grading is being done ? 
I suppose the answers are not unique..
I get different yet comparable results every time I run the code .. and the answers are pretty close to the suggested answer .. It is sad to see all Xs :(

Please staff. Have a look at this ! Thanks a lot !>
12:<[Edit: sorry, I missed the other thread on this topic [here][1]]

Hi,

Is anybody else having trouble with the last part of HW3? The earlier sections went fine but here on my first attempt I had no questions correct :(

I implemented the classical portfolio optimization problem in CVX, in the same way as it is described on page 155 of the book, but without the constraint on short positions (except for the third part of the problem). As answer I take the risk term, which I understand to be x' * S * x, where x is the solution variable. The risk values I find are typically very small (order 1<zipRedac>*-3, 1<zipRedac>*-4).

As one of the constraints I have that pbar' * x == pbar' * x_unif (since the assignment says "Find minimum-risk portfolios with the same expected return as the uniform portfolio"), but when I change it to an inequality (accept solutions at least as good as the uniform portfolio) this does not really seem to affect my outcome.

Any suggestions as to what I might be doing wrong?

Thanks!
<nameRedac_anon_screen_name_redacted>


  [1]: https://class.stanford.edu/courses/Engineering/CVX1<zipRedac>1/Winter2<zipRedac>14/discussion/forum/i4x-Engineering-CVX1<zipRedac>1-hw3/threads/52f<zipRedac><zipRedac>733c91dff<zipRedac><phoneRedac>78 "here">
13:<Ah of course! I feel silly to not think about that option. 

Anyway, I implemented it and my upper bound decreased with 0.0024. 
I guess it is still so close enough that both are recognized as "correct" by the system. 
My new lower bound is positive, but still not correct :(>
14:<Suppose $x=(x_1,x_2)$, then $xin[0,1]^2$ means $x_iin[0,1],~~i=1,2$.>
15:<This is a [cartesian product][1] of $[0,1]$ with itself.

$[0,1]^2=[0,1]	imes[0,1]$.


  [1]: https://en.wikipedia.org/wiki/Cartesian_product>
16:<I hope This post saves some of you from making the same (stupid) mistake as me. If you have an older version of matlab you need to use rand('state',5) to generate the correct pbar and **randn**('state',5) for S!

Unfortunately I found out only after getting it wrong twice :(

better luck to you!>
17:<No, I'm not a TA for this class.

My original background is in computer science and math. 

Two years ago I've finished an apprenticeship in software engineering in Germany. I've studied some math as an auditing student when I was in highschool and during my apprenticeship, too. But I've never formally studied at a university because I don't have my a-levels yet. My current plan is to start my a-levels via evening school this summer. Though I don't really like this plan because it's 3 more years to go until I can finally go to university... :-(

I agree, this isn't the typical engineering course even though it runs under the name "EE364A". It's very mathy. I'd consider it more an applied math course. 

I take functional analysis on Coursera parallel to this and it's a huge help, there is a lot of transfer knowledge. I haven't studied convex analysis/optimization much before... but studying it parallel to functional analysis is just fun. I encounter questions that could have been asked in either class.>
18:<Michele, please correct me if I've misunderstood you, but I'm not sure about that interpretation of a norm. Since for norms other than $ell_2$ balls are not round, I'm assuming you mean "norm ball", but since a norm ball with centre at $0$ is defined as ${p mid |p | le r}$, the smallest that contains $x$ is simply the one with $r = |x|$, so your interpretation is circular.

What I've gleaned from the definition of norms is that their level sets (when graphed) are concentric, convex, similar figures that are symmetric about the origin.

TCubed, sketch one level curve of the $ell_infty$ norm in two dimensions: it is a square. Sketch one level curve of the $min$ function: it is not even a closed curve. The $ell_1$ norm also has squares as level curves, and $ell_p$ with $p in (1, infty)$ looks like something in between those two squares. So I think the only reason $max(|x_1|,ldots)$ is a norm is that it's the limit of $left(sum_i x_i^pight)^{1/p}$ as $p ightarrow infty$. It was an interesting idea you had, but I'm not sure if there's any way to salvage it. :(>
19:<Same here. Just ran the Matlab file I used initially. :(>
20:<Sure, I don't claim prizes for well-commented code, but here's a script that gave points. (EDIT: sorry, copied wrong assignment code ;)

    clear all
    clc
    
    m = 200; %outcomes
    n = 4; % existing options
    r = 1.05; % return on safe option
    F=0.9; % floor F and ceiling C on collar
    C=1.15;
    
    % prices for safe, underlying and then  existing options
    p = [1, 1, 0.06, 0.03, .02, .01]'; 
    
    K = [1.1, 1.2, .8, .7]; % existing strike prices
    
    S = linspace(1/2, 2, m); %possible outcomes
    
    V = zeros(m, n+2+1);
    for k=1:m
       V(k, 1) = r; % safe option
       V(k, 2) = S(k); % underlying asset
       for l=1:n % existing options
           if K(l)>1 % call
                V(k, 2+l) = max([0, S(k)-K(l)]);
           else %put
               V(k, 2+l) = max([0, K(l)-S(k)]);
           end
           
       end
       V(k, 2+n+1) = min([max([S(k), F]), C])-1; % collar
    end
    
    Vt = V';
    Vt1=Vt(1:(end-1),:); % split V into known part and collar part
    size(Vt1)
    Vtlast=Vt(end,:);
    size(Vtlast)
    cvx_begin 
        variable collarP
        variable y(m)
        minimize collarP
        subject to
            Vt1 * y == p
            Vtlast*y == collarP
            y >= 0
    cvx_end
    minprice = cvx_optval
    
    cvx_begin quiet
        variable collarP
        variable y(m)
        maximize collarP
        subject to
            Vt1 * y == p
            Vtlast*y == collarP
            y >= 0
    cvx_end
    maxprice = cvx_optval
    
    range = [minprice, maxprice]>
21:<Below gives the upper bound. Replace maximize with minimize to get the lower bound. 

    N = 200;
    S = linspace(0.5,2,N);
    S = S.';  % stock price  % 200-by-1
    S0 = 1;  % price of underlying stock
    r = 1;  % price of risk free asset
    F = 0.9; 
    C = 1.15;
    
    % set up the V matrix for known asset prices
    % [risk-free, stock, call-1,call-2,put1-,put-2]
    V1 =[1.05*ones(N,1), S, max(0,S-1.1),max(0,S-1.2),max(0,0.8-S),max(0,0.7-S)];
    % prices of known assets
    p1 = [1;S0;0.06;0.03;0.02;0.01];
    % we have the equality: V1^T* y = p1 , y>=0
    % unknown collar
    Collar = (C-S0)*(S > C) + (S-S0).*(S <= C).*(S >= F) + (F-S0)*(S <= F);
    
    cvx_begin
        variables y(N) pc
        maximize pc
        subject to
                V1'*y == p1
                Collar'*y == pc
                y >= 0
    cvx_end>
22:<I hope someone from the course staff or knowledgable in statistics comments on this question!

@lolwut12<zipRedac> Thanks very much for the question - I'm not alone in not understanding the equivalence of the 2 problems. This point is almost always glossed over in statistics papers :(. 

@JonCohen I think your intuition, informed by practice, is in the right direction. See this math.stackexchange question and response: http://math.stackexchange.com/questions/<zipRedac><zipRedac>5<zipRedac>06/why-are-additional-constraint-and-penalty-term-equivalent-in-ridge-regression

Intuitively it seems to me, that since the penalty form of regularized regression ('ridge regression' if the norms are Euclidean) always have a unique solution (call it $x^*$), we are guaranteed to have $||x^*||_2 le t$ (where t > 0 is uniquely determined by the penalty $lambda$). It may even be possible to write expressions for the relation between $lambda$ and $inf_lambda t$. For ridge regression it's certainly doable.>
23:<Hi, 
This course is awesome. It gives insights to applications, I never looked at that way.

Nevertheless, I could not get the topic "Experiment Design" quite well.

 1. How did we get the covariance matrix $E=(sum_{i=1}^{m}a_ia_i^T)^{-1}$?

 2. I cant understand the interpretation described with respect to the ellipsoid :"**optimal experiment uses vectors vk on boundary of ellipsoid defined by W**" and how is that shown in the figure below?

  ![List item][1]

Thanks !


  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>73.png>
24:<Just a few days back I answered a question on concavity of log likelihood of  Generalized Linear Models(GLM). Thought I should share it, if someone would like it.
So here it goes.

The definition of exponential family is:

$ p(x|	heta) = h(x)exp(	heta^Tphi(x) - A(	heta))$ where $ A(	heta)$ is the log partition function. 

Now one can prove that three things for 1D case and it actually generalizes to higher dimensions:(You can lookup into properties of exponential families or log partition)

1. $ frac{dA}{d	heta} = mathbb{E}[phi(x)]$

2. $ frac{d^2A}{d	heta^2} = mathbb{E}[phi^2(x)] -mathbb{E}[phi(x)]^2 = var(phi(x)) $

3. $ frac{ partial ^2A}{partial	heta_ipartial	heta_j} = mathbb{E}[phi_i(x)phi_j(x)] -mathbb{E}[phi_i(x)] mathbb{E}[phi_j(x)] = cov(phi(x)) Rightarrow Delta^2A(	heta) = cov(phi(x))$

The above result prove that $A(	heta)$ is convex(as $cov(phi(x))$ is positive semidefinite).

Now we take a look at likelihood function for MLE:

$p(mathcal{D}|	heta) = [prod_{i=1}^{N}{h(x_i)}]exp(	heta^T[sum_{i=1}^{N}phi(x_i)] - NA(	heta))$

$ log (p(mathcal{D}|	heta)) = 	heta^T[sum_{i=1}^{N}phi(x_i)] - NA(	heta)) =  	heta^T[phi(mathcal{D})] - NA(	heta))$

Now $	heta^T[phi(mathcal{D})]$ is linear in theta and $-A(	heta)$ is concave. Therefore, a global maximum(or atleast whatever we get, we cannot do better than that).

Ref: http://stats.stackexchange.com/questions/<zipRedac>7615/does-log-likelihood-in-glm-have-guaranteed-convergence-to-global-maxima/<zipRedac>7<zipRedac><zipRedac>4#<zipRedac>7<zipRedac><zipRedac>4>
25:<Check your code with the following case.

    A = [0 1; 0 -1;1 0; -1 0;-1 1];
    m = 5;
    n = 2;
    b = [1 1 1 1 0];

The hyperspace above is just the intersection of the rectangle $[-1, 1] 	imes [-1, 1]$ with $y-xleq 0$, so you should be getting 1 as an answer, with $l = [0,-1]$ and $u=[1,0]$. (Check me - I might be incorrect.)

My solver does handle this case correctly. But for the original question, I'm still getting a really large value.>
26:<I don't see why this should not work:

    cvx_begin
        variable x(N);
        minimize(...)
        subject to
        x >= 0;
        x(1:(N-1)) <= x(2:N);
    cvx_end>
27:<The way I solved this problem in HW6, problem 1 is by defining a CVX expression inside the `cvx_begin` ... `cvx_end` delimiters:

    cvx_begin
        variable x(N);
        expression t(N+k-1);
        t=conv(x,h);
        minimize(norm(y - t(1:N)))
        subject to
        x >= 0;
        x(1:(N-1)) <= x(2:N);
    cvx_end>
28:<N = 100;
    % create the matrix for inequality
    B = -eye(N-1);
    C = zeros(N,N);
    C(2:N,1:(N-1)) = B;
    A = eye(N) + C;
    % now A*x >=0 gives us 0 <= x(1) <= x(2) <= ... <= x(N)>
29:<## First question

Let $A$ be $:m	imes n$ matrix of rows $a_i$ for simplicity.

So, as a least-squares estimate, we get $;hat x=(A^TA)^{-1}A^Ty$.

Then we can express $e=hat x-x$:
$e = hat x-x = (A^TA)^{-1}A^Ty-x = (A^TA)^{-1}A^T(Ax+w)-x =\
= (A^TA)^{-1}A^TAx+(A^TA)^{-1}A^Tw-x = (A^TA)^{-1}A^Tw$

$mathbb Ee=(A^TA)^{-1}A^Tmathbb Ew=0:$ and therefore the covariance matrix $;E=mathrm{var}(e):$ is basically $;mathbb E[ee^T]$.

$E = mathbb E[ee^T] = mathbb E[(A^TA)^{-1}A^Tww^TA(A^TA)^{-1}] =\
= (A^TA)^{-1}A^Tcdotmathbb E[ww^T]cdot A(A^TA)^{-1} = (A^TA)^{-1}$

Notes:

1. $A^TA;$ is symmetric and hence $;(A^TA)^{-1};$ is symmetric.
2. $mathbb E[ww^T]=I;$ since it is $;mathrm{var}(w);$ and all the measurement errors $w_i$ are IIDs and standard-normal (this is the initial assumption).
3. Well, $A^TA=sumlimits_{i=1}^m(a_ia_i^T)$. You can check it zillions of ways.

## Second question

Comlementary slackness condition for the dual of the D-optimal design problem (slide 7-13 of the lecture) shows that $:lambda_k(1-v_k^TWv_k)=0$. It means that if $lambda_k>0$ (the measurement is selected) then $v_k$ belongs to a boundary of the ellipsoid.

In fact if $:v_k^TWv_k<1:$ the point $v_k$ is inside, and if $;v_k^TWv_k>1:$ it is outside the ellipsoid. For example, if $;W=Iinmathbb R^{2	imes2}$, then the boundary is  ${(x,y)inmathbb R^2,|,x^2+y^2=1}$ and is a plain circle. In general, that equation for a positive definite matrix forms an ellipsoid.

On the next slide you see the picture which illustrates that idea. Dashed line is the ellipsoid $;v^TWv=1$, and only two points are chosen with lambdas 0.5, and those are exactly ones that lie on that ellipsoid's boundary. You get the idea.

Another question you could ask is why that problem is the dual of the original one of D-optimal design and why the complementary slackness holds, but that's another story. I hope you can find a more detailed explanation in the book.>
30:<I believe it's before 10pm, March 2 (PST) now but it looks like the site no longer accepts submissions. :-(

If the instructor meant Sunday midnight, shouldn't the deadline be 0:00, March 3?>
31:<Same here :-(>
32:<Approximation & Fitting  also.....
same here :-(>
33:<Same here :-(>
34:<same here!! :(>
35:<me too :( too bad that this is for 2 due dates....>
36:<My progress dropped 5 percent as well. And I cannot submit the home work on statistical estimation now:(>
37:<I could not see the HW deadlines in the Courseware section. I was still doing HW5 Q3 which was worth 60 points and finally, I got the answers, but I could not submit the HW. After seeing these posts, I feel HW6 was also due today :(. The deadlines are not mentioned in the Courseware section.

Please help!>
38:<we'll see, sometimes they might take a reasonable decision, becoz we all faced the same problem :-(>
39:<I got the answer without the correlation constraint but when i include the following constraint sum(sum(p.*(r1*r2')))==(-0.25*sig1*sig2) i am ending up status failed. Any help>
40:<No help yet so I just plod along. There must be an easy way to see this but I can't. If I try to solve this for the general quadratic norm $left | x ight |_P$ it looks like:

$Delta_{nsd}=argminleft{
abla f(x)^Tvmid (v^TPv)^frac{1}{2}=1ight}$

so to just pound through it:

minimize $
abla f(x)^Tv$ subject to $(v^TPv)^frac{1}{2}leq1$

$L(v,lambda)=
abla f(x)^Tv+lambdaleft [(v^TPv)^frac{1}{2}-1ight ]$

$frac{partial L}{partial v}=
abla f(x)+lambda (v^TPv)^{-frac{1}{2}}Pv=0$

$frac{partial L}{partial lambda}=(v^TPv)^frac{1}{2}-1=0$

solving for $v$ gives

$v=P^{-1}
abla f(x)(
abla f(x)^TP^{-1}
abla f(x))^{-frac{1}{2}}$

Then substituting $P=
abla^2 f(x)$

$v=alpha(
abla^2 f(x))^{-1}
abla f(x)$

$alpha=(
abla f(x)^T(
abla^2 f(x))^{-1}
abla f(x))^{-frac{1}{2}}$

Surely there is an easier/clearer way to do this (and I seem to have lost track of a minus sign somewhere along the way...)???>
41:<Another piece that took some figuring for me is that the preferences array is a list of indexes which people like column 1 more than column 2, look at the data generation code.

    % %coherent preferences
    % for i=1:numpref*.9
    %     ranking1=map_object_to_ranking(objects(i,1));
    %     ranking2=map_object_to_ranking(objects(i,2));
    %     if ranking1 < ranking2
    %         preferences(i,:)=[objects(i,2) objects(i,1)];
    %     elseif ranking1 > ranking2
    %         preferences(i,:)=[objects(i,1) objects(i,2)];
    %     end
    % end>
42:<Hi,

Here's my code:

    %% Minimax rational fit to the exponential
    k = 201;
    ti = linspace(-3,3,k);
    X = [ti.^2; ti; ones(1,k)]';
    yi = exp(ti).';
    
    lb = -1100; ub = 1000; % Arbitrary lower/upper bound
    err = 1e-3;  % Error tolerance
    
    % At most log2((u-b)/err)) iterations
    acoeff = zeros(3,1);
    bcoeff = zeros(2,1);
    iter = 1;
    
    % Using bisection for quasiconvex optimization
    disp(' ');
    disp('***** Minimax Rational Fit to the Exponential: *****');
    fprintf(1,'Computing feasibility...');
    while ub - lb > err
        g = (lb+ub)/2;
        C0 = [X bsxfun(@times, g+yi,-X)];
        C1 = [X bsxfun(@times, g-yi, X)];
        
        fprintf('
Iter = %d:
',iter);
        % Feasibility problem
        cvx_begin quiet
            variables a(3) b(2)
        	minimize( 0 )
        	subject to
                C0*[a;b;1] <= 0
                C1*[a;b;1] >= 0
                X*[b;1] >= eps
        cvx_end
        % If feasible
         if strcmpi(cvx_status,'Solved')
             ub = g;
             acoeff = a;
             bcoeff = b;
          else
             lb = g;
         end
         iter = iter + 1;
         fprintf('	[lb,ub]=[%f,%f]',lb,ub);
    end
    disp(' ');
    disp('Done!');
    disp(' ');
    disp('Optimal coefficients: ')
    fprintf('[a2,a1,a0] = [%f,%f,%f]
',acoeff(1),acoeff(2),acoeff(3));
    fprintf('[b2,b1] = [%f,%f]
',bcoeff(1),bcoeff(2));
    fprintf('Optimal objective value: %f
',...
        max(abs( (X*acoeff)./(X*[bcoeff;1]) - yi )));>
43:<I was also under the assumption that usually the deadline would be Sunday midnight, missed all problems except one :(>
44:<@jmberg But on the course info it says : […] As always, homework is due at midnight PST on Sunday nights.
And the posted deadlines on the hw problems were a little bit confusing, so i interpreted this as 'always Saturday midnight PST'. But you are right, March 9 00:00 PST has indeed past.  :-(>
45:<Same here! I was ready to do the homework today and I just noticed that the deadline was yesterday, not today... :(>
46:<I just wanted to share this geometric interpretation of L1 norm minimization by a scalar value (referring to hw problem on approximation and fitting). I hope it may be helpful to some of the fellow folks.

Here we want to minimize ||**b**-**1**y|| where b is a vector of n dimension, y is scalar. Lets sort **b**'s elements.
Now we can assume that

     bi = f(i); for i: 1 to n

here f is a non decreasing function. Lets assume all bi >= 0. Then we can relate ||b||1 norm with the area under curve of function f. Now if we subtract y = f(k) where k >= 1 and k < n/2, the norm ||**b**-1y|| would be same as the area under f and the line y = f(k). Please refer to the figure below. There the black curve represents the function f and horizontal lines are y = f(k). The gray region represents the area under curve and the blue line. If we observe the figure the area decreases if we drag the blue line upwards (similarly the cyan line downwards). Clearly both blue and cyan line has to reach mid of the [1-n] range to obtain the minimum area. Which proves that the norm minimizer has to be median of **b**.

This thing can also be modeled in integral form and then minimizing it will lead to the same solution.

Oops it took so many lines to explain.. :(
![Pictorial explanation][1]


  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>2.png>
47:<I just spoke with Prof. Vandenberghe on this (I'm taking 236B here at UCLA). The solution is the LP formulation that goes as follows, but on account of verifying just one thing. 

When you make the change of variables $x_c^Tx_c-r^2=t$, you get an LP (which the PDFs in this thread explain) to solve which is $sum_{i=1}^M (u_i^T cdot u_i - 2u_i^T cdot x_c + t)^2=|Ax-b|_2^2$ which is a least squares for appropriate $A$ and $x=[x_c;t]$ (in MATLAB convention). 

Now all you have to do is show that at the optimum $x_c,t$ you can recover $r=sqrt{x_c^Tcdot x_c-t}$ which means you have to show that $x_c^Tx_c-t ge 0$. This is where the apparent power of formulating a nonlinear variable as a linear has really helped us. To show this, take the KKT condition for the least squares problem and try to verify that $A^TAx=A^Tb implies x_c^Tx_c-t ge 0.$ And it's not hard to verify this. This also suggests that there is a solution to the original problem $iff$ the required condition is satisfied. Prof. Vandenberghe also suggested that a lot of other ill-conditioned non-linear problems can be posed as simpler ones provided you can verify that at the optimum you can recover the variables.>
48:<I could use some pointers on this one, I thought I figured it out, but I got all answers wrong on my first go, so I guess not :(.

What I was thinking was that there are 2 main steps,

1. Compute $C^T C$ where C is just A blocked over $ho I$, and is a (n+m) by n matrix. This should be order $2 n(m+n)^2$ from what I understand.
2. Factor $C^T C$ with an LDL. This should have a similar property of not having to recompute the factorization if rho changed, only D would be different. This is $1/3 (n+m)^3$ ops.

Other solves should be $n^2$
Therefore I answered (exactly typed as so):

m^2*n for the first part. ($m>n$)

n^3 for the second, ($n<m$)

and n^2 for the last.

What am I doing wrong?>
49:<Same here. i got revenue correct, penalty and profit got wrong. :(


----------
anyone got this correct put up a post will you>
50:<I still could not understand why all the vmax values should be capped to the same value. Dude, each entry of vmax represents the maximum rate for a specific reaction, which should be different from others. Capping them to justify the correctness of the answer doesn't make me feel better. I would just wait :(>
51:<My answer was accepted in the second attempt but I still don't really understand the model and stumbled over the right solution by dumb luck. :-(

It says that : '… the last reaction, R9, corresponds to the biomass creation, or cell growth…'

So i thought, it wouldn't make so much sense to cap the cell growth rate to find the reaction rate limit the cell growth rate is most sensitive to. 
My first approach was to cap the upper bounds of vmax without vmax(9), the result i get seemed reasonable to me, but of course it wasn't accepted. When i included the last reaction rate limit, of course the maximum cell growth rate reacts sensitive to that.>
52:<It sure looks convex.  Here is matlab code to plot it

    x=[-4:.01:4];
    log_norm_cdf=log( (erf(x/sqrt(2))+1)/2 );
    plot(log_norm_cdf)

I think it exists as a cvx primitive because it is convex and cannot be derived from other DCP rules.>
53:<Just some comments on "Shortest Path problem is LP."

I guess ppl form $Ax=b$ constrain, where $b$ is [-1; 0; 0 ....0; 1] to find valid path. (I found this from some reference). In this case, it will be a constrain on possible shortest paths. 

Take a simple 3 nodes example. One path is 1->3 and the other path is 1->2->3.
$A$ will be [-1 0 -1; 0 -1 1; 1 1 0] and b will be [-1 0 1].
Any $x=[x1; x2; x3]$ where $x1+x2=1$ will satisfy the constrain, which means e.g [ 1 0 0] 1->3 path is valid. [0 1 1] 1->2->3 is valid, and [0.5 0.5 0.5]  is valid as well. In the last case, there are two paths. 

Please correct me if I understand things wrong. Thanks.>
54:<Here is my approach: 

    rand('state',0);
    n=10;m=20;
    edges=[[1 1 1 2 2 2 3 3 4 4 5 5 6 6 7 7 8 7 8 9]'...
        [2 3 4 6 3 4 5 6 6 7 8 7 7 8 8 9 9 10 10 10]'];
    A=zeros(n,m);
    for j=1:size(edges,1)
        A(edges(j,1),j)=-1;A(edges(j,2),j)=1;
    end
    a=2*rand(m,1);
    x_max = 1+rand(m,1);B=m/2;
    
    % code to plot the graph (if you have biograph)
    %G=sparse(edges(:,1),edges(:,2),1,n,n);
    %view(biograph(G));

    c = zeros(n,1);
    c(end) = 1;    
    
    cvx_begin
        variables x(m) LogP(n)
        minimize (c'*LogP)   % = LogP(n)
        subject to 
            ones(m,1)'*x <= B
            x <= x_max
            x >= 0
            LogP(1) == 0
            for nodeId = 2:n
                EdgeList = find(edges(:,2) == nodeId);
                SourceNodeList = edges(EdgeList,1);
                % max of P_prev = P_current implies each of P_prev =
                % P_current
                LogP(SourceNodeList)-a(EdgeList).*x(EdgeList) == LogP(nodeId);            
            end
    cvx_end
    disp(['P^{max,*} = ', num2str(exp(LogP(n)))]);
    
    % below is the equal power allocation problem
    % no need for any optimization
    LogP_EqPower = zeros(n,1);
    LogP_EqPower(1) = 0;
    x_eq = (B/m)*ones(m,1);
    for nodeId = 2:n
        EdgeList = find(edges(:,2) == nodeId);
        SourceNodeList = edges(EdgeList,1);
        LogP_EqPower(nodeId) = max(LogP_EqPower(SourceNodeList)-a(EdgeList).*x_eq(EdgeList));    
    end
    disp(['P^{max} with equal allocation = ', num2str(exp(LogP_EqPower(n)))]);>
55:<@rannavajjala: Thank you very much for posting your code. 
I still fail to understand how: 
$	ext{log}P_j = underset{i}{	ext{max}}(	ext{log} P_i - a_k x_k)implies 	ext{log}P_j = 	ext{log} P_i - a_k x_k$ $forall i$, where $i < j$ and $k$ labels the edge $(i,j) in 	ext{edges}$. 

As an example,  if $j=3$, then your CVX code says: $ left( egin{array}{c}
logP_1 \ logP_2 end{array} ight) - left( egin{array}{c}
a(2)*x(2) \ a(5)*x(5) end{array} ight) == logP_3$. Since edge 2 goes from  $1	o 3$ and edge $5$ goes from $2 	o 3$. This seems to be missing the $max$ operator which isn't CVX representable for equality constraints (to my knowledge). Indeed, in solving the 'equal allocation problem' you used the $max$ (as we should), but that's no longer a problem since it's not inside CVX (and doesn't need to be, as you point out).

The other big picture concern I've is that method *does* enumerate all valid paths. I understand it as a greedy algorithm, where one finds $P_j$ by finding the maximum joint probability $p_k*P_i$, over all edges $k$ incoming to node $j$ from source nodes $i$. This technique necessarily visits *each* node sequentially and must consider *all* edges leading to the current node, thereby enumerating all valid paths. 

I know I must be trying your patience, but this HW problem really has me flummoxed! None of the approaches I've seen, I can fully understand (including the one chengzhang and bnmg have discussed) :(!>
