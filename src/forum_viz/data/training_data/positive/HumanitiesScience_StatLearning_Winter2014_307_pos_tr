0:<Cheers. :-)>
1:<Thank you, and nice to meet you :-) Been looking forward to doing this course as the authors & ESL are so well known. Great choice of topics!>
2:<I think that R is better for quick data analysis, but if you want implement some stat's algos for your system than Python would be better choice. :)>
3:<Hi all,

I picked up the StatLearning book a few months ago and started working on every problem in the book up to Chapter 6 so far.

https://github.com/<nameRedac_anon_screen_name_redacted>/stat-learning

I'd like to collaborate to complete the entire solutions manual during the course. Pull requests gladly accepted. :)

Cheers,

<nameRedac_anon_screen_name_redacted> <nameRedac_anon_screen_name_redacted>>
4:<Yes, They are :)
Privet! and lets collaborate>
5:<Hi! My name is Edward, I'm from Moscow
Is there anybody from Russia?:)>
6:<Hello!

It's great that I can participate in this course and be taught by the best! :)

Best wishes,
DD>
7:<??????! ? ? ???? ?? ???????! ??????? ???????! :-)>
8:<?????? ???? ???????????????? :)
???? ?? ??????? (skype:gomelkiev)>
9:<Great. Capital One came to my campus for recruitment last week only. But they came to Delhi, i don't know whether it's the same company or not. Anyways, since lots of Phds here, i am gonna ask tons of questions here. Pretty excited so much that didn't sleep last night :)>
10:<Hi,

You can get the URL of Youtube (right click on video and get URL) so you can download the youtube video using your preferred software. Some software options (https://www.google.com.br/?gws_rd=cr&ei=8VTfUpXbHYvNkQfx-IHoDw#q=download+youtube+video&safe=off).

The honor code says:
Use of Site and Materials
Unless otherwise indicated, the Site and content are the property of Stanford. In consideration for your agreement to the terms and conditions contained here, Stanford grants you a personal, non-exclusive, non-transferable license to access and use the Site. User may download material from the Site only for User's own personal, non-commercial use. User may not otherwise copy, reproduce, duplicate, retransmit, distribute, publish, sell, resell, commercially exploit or otherwise transfer any material or modify or create derivative works of the material. The burden of determining that use of any information, software or any other content on the Site is permissible rests with User. Some content on the Site is provided by third parties, and the use of that content is subject to rules and restrictions as indicated on the Site. You agree to be bound by all such rules and restrictions." so I think you can download and use for personal purpose.

Limitations: When downloading you lost the Close Captions.

Best regards>
11:<Looking forward to this course ;)>
12:<Yep it is **21-MARCH-2014**! :)

Are all deadlines set at 21/March/2014? Great if yes!>
13:<Thanks for the free online textbook! :)>
14:<Hi everyone! I'm <nameRedac_anon_screen_name_redacted> <nameRedac_anon_screen_name_redacted>, from Malaysia. Currently I'm doing my PhD at Monash University Malaysia. My research is on the relationship between religion,spirituality and diet. I'm at my second year of study and currently collecting data. I'm taking this course because it'll help me to learn R! :)>
15:<Thank you, Sir, & Prof. Tibshirani 

 - for this course
 - for the free text books (both ISL & ESL)
 - for the open source codes in the *lingua franca* of Statistics
 - and for your contributions to the Subject of Statistics

(not necessarily in that order :-))

<nameRedac_anon_screen_name_redacted>>
16:<We are a billion strong nation :) I'm from Delhi>
17:<Actually it's 1.2 Billion strong :)

Maybe if we find couple of guys around our cities, then we can arrange a meet up and exchange notes and best way to learn ML is doing a size able project in a team. what you guys think ??>
18:<Hey Rakhlin , It's good to see here buddy :)>
19:<Hi there,

I wanted to be able to download videos and watch them offline (coursera supports this). But could not find the download link, so I modified the edx-downloader tool I found on github to support the Stanford OpenEdX platform. 


First install youtube-dl first from: http://rg3.github.io/youtube-dl/download.html

Then, you can get my modified downloader at: https://github.com/<nameRedac_anon_screen_name_redacted>e/edx-downloader

Run it like this:
python edx-dl.py --platform=stanford -u=your@email.com -p=password
Then in interactive mode it's easy to figure out the rest.

I feel really fortunate to be enrolled in this wonderful course! Big thanks to the professors for doing this. I'm sure their efforts will be appreciated much more than they imagine :)

-<nameRedac_anon_screen_name_redacted>>
20:<Hi :)  
That is great that  You are taking this course Dominka. I am PhD student in Psychology at University of Gdansk, maybe we could take an example from others and create  a little polish study group? What do You think?
Best wishes z pozdrowieniami znad morza, 
Jana <nameRedac_anon_screen_name_redacted>>
21:<Thanks :)>
22:<Thanks Dmitri, it worked for me, too.  :-)>
23:<Hej Dan!
Roligt att se någon från Sverige här! Mina hälsningar till Stockholm :) Själv kommer jag dit om en månad // <nameRedac_anon_screen_name_redacted> från Moskva>
24:<For a moment i had the same confusion.
In Spanish "Mar 21" is read like "tuesday the 21st", that is the day the course started :-)
Later I realized that all quizzes had the same limiting date so it had to mean something different.>
25:<???????, ??? ??? ??? ?????! ???? ????? ???? ,)
?? ????? Coursera ????? ???????? ?????? ? FB, ???????? ? ???????????? ???????? :)
??????????? ?????? ???????, ?? ??? FB ??????? ??????? ???????? ????-?? ? ??????...
?????? ? ???? ???-?? ? ???????????.

??????????, ???, ??? ??????? ??? ?????? ???????? ???? :) <nameRedac_anon_screen_name_redacted> <nameRedac_anon_screen_name_redacted>>
26:<Alex, I knew I'd learn new things taking this class. ;)>
27:<Thanks for bringing all these stuff together in a user-friendly course! It seems that we're gonna have fun with statistics all over the World :)>
28:<?????? ????! ? ????? ?? ??????? (???????). ? ?????????? ??? ? ???????, ??? ??? ???? ???? ??? ????? ??????. ????????? ???????? ???????? ??????, ??????? ??????????? ??? ????? ??????:-) ??? ????????? ? ???????? http://vk.com/id1<phoneRedac>
???? ????-?? ????? ?????? ? ???????? ?????????? ??????, ??????????? ??????? ? ?.?. ?????? ? ????????????? (????? ?? ??????? ?????????) :-)>
29:<That is a good point. My though would be something like the following:

Built some data of genes cluster interaction pattern, which will be used as supervised learning reference. Then when a new cluster is available compare it with the reference and predict.

It may sound crazy :)>
30:<Thanks, this is very helpful :D !>
31:<Great :)>
32:<Hi Bhabani

I guess your confusion lies in the nature of clustering experiments :)

The goal of gene clusters might be the following: 

Let's say we analyse the expression of 1000 genes in cancer cells after drug treatment at four time points: 0, 1h, 5h, 1d.
What we now get out of a cluster analysis can be results like this:

 - genes 10, 102, 569, ... are not expressed at the beginning, then increase
 - genes 1, 17, 32, ... are expressed at the beginning, but then decrease upon drug treatment
 - genes 2, 20, 40, ... reach a peak after 1 hour and then drop
 - and so on

There is actually no way to predict that before, but those clusters may give very interesting insights, the experiment is actually aiming for (for example about how the drug interacts with different intracellular processes). The only way to have a training set, would be that someone has already performed the experiment before - and then it is not interesting any more to do it (publish of parish :) )>
33:<Of course, but in Iran:) youtube has been filtered!>
34:<@natedolson. To clarify, the *irreducible error*  is noise, it's random. The *reducible error* is how good our fhat is as a fit for f. 

Feel free to write back if I'm mistaken. :-)>
35:<First, I would try to install R Studio if you haven't done so. It makes working with R easier.

Second, if you like videos, do a search for R in YouTube. This channel looks pretty good: https://www.youtube.com/playlist?list=PL69A9CCD816A5F3A5

Third, if you want to learn step by step, this is a good site: http://tryr.codeschool.com

As far as books, I like R Cookbook and Learning R.

Good luck!>
36:<That actually answered the questions I had about this.  Thanks :)>
37:<in lecture 2.4, we see the use of test data works better than training data in assessing model accuracy. but how do we choose test data? do we need to randomly choose some data points as test data and fit the model with the remaining data only? thanks:)>
38:<I may not have listened carefully enough to 2.2, but this question seems much better fitted (pun intended ;) to the "Model Selection and Bias-Variance Trade-off" section (2.3)>
39:<Thank you! I guess I was just too tired. I used the notes of the next section to answer this question. :)>
40:<I guess this is about larger/less number of predictors, since the data and the probability assumptions remains unchanged.

But the question is: It is worth considering this bunch of predictors, while with only few of them the model will perform just as good as? I mean, the training set error will strictly decrease, but how strong will be this decreasing that matters. It is about a decision regarding what is a lot or just a negligibly little gain in the precision. And depending on which predictor you withdraws from the model, it will cause a more or less impact, so the problem lies in **which** predictors is "the best" and not on **how many**.

PS: I am not a native english speaker either, hope you understand me =P.>
41:<Yes :-)

Strong correlation, but not statistically uncommon.>
42:<These 2 informations are different.  
The slope gives the weight of the predictor, the $R^2$ gives the accuracy of the model.

Look at these 2 examples:  
- on the left panel, $eta_1simeq.5$ and is significantly different from 0 (t=14.8). But as the noise is low, the model fits the data pretty well, $R^2=.865$.  
- On the right panel, $eta_1simeq10$, still significantly non 0 (t=6.72), but the model does not fit the data as in the previous example, and $R^2=.474$

The slope (weight) can be huge, but the model poor...

Hope this help. :)

![enter image description here][1]


  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac>0<phoneRedac>7.png>
43:<Hi!

Just wanted to say that the course is great. Fun to follow, and a great learning experience! Compliments to the teachers and all the staff :)>
44:<Excellent! I would vote your post $infty$ times if I could :)>
45:<I agree, and will do the same soon :)>
46:<Hi,
Yes, I agree :)>
47:<I've resorted to just hacking the CSS in Chrome with "Inspect Element". :-)>
48:<I'm no expert (I'm not a member of the teaching staff, and I'm taking the course just as you are :) ), so I have no experience with that.

But, first of all, if you report that the mean satisfaction is 78%, then you do not report an interval at all, right? You just reported a point. What is the probability that the average rating is really 78%? Probably zero. Could be 78.1%, or 78.05%, and so on.

If you want to be precise, then you say that you estimate the average satisfaction to be between 75 and 80% (or whatever your sample gives you).

When asked how likely it is that this is correct, then the only correct answer is that you are 95% certain that your poll will produce the correct result.

If you want to lie (and then you have to hope that none of the execs knows statistics), then you say that you are 95% sure, because it sounds snappy.

On the other hand, if you do not want to lie if you make such a statement, then you can have a look at Bayesian confidence intervals (also called credible intervals). In these, the true sample mean is considered to be a random variable. Therefore you can truly say things like "Given the results we have, we are 95% sure that the true average satisfaction lies between A and B".>
49:<$epsilon$. 
Whoa :)>
50:<You're welcome :)>
51:<I just finished "Computing for Data Analysis" (Coursera) taught by Roger Peng. I thought the content was excellent. In order to get a firm grasp on the basics of R, I highly recommend week 1 of his lectures, which you can watch on YouTube:
https://www.youtube.com/playlist?list=PLjTlxb-wKvXNSDfcKPFH2gzHGyjpeCZmJ

Specifically, the topics of data types, subsetting, and vectorized operations are critical.>
52:<I don't think "all hope is lost", it is just that the interpretation of the coefficients in the case of multiple covariates that are correlated becomes more difficult. Hence in that figure, subsets regression and the LASSO are consider more interpretable than the OLS method, as these methods aim to remove some of the predictors (or reduce their effects) giving fewer "things" to consider at once.

There are ways to help one interpret the effect on $y$ of changes in the covariates $X$, such as partial plots, where you hold all bar 1 or 2 of the covariates at some value, vary the values of the 1 or 2 covariates not held fixed and draw the change in the response as you vary them. Of course, if there are correlations or interactions, the effect of the covariates you are varying can change with the values of the other covariates you held fixed. Hence you might repeat this exercise, say using the lower quartile, median and upper quartile values to fix covariates at.

The point is not to give up and not interpret the meaning of the coefficient estimates, but to critically think through what they *actually* mean. Yes, this gets more difficult with many correlated or interacting predictors, but no-one said doing stats was easy or quick :-)>
53:<Since there's no actual class, I suppose it's *all* homework. :-)

If you mean "what are the things we need to do and turn in?", then from what I can tell, it is this:

 - After nearly every video, there is a very short quiz to test your
   understanding of a key point or two from the video. To get to them,
   either click the right arrow underneath the video, or choose the
   right icon in the bar that appears above the video (it looks like an
   old tractor-feed printer to me, but I guess it's supposed to
   represent a "test")
 - At the end of each week, there's a quiz as the last "subsection"
 - You can review the things that have been scored and the things
   yet-to-be scored from the "progress" screen.

Other that that, there's nothing else to submit or turn in, as far as I can see.

(Even though you don't turn them in, I'd suggest that you follow along with the R labs, or at least do them from the book, as well as take a look at the questions at the end of the book chapters.)>
54:<Hi JeffJetton,

For the first simulation:

n <- 100 # number of training obs

m <- 1000 # number of test obs

p <- 500 # number of features

x <- matrix(rnorm(n*p), ncol=p)

xte <- matrix(rnorm(m*p), ncol=p)

y <- rep(1:4,each=25)

yte <- rep(1:4, each=250)

x[y==1,1:25] <- x[y==1,1:25] 0.7

x[y==2,26:50] <- x[y==2,26:50] +0.7

x[y==3,51:75] <- x[y==3,51:75] +0.7

x[y==3,76:100] <- x[y==3,76:100] +0.7

xte[yte==1,1:25] <- xte[yte==1,1:25] + 0.7

xte[yte==2,26:50] <- xte[yte==2,26:50] +0.7

xte[yte==3,51:75] <- xte[yte==3,51:75] +0.7

xte[yte==3,76:100] <- xte[yte==3,76:100] +0.7

Is It correct?>
55:<????, ???????????? www.linkedin.com/in/mshokin/ - ????? ??? ?????? ?? ??????, ? R ?????? ?????? ???????, ??? ??? ???? ?????? ?????? ? ??????????... ???? ????? :)>
56:<Thanks so much for the response. Although I don't understand all of the concepts you've presented, I get the gist, and maybe I'll have a deeper understanding by the end of the course :)>
57:<+1. I guess the question assumes more maths knowledge than I was expecting I needed to feel completely comfortable with this course... :)>
58:<lm.auto<-lm(mpg ~poly(horsepower, 5), data=Auto)

x<-seq(50,250,length=100)

y<-predict(lm.auto,newdata=data.frame(horsepower=x))

plot(horsepower, mpg, data=Auto)

lines(x,y, type="l", col="green")>
59:<I don't really understand what is meant by the how term "flexibility" is used here. 

So is a model **MORE** flexible if it has **HIGH** variance and therefore tending to OVERFIT **?**

Or is that wrong? It seems a model is maybe flexible if it's high bias and therefore running down the middle more... (like a linear straight line equation on muliple points in a scatter plot instead of weaving all over to every point [over fitted])

For that matter, I'm not sure I understand if high Bias is same as high Variance.

Can someone explain what "flexible" means?

[I've had statistics, machine learning, and R classes before, so not a complete newbie... just dumb on this for now.] 

Thanks. :)>
60:<Can someone help me understand the question?

(A hypercube with side length 1 in d dimensions is defined to be the set of points (x1, x2, ..., xd) such that 0?xj?1 for all j = 1, 2, ..., d. Define the boundary region of the hypercube to be the set of all points such that there exists a j for which 0?xj?.05 or .95?xj?1 (i.e., it is the set of all points that have at least one dimension in the most extreme 10% of possible values). What proportion of the volume of a hypercube of dimension 50 is in the boundary region?

Please give your answer as a value between 0 and 1 with 3 significant digits. If you think the answer is 50.52%, you should say 0.505:)>
61:<I have tried to buy the book also. I bought it but they don't have it at stock. I have to wait :-) and they couldn't say to me how much!>
62:<Excellent! Thanks alhf :)>
63:<A very good reason, I will say :)>
64:<This drawing is really helpful and I suggest to start just with this one and then focus on 3rd dimension. All you have to do then is to compare volume of your cube with the smaller one without that boundry ;)>
65:<Hej Erik!
You might have solved it already, but I'll still write :)
As sallysue has written, one has to download the file from the book's website, and then follow instructions on the p.48: first File/Change dir. (choose the directory where the data file is saved) and then run the read.table command. Worked for me, although I also had problems at first, when I tried to load the data through "Open file" etc.
Hälsningar från Moskva! :D>
66:<Hi, am a bit late to post this. :) 

Am from Bangalore.>
67:<That's definitely correct page. Actually the value of 50% makes your equation very easy to solve;)>
68:<This question maybe at first seems to be quite tricky, but as a lot of participants said, it is easier to understand the problem considering lower dimensions. Some useful pictures for square was posted in some topic, so maybe it's good to look at this. Actually the problem is not very complex and you don't have to know higher maths to figure it out. Even student from secondary school has all mathematical knowledge to compute it, the problem just requires to notice the pattern for lower dimensions and generalize it;)>
69:<I bought the book. I love that they provide the PDF for free, but I'm a sucker for the printed page. With the discount, the price is quite reasonable for a textbook these days, and when you consider that the class itself is free, well...

Incidentally, I got mine in the mail very quickly. That might just be because one of the publisher's distribution centers is one town over from me though. :-)>
70:<I need some simple help with linear algebra. What does a diagonal covariance matrix look like? For example, let us say that I have 4 explanatory variables and 1 dependent variable that takes on 2 classes (1=default, 0 =non-default).>
71:<Thank you very much;)>
72:<Thanks much for the pointers man. I'm aware of the sigma notation for addition but wasn't sure show the end formula was derived. You mentioned Sal demonstrating the process, I'll sure check that out. When I posted the question (2 days ago), I thought I'd actually dropout of the class, brush up math a little and come back when I'm able. I'm kinda OK with conditional P but will brush up on Bayes's rule. I'm ok with basic matrix manipulations as well. It's just been a long time since I did math (years ago), I guess the symbols and notations are getting me. Even the SE formula at 9.56 minutes in section 3.1, it's not a hard formula at all but I wasn't happy with myself in understanding how we arrived there.

I'm brush up on some basic stats & probability first (at Khan Academy,)come back in a week or two and see if I feel more comfortable taking this class. I hope they archive it even when it's over if I don't make it back soon enough :D

But I really appreciate your response man, especially pointing to Sal's videos. I'm sure that'll help.>
73:<that sounds very serious.. though i'd like to converse with you at a more beginner level if possible :) I think i'll be more confident tackling Elements of Statistical learning after finishing this course first.

I am just not sure what aspect of the heterosdasticity problem is solved by fitting log(Y) with X (or is it log(X)) ..from my newbie intuition i think the problem of having a non-constant variance with respect to X is still present and that we still need to be careful on how we convey the confidence interval and how we do hypothesis testing. 

Am I right to think like that? Or is it as you've said, that the purpose of logging things have something to do with changing additive property of a problem into a multiplicative one (maybe for easier computation?) , but that's all it does without actually curing the proverbial 'acne'?

Thank you>
74:<Ohh, that's what you meant when you wrote above that you don't know the equation. I thought you were referring to the regression equation :p

Thanks for your inputs though :)
Will look into the R function.>
75:<It's bad. The second mention must have just been a slip on his part.

A good way to remember:

 - LDL - "L" for "Lethal"
 - HDL - "H" for "Healthy"

:-)>
76:<Perhaps you could help me with an R issue. I have just started using R via Rstudio today. Following along with video 3.R --> I get the following error when trying to plot:
## R:
Error in plot(medv - lstat, Boston) : object 'medv' not found
##

I did get this warning re the ISLR package, which may be relevant, although I think it is installed correctly: 

## R:
Warning messages:
1: package 'ISLR' was built under R version 3.0.2 
2: In loadNamespace(package, c(which.lib.loc, lib.loc), keep.source = keep.source) :
  package 'ISLR' contains no R code
##



And if it helps here is the result of my search():

## R:
search()
 [1] ".GlobalEnv"        "package:ISLR"     
 [3] "package:MASS"      "tools:rstudio"    
 [5] "package:stats"     "package:graphics" 
 [7] "package:grDevices" "package:utils"    
 [9] "package:datasets"  "package:methods"  
[11] "Autoloads"         "package:base"     
##




Any help much appreciated. Likely something simple / I hope!>
77:<@GBL: I have contributed in a minor way (and so have a handful of others), but that solutions guide was primarily the work of the repository owner (https://github.com/asadoughi). Just wanted to make sure I wasn't being credited! :)>
78:<So far, pretty much all the exercises have been doable in either Python (w/ pandas and statsmodels) or pencil and paper. I only fired up R to answer the "what does R say when you type this" kind of question.

(Nice to see you here too, Peter :)>
79:<Thanks a lot!! Was frustrated with the question. Your explanation helped me get the logic. Got it right finally!!:-)>
80:<My guess is X becomes the new Y in discriminant analysis, and k (number of classes in the original Y) becomes the new X. Usually k has limited dimensions/ classes, while X could be rich in dimensions?

But after all, it's only my guess :p>
81:<Thank you Kevin - best explanation - helped me a lot  :)>
82:<The videos of a Coursera course on an introduction to programming with R are on Youtube:  
http://www.youtube.com/watch?v=EiKxy5IecUw&list=PL7Tw2kQ2edvpNEGrU0cGKwmdDRKc5A6C4>
83:<*From the dept of "too much free time," I created SAS programs to duplicate the output from the R code in the Chapter 4 Lab.;
    
proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

***** 4.6.1 The Stock Market Data *****;
title '4.6.1 The Stock Market Data';

* csv file created from Smarket dataset (> write.csv(Smarket,file="Smarket.csv");
* note: need to delete 1st column in resulting csv file before importing into sas;
proc import out=Smarket dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookSmarket.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=100;
run;

* > names(Smarket);
proc contents data=Smarket;
run;

* > summary(Smarket);
proc univariate data=Smarket;
run;

* > cor(Smarket[,9]);
proc corr data=Smarket;
run;

* > plot(Volume);
data Smarket;
   set Smarket;
   Index + 1;
run;

proc sgplot data=Smarket;  * note: by default, places a png file in current folder;
   scatter x=Index y=Volume;
run;

quit;>
84:<*From the dept of "too much free time," I created SAS programs to duplicate the output from the R code in the Chapter 4 Lab.;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

* csv file created from Smarket dataset (> write.csv(Smarket,file="Smarket.csv");
* note: need to delete 1st column in resulting csv file before importing into sas;
proc import out=Smarket dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookSmarket.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=100;
run;

***** 4.6.2 Logistic Regression *****;
title '4.6.2 Logistic Regression';

* > glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial)
  > summary(glm.fit)
  > glm.probs=predict(glm.fit,type ="response");
proc logistic data=Smarket;
   title2 'Full Model: All Data';
   model Direction(event='Up') = Lag1 Lag2 Lag3 Lag4 Lag5 Volume;
   output out=Preds resdev=DevResid predprobs=i;
run;

proc means data=Preds min p25 median p75 max;
   var DevResid;
run;

* > glm.probs [1:10];
proc print data=Preds (obs=10);
   var IP_Up;
run;

* confusion matrix (http://support.sas.com/kb/22/603.html);
* > glm.pred=rep("Down",1250)
  > glm.pred[glm.probs > .5]="Up"
  > table(glm.pred,Direction);
proc freq data=Preds;
   table _INTO_*Direction / out=CellCounts norow nocol nopercent;
run;

* calculate % logistic regression correctly predicted;
* > mean(glm.pred==Direction);
data CellCounts;
   set CellCounts;
   Match = 0;
   if Direction = _INTO_ then Match = 1;
run;

proc means data=CellCounts mean;
   freq count;
   var Match;
run;

* create model from training data (year < 2005);
* > train=(Year < 2005)
  > Smarket.2005=Smarket[!train,]
  > Direction.2005=Direction[!train];
data TrainingData Smarket2005;
   set Smarket;
   if Year < 2005 then output TrainingData;
     else output Smarket2005;
run;

* fit reduced model on training data and apply to test data;
* > glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial,subset=train);
proc logistic data=TrainingData;
   title2 'Full Model';
   model Direction(event='Up') = Lag1 Lag2 Lag3 Lag4 Lag5 Volume;
   * > glm.probs=predict(glm.fit,Smarket.2005,type="response");
   score data=Smarket2005 out=Preds2;
run;

* confusion matrix on test data (http://support.sas.com/kb/22/603.html);
* > glm.pred=rep("Down",252)
  > glm.pred[glm.>
85:<*From the dept of "too much free time," I created SAS programs to duplicate the output from the R code in the Chapter 4 Lab.;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

* csv file created from Smarket dataset (> write.csv(Smarket,file="Smarket.csv");
* note: need to delete 1st column in resulting csv file before importing into sas;
proc import out=Smarket dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookSmarket.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=100;
run;

***** 4.6.3 Linear Discriminant Analysis *****;
title '4.6.3 Linear Discriminant Analysis';

* create test & training datasets;
data TrainingData Smarket2005;
   set Smarket;
   if Year < 2005 then output TrainingData;
     else output Smarket2005;
run;

* fit an LDA model to the training data and apply it to the test data;
* > lda.fit=lda(Direction~Lag1+Lag2,data=Smarket,subset=train)
  > lda.pred=predict(lda.fit, Smarket.2005);
proc discrim data=TrainingData testdata=Smarket2005 method=normal pool=yes simple testout=Preds4;
   * 'pool=yes' performs LDA: 'the LDA classifier results from assuming ... a common variance'; 
   title2 'Reduced Model: From Training Data Applied to Test Data';
   class Direction;
   priors proportional;  * 'In the absence of any additional information, LDA estimates pk using the ';
   var Lag1 Lag2;        * 'proportion of the training observations that belong to the kth class     ';
run;

* confusion matrix (http://support.sas.com/kb/22/603.html);
* > lda.class=lda.pred$class
  > table(lda.class,Direction.2005);
proc freq data=Preds4;
   table _INTO_*Direction / out=CellCounts4 norow nocol nopercent;
run;

* calculate % correctly predicted;
* > mean(lda.class==Direction.2005);
data CellCounts4;
   set CellCounts4;
   Match = 0;
   if _INTO_ = Direction then Match = 1;
run;

proc means data=CellCounts4 mean;
   freq count;
   var Match;
run;

* recreate the predictions using the posterior probabilities;
* > sum(lda.pred$posterior[,1]>=.5)
  > sum(lda.pred$posterior[,1]<.5)
  > sum(lda.pred$posterior[,1]>.9);
data Thresholds;
   set Preds4 end=eof;
   if Down ge 0.5 then N_AbovePoint5 + 1;
   if Down < 0.5 then N_BelowPoint5 + 1;
   if Down > 0.9 then N_AbovePoint9 + 1;
   keep N_AbovePoint5 N_BelowPoint5 N_AbovePoint9;
   if eof then output;
run;

proc print data=Thresholds;
run;

quit>
86:<proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

* csv file created from Smarket dataset (> write.csv(Smarket,file="Smarket.csv");
* note: need to delete 1st column in resulting csv file before importing into sas;
proc import out=Smarket dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookSmarket.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=100;
run;

* create test & training datasets;
data TrainingData Smarket2005;
   set Smarket;
   if Year < 2005 then output TrainingData;
     else output Smarket2005;
run;

***** 4.6.4 Quadratic Discriminant Analysis *****;
title '4.6.4 Quadratic Discriminant Analysis';

* > qda.fit=qda(Direction~Lag1+Lag2,data=Smarket,subset=train)
  > qda.class=predict(qda.fit,Smarket.2005)$class;
proc discrim data=TrainingData testdata=Smarket2005 method=normal pool=no simple testout=Preds5;
   * 'pool=no' performs QDA: 'QDA assumes that each class has its own covariance matrix'; 
   title2 'Reduced Model: From Training Data Applied to Test Data';
   class Direction;
   priors proportional; 
   var Lag1 Lag2;
run;

* confusion matrix (http://support.sas.com/kb/22/603.html);
* > table(qda.class,Direction.2005);
proc freq data=Preds5;
   table _INTO_*Direction / out=CellCounts5 norow nocol nopercent;
run;

* calculate % correctly predicted;
* > mean(qda.class==Direction.2005);
data CellCounts5;
   set CellCounts5;
   Match = 0;
   if _INTO_ = Direction then Match = 1;
run;

proc means data=CellCounts5 mean;
   freq count;
   var Match;
run;

quit;>
87:<*From the dept of "too much free time," I created SAS programs to duplicate the output from the R code in the Chapter 4 Lab.;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

* csv file created from Smarket dataset (> write.csv(Smarket,file="Smarket.csv");
* note: need to delete 1st column in resulting csv file before importing into sas;
proc import out=Smarket dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookSmarket.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=100;
run;

* create test & training datasets;
data TrainingData Smarket2005;
   set Smarket;
   if Year < 2005 then output TrainingData;
     else output Smarket2005;
run;

***** 4.6.5 K-Nearest Neighbors *****;
title '4.6.5 K-Nearest Neighbors';

* model using k=1 (results may differ slightly from R b/c of ties/use of seed - i think);
* > library(class)
  > train.X=cbind(Lag1,Lag2)[train,]
  > test.X=cbind(Lag1,Lag2)[!train,]
  > train.Direction=Direction[train]
  > set.seed(1)
  > knn.pred=knn(train.X,test.X,train.Direction,k=1);
proc discrim data=TrainingData testdata=Smarket2005 method=npar k=1 testout=Preds6 noprint;
   title2 'K=1 Reduced Model: From Training Data Applied to Test Data';
   class Direction;
   var Lag1 Lag2;
   priors proportional;
run;

* confusion matrix (http://support.sas.com/kb/22/603.html);
* > table(knn.pred,Direction.2005);
proc freq data=Preds6;
   table _INTO_*Direction / out=CellCounts6 norow nocol nopercent;
run;

* calculate % correctly predicted;
data CellCounts6;
   set CellCounts6;
   Match = 0;
   if _INTO_ = Direction then Match = 1;
run;

proc means data=CellCounts6 mean;
   freq count;
   var Match;
run;

* model using k=3 (results may differ slightly from R b/c of ties/use of seed - i think);
* > knn.pred=knn(train.X,test.X,train.Direction,k=3);
proc discrim data=TrainingData testdata=Smarket2005 method=npar k=3 testout=Preds7 noprint;
   title2 'K=3 Reduced Model: From Training Data Applied to Test Data';
   class Direction;
   var Lag1 Lag2;
   priors proportional;
run;

* confusion matrix (http://support.sas.com/kb/22/603.html);
* > mean(knn.pred==Direction.2005);
proc freq data=Preds7;
   table _INTO_*Direction / out=CellCounts7 norow nocol nopercent;
run;

* calculate % correctly predicted;
data CellCounts7;
   set CellCounts7;
   Match = 0;
   if _INTO_ = Direction then Match>
88:<*From the dept of "too much free time," I created SAS programs to duplicate the output from the R code in the Chapter 4 Lab.;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

***** 4.6.6 An Application to Caravan Insurance Data *****;
title '4.6.6 An Application to Caravan Insurance Data';

* csv file created from Caravan dataset (> write.csv(Caravan,file="Caravan.csv");
* note: need to delete 1st column in resulting csv file before importing into sas;
proc import out=Caravan dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookCaravan.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=1000;
run;

* > attach(Caravan)
  > summary(Purchase);
proc freq data=Caravan;
   table Purchase / nopercent nocum;
run;

* standardize the data;
* > standardized.X=scale(Caravan[,-86]);
proc standard data=Caravan mean=0 std=1 out=standardizedX;
run;

* check variances of original & standardized data;
* > var(Caravan[,1])
  > var(Caravan[,2]);
proc means data=Caravan var;
   title2 'Original Data Variances';
run;

* > var( standardized.X[,1])
  > var( standardized.X[,2]);
proc means data=standardizedX var;
   title2 'Standardized Data Variances';
run;

* create test & training datasets;
* > test=1:1000
  > train.X=standardized.X[-test,]
  > test.X=standardized.X[test,]
  > train.Y=Purchase[-test]
  > test.Y=Purchase[test];
data trainX testX;
   set standardizedX;
   if _n_ le 1000 then output testX;
   else output trainX;
run;

* model using k=1 (results may differ slightly from R b/c of ties/use of seed - i think);
* > set.seed (1)
  > knn.pred=knn (train .X,test.X,train .Y,k=1);
proc discrim data=trainX testdata=testX method=npar k=1 testout=Preds8 noprint;
   title2 'KNN: K=1';
   class Purchase;
   priors proportional;
run;

* confusion matrix;
proc freq data=Preds8;
   table _INTO_*Purchase / out=CellCounts8 norow nocol nopercent;
run;

* > mean(test.Y!= knn.pred)
  > mean(test.Y!=" No");
data CellCounts8;
   set CellCounts8;
   NoMatch = 0;
   if _INTO_ ne Purchase then NoMatch = 1;
   if Purchase = 'Yes' then Yes01 = 1;
     else Yes01 = 0;
run;

proc means data=CellCounts8 mean;
   freq count;
   var NoMatch Yes01;
run;

* model using k=3 (results may differ slightly from R b/c of ties/use of seed - i think);
* > knn.pred=knn(train.X,test.X,train.Y,k=3);
proc discrim data=trainX testdata=testX method=>
89:<From what I understand (see here: http://www.google.co.uk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=9&ved=0CG4QFjAI&url=ftp%3A%2F%2Fftp.sanger.ac.uk%2Fpub4%2Ftheses%2Fdown%2Fchapter2.pdf&ei=D5r1UtzOBIOQ7AaC_oDoAw&usg=AFQjCNFHDUZt5z-6Ow7_AqRXR6iRfXCnHw&sig2=H4k87eZDTSHW3H3YhZNYuw&bvm=bv.<phoneRedac>3,d.ZGU) sparse learning has to do with selecting a small number of highly predictive features. I think we'll learn about this in future weeks (but if we don't, there's always google).

(If I understand this correctly, perhaps we've already learned a bit about that when we talked about forward and backward selection of significant predictors in logistic regression).>
90:<I figured it out :).>
91:<Hi,

With regards to slide 20 of Lecture 4.5, I'm not convinced when prof. Hastie says that $f_k(x)=P(X=x|Y=k)$ is the pdf (probability distribution function) for $X$, given $Y$ in class $k$. If $X$ is a continuous random variable, then 

$0=P(X=x)geq P(X=x|Y=k) geq 0 implies P(X=x|Y=k)=0$

By the way, that would also make the denominator of Bayes Theorem zero,which also is an issue. So, how do we solve that? I guess that when we write $X=x$, we are really thinking of $X in [x,x+dx]$. This way, $P(X in [x,x+dx])=f_X(x)dx$ and $P(X in [x,x+dx]|Y=k)=f(x|Y=k)dx$, where $f_X(x)$ denotes the marginal density of $X$, while $f(x|Y=k)$ is the conditional density of $X$ given $Y=k$. By eliminating $dx$ between numerator and denominator, and using the Total Probability theorem, we get the final formula in slide 20. Is this correct? Thanks, 

Best Regards

<nameRedac_anon_screen_name_redacted> <nameRedac_anon_screen_name_redacted>>
92:<> "But the height of the student does not seemed to be related."

Says who? You can't actually know for sure if it's related until you run the model and check.

For example, maybe height correlates with nutrition/diet, and malnourished students might perform worse academically? Or maybe taller students get called on in class more often and/or otherwise make more favorable impressions on the teacher? Or maybe taller students are more likely to be on the basketball team and thus have less time to study?

Or maybe you're right and there's no relation whatsoever. You would still need to choose a type of model to *check* that assumption, and some models would be more appropriate than others.

Don't forget that you can choose more than one! :-)>
93:<Patience! :-)

The new chapters seem to be "opened" at *some* point Saturday morning (California time), but not necessarily at the stroke of midnight.

I assume we'll see something within an hour or so, if the pattern holds.>
94:<We want to to see for which $k$ $p_k(x)$ is largest. Take a closer look at the denominator of $(4.12)$. As this is just a sum over all possible classes (and does not depend on any particular $k$) we can ignore it. 

The next step is to take the $log$ of the numerator of $p_k(x)$ (Since this is just a monotone transformation). When you do that and discard the unimportant terms (i.e. all that don't depend on $k$) you get $(4.13)$.

To get from $(4.13)$ to $(4.14)$ set $delta_1(x)=delta_2(x)$ (why?) and solve for $x$.>
95:<There! :-)>
96:<Many Thanks JeffJetton and much appreciated :)>
97:<This is a bit Out of Topic.
I think we should take results of statistical models with a pinch of salt.
Although I am a great fan of data-driven decisions, I believe that the power of statistics lies in the humans making a great use of it ;)

In the specific case, it would be very hard to convince anyone that height can correlate with grades. If it does, there's something else behind and the true correlation is not with height actually.

Some of students who excel in basketball could have extra reasons to study statistics, e.g. comparing NBA players and decide who is stronger, comparing their own performances with others on solid grounds, etc.>
98:<The bad "translations" of his accent in the captions are sometimes funny too.

They've typed "foot" when he said "fit" on at least one occasion. :-)>
99:<[Intro Probability: The Science of Uncertainty][1] at edX just started.  It goes up to bayesian inference and markov chains.

[Data Analysis and Statistical Inference][2] starts in a week.  It would have been great to take before this course, but you can't have everything. :)

[The Analytics Edge][3] at edX starts in about a month.  It appears to cover similar material to this course, also using R.

The Coursera [Data Science specialization][4] starts in two months and consists of nine one-month classes which run three at a time.  It's by Roger Peng and Jeff Leek of Johns Hopkins.  I'm definitely going to take all those.

So there's a ton of good data science moocs being offered this year.  Thanks to all the good people at Stanford, MIT, Duke, and Johns Hopkins.


  [1]: https://courses.edx.org/courses/MITx/6.041x/1T2014/info
  [2]: https://www.coursera.org/course/statistics
  [3]: https://www.edx.org/course/mitx/mitx-15-071x-analytics-edge-1416
  [4]: https://www.coursera.org/specialization/jhudatascience/1/courses>
100:<at BRM ... Thanks for clarifying. Now the question makes sense. :)>
101:<Hello folks, I am a Web Analyst from Ahmedabad. Very excited to be a part of this course. :)>
102:<Whew! It's just me or has the pace increased steadily from Chap.4 onwards? I found Classification to be quite an hard chapter, and I got a little behind, but I'm working hard to get back on rails :) What about the others? Did you find this chapter challenging, or was it just more of the same for you?>
103:<I do not think linear regression and LDA are equivalent, but they both contain the word "linear". :-)>
104:<Ah, ok, it's the decision boundary which is the same, not the probabilities! That definitely makes sense. I thought that would be the case, but I wanted to be sure before trying to prove it. I will try and prove it. Do I get extra credit if the proof is correct? ;))>
105:<For those interested in trying this out for themselves, 
here's some R code for the plots above.

(Hopefully it's self contained, so you can just
cut-and-paste to get started)

First plot (not perfect separation)

    data1 <- data.frame(x=c(1,2,3,4,5,6), y=c(0,0,1,0,1,1))
    plot(y ~ x, data=data1)
    glm.fit <- glm(y ~ x , family="binomial", data=data1)
    glm.fit
    b0 <- glm.fit$coefficients[1]
    b1 <- glm.fit$coefficients[2]
    prob.curve.data <- data.frame(x=seq(1,6,by=0.01))
    prob.curve.data <- transform(prob.curve.data, prob=exp(b0+b1*x)/(1+exp(b0+b1*x)))
    plot(y ~ x, data=data1)
    lines(prob ~ x, data=prob.curve.data, col="blue")
    
Second plot (perfect separation)
    
    data2 <- data.frame(x=c(1,2,3,4,5,6), y=c(0,0,0,1,1,1))
    plot(y ~ x, data=data2)
    glm.fit <- glm(y ~ x , family="binomial", data=data2)
    glm.fit
    b0 <- glm.fit$coefficients[1]
    b1 <- glm.fit$coefficients[2]
    prob.curve.data <- data.frame(x=seq(1,6,by=0.01))
    prob.curve.data <- transform(prob.curve.data, prob=exp(b0+b1*x)/(1+exp(b0+b1*x)))
    plot(y ~ x, data=data2)
    lines(prob ~ x, data=prob.curve.data, col="red")>
106:<One of the things I thought about as I was going through the R exercises for using LDA to predict Smarkets direction is that one does not have to trade every day.  If you could understand which days represent situations that are really set up for up days (or down days) then you could keep your powder dry until one of those days happened to come by and then pounce on the opportunity.

As I thought about how it might make sense to use the LDA Up and Down predictions to do so, I came up with the idea of looking for days where the absolute difference between probabilities of being in either k(given x) was > the .95 percentile and the prob of being in the predicted k (given x) was > .51.  This improved the proportion of correct guesses to .61-ish.  This rate may be represent a bad sample given I've only now got 13 yhat/y pairs on which to base the statistic AND in practical terms this means one only has 13 trading days (out of the original 252 for Smarket.2005), but if there is truly some "there" there, I would say most folks would take that trade off?

What I'm not sure about is the way in which I intuited this method and whether the absolute difference in probabilities of being in either k (given x) truly has the meaning which I'm inferring? Anyhow, the more I think about it, the more I find I'm talking myself out of thinking this is a good idea.  I think it falls more into the realm of total voodoo, but anyhow it was a fun exercise, and I offer up the code below...

    require(ISLR)
    require(MASS)
    lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005) 
    Smarket.2005=subset(Smarket,Year==2005) # subset data where year = 2005
    lda.pred=predict(lda.fit,Smarket.2005) # use it as data to make predictions
    mean(lda.pred$class==Smarket.2005$Direction)
    df1 <- data.frame(lda.pred)
    df1$diff <- abs(df1$posterior.Down-df1$posterior.Up)
    quantile(df1$diff, .95) # the 95th percentile of the absoluet difference
    df1$max <- pmax(df1$posterior.Down, df1$posterior.Up)
    View(df1[ df1$diff > quantile(df1$diff, .95) & df1$max > .51 ,])
    df1p_gt.95 <- df1[df1$diff > quantile(df1$diff, .95)& df1$max > .51 ,]
    df2.test <- Smarket.2005[ df1$diff > quantile(df1$diff, .95) & df1$max > .51 ,]
    mean(df1p_gt.95$class==df2.test$Direction)
    table(df1p_gt.95$class, df2.test$Direction)
    data.frame(df1p_gt.95$class, df2.test$Direction)>
107:<No,no, don't worry, it's not a convention :) it's just that for some examples they don't use field data, but simulated data from known distributions. I.e., they know exactly which are $pi_k$ and $f_k(x)$ for each $k$ , so they know which are all the terms in the Bayes theorem and they can exactly compute $P(Y=k|X=x)$ . Once you have this conditional probability law, then it's trivial to construct the Bayes classifier. But in real life you only get a sample of data from a population! You don't know which are the distributions that generated it. Sometimes, that's exactly one of the things you would like to find out! In the case of LDA, for example, you *assume* that the $f_k(x)$ are normal and share a common standard deviation, but you don't know for sure. Also, you have no idea which are the means $mu_k$ and standard deviation $sigma$ of these (assumed) normal distributions, and you don't know which are the $pi_k$. Of course you can *estimate* them from data, but you don't know for sure.

By the way, one of the examples where LDA got so close to the Bayes Classifier was an example with two classes,where $f_1(x)$ and $f_2(x)$ were *chosen* to be normal with the same standard deviation. That's also a reason why LDA did so well :)>
108:<Hi!

I posted the same question some time ago, and got a great answer:

http://goo.gl/tg4wSc

Best of luck with your pretty formulas :)>
109:<I have problems as well. Can anyone help?

I used the following code:


#(fixed) block bootstrap


load("5.R.RData")

blocksize=<zipRedac>00

nblocks = nrow(Xy)/blocksize

Xy$block=rep(<zipRedac>:nblocks,each=blocksize)

n=<zipRedac>0000

outmat <- matrix(ncol=3,nrow=n)


for(i in <zipRedac>:n) {


 idx <- sample(<zipRedac>:nblocks,replace=T,size=nblocks)

 d.i<-data.frame()


 for(j in <zipRedac>:<zipRedac>0){


 	d.j <- Xy[Xy$block==idx[j],]

 	d.i <- rbind(d.i,d.j)


 }


 outmat[i,]<-coef(lm(y~X<zipRedac>+X2,data=d.i))

}


# s.e. 

sum((outmat[,2]-mean(outmat[,2]))^2)/(n-<zipRedac>) # (5.8)

[<zipRedac>] 0.0<phoneRedac>>
110:<This forum needs some tender love.  :)  Looking for an answer is like digging through a dumpster.  :(
Moderator, I suggest the following:

 1. Could you expand the list of questions (left side) bigger?  Working through that on my 13' MBA is tedious.
 2. Sort the chapter questions by chapter.  Say, I want to know about interaction from chapter 3.3, I'd head to that sub-category.  I don't want to see "how to install R package".
 3. I am aware of the search box, but what if the person failed to use a keyword which came to my mind.  I could be double posting on here.

Thanks,  
<nameRedac_anon_screen_name_redacted>>
111:<In ISLR, 5.3.1, the predict() is done on the whole dataset, and then the values to do with train thrown away ([-train]), then take the mean of the square to get the MSE.

Why couldn't/wouldn't you take the predict() of the test dataset only, and then mean the square?  [That would give you ...mpg-predict(lm.fit,Auto[-train]))^2)  , i.e. [-train] inside the bracket  ]

The second way gives a lower MSE, but I can't fathom out why.

If it's obvious, would appreciate being put out of my misery :-)>
112:<cool, I did not know this was possible - I am not sure if I will get to the bottom of the lda thing, but this tip will come in handy :)
Cool Blog by the way>
113:<thx.. now it works for me !! :)>
114:<Anyone in the SF Bay Area interested in teaming up for the DREAM8 Challenge?
Details below:

The Rheumatoid Arthritis Responder DREAM Challenge is Now Open for Participation
Dear Colleague,  

The Rheumatoid Arthritis Responder DREAM Challenge tasks participants to predict the response of Rheumatoid Arthritis patients to anti-TNF therapy based on genetics and clinical data. 

The Data. Challenge participants can now download the 2.5 million SNPS and  clinical information of about 2000 patients. Data can be downloaded from the Challenge website (https://www.synapse.org/#!Synapse:syn<zipRedac>734<zipRedac>72/files/).   

Data Download Requirements. To download the data please follow the directions on the Challenge website to register for the Challenge and complete the mandatory data use agreement. 

Leaderboard/Compute Resources. We will implement a Leaderboard for the preliminary assessment of predictive models. We will also shortly provide access to an IBM ZEC<zipRedac>2 system with 20 processors, <zipRedac>24 GB memory and with 9 TB storage space in order to support the computational need of the Challenge participants.

Publication Opportunities. We are working closely with Nature Genetics to implement a novel process of Challenge Assisted Peer Review as anticipated in Nat Genet. 20<zipRedac>3 May;45(5):468-9. Our aim is to increase transparency of peer review and help in the identification of methods deserving of publication.

Presentation at the DREAM Conference. The best performing teams will be invited to attend and give an oral presentation at the 20<zipRedac>4 RECOMB/ISCB Regulatory, Systems Genomics and DREAM conference

Discussion Forum. A community forum is available where you can post your questions about the Challenge and interact with other Challenge participants as well as Challenge organizers.  We will be posting information to the forum throughout the Challenge.  To sign up for the forum, please follow [this link:](http://support.sagebase.org/sagebase/topics/welcome_to_the_rheumatoid_arthritis_responder_challenge?rfm=<zipRedac>).

Challenge Webinar. On February 26th at 9am Pacific, we will be holding a live webinar to describe the scientific rationale behind the Challenge, a tutorial for data access, information on timelines, incentives, and leaderboards. To register for the webinar, please follow this link: https://attendee.gotowebinar.com/register/<phoneRedac>0<phoneRedac>3 

Sincerely,  
The Rheumatoid Arthritis Responder DREAM Challenge>
115:<Hi,
I'm having trouble with R code and the tsboot function.  Specifically, passing arguments to my statistic function nested within the tsboot function. Here's what I have **first:**

**se=function(data,index){
  coef(summary(lm(y~.,data=data,subset=index)))[,"Std. Error"]
}**


I get into trouble because 1) I do not know how to pass the data into this function from the tsboot function and 2) I think the index argument is (will be) a problem since tsboot samples the data (if that makes sense).  The function returns Standard Errors for the coefficients of a linear model. 

**Second:**

**tsboot(Xy[,3],se,R=10,l=100,sim="fixed")**

I get the following: *Error in terms.formula(formula, data = data) : 
  '.' in formula and no 'data' argument*

I interpret that to mean the se function is not getting the data (and next it won't get the index, but it shouldn't need the index because tsboot is sampling the data - *stream of consciousness*).  I thought about wrapping tsboot inside another function in order to pass the arguments to the se function, but that seems too complicated.

So I'm stuck.  I must be missing something simple, but what?>
116:<Here is my approximation of the left panel graphic on page <zipRedac>29 of the text. This is a close approximation, but not exact because the Left panel is not a plot of ALL data points, only a subset. You can cut and paste the code below into an R script to run. 

My solution to the sampling issue was to sample the entire dataframe by <zipRedac>/3rd. However it may be more accurate to sample the dataframe for only the rows where no default occurred (Default$default=='No');  I'm still working thru that.

    # setup your environment
    require(ISLR)
    data(Default)
    par(mfrow=c(<zipRedac>,2))
    
    ### plotting symbol reminder
    plot(c(<zipRedac>:<zipRedac>0),c(<zipRedac>:<zipRedac>0),pch=c(<zipRedac>:<zipRedac>0))
    
    # (p<zipRedac>28)
    ###  "The overall default rate is about 3%, 
    ###   _so_we_have_plotted_only_a_fraction 
    ###   of the individuals who did not default"
    
    # Therefore, need to create sample subset
    ?sample
    set.seed(42)
    subsample=sample(<zipRedac>:<zipRedac>0000, 333, replace=FALSE)
    DefaultSubset=Default[subsample,]
    
    ### create a scatterplot of subset sample of data
    colors = ifelse(DefaultSubset$default=='Yes',"orange","blue")
    symbols = ifelse(DefaultSubset$default=='Yes', 3, <zipRedac>)
    plot(DefaultSubset$balance, DefaultSubset$income, col=colors, pch=symbols, xlab="Balance",ylab="Income", main="FIGURE 4.<zipRedac> Subsetted Data")
    
    # create a scatterplot of all data in Default dataset
    plot(Default$balance, Default$income, col=colors, pch=symbols, xlab="Balance",ylab="Income", main="FIGURE 4.<zipRedac> All Data")>
117:<* I created SAS programs to duplicate the output from the R code in the Chapter 5 Lab.;
* No warranty.  :-);
* Sorry about the formatting. It appears to be automatic.;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

***** 5.3.1 The Validation Set Approach *****;
title '5.3.1 The Validation Set Approach';

* csv file created from Auto dataset (> write.csv(Auto,file="Auto.csv");
* note: need to delete 1st column in resulting csv file before importing into sas;
* note: sas imports horsepower as character;
proc import out=Auto(rename=(horsepower=horsepowerchar)) dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookAuto.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=1000;
run;

data Auto;
   set Auto;
   horsepower = horsepowerchar + 0;
   drop horsepowerchar;
   horsepowersq = horsepower**2;
   horsepowercb = horsepower**3;
run;

* randomly sample 196 obs for the training and test datasets & fit first order model to training data;
* > set.seed(1)
  > train=sample(392,196)
  > lm.fit=lm(mpg~horsepower,data=Auto,subset=train);
ods listing close;  * supress printed output;
proc glmselect data=Auto seed=1;
   title2 'Seed = 1, 1st Order Model';
   model mpg = horsepower / selection=none;  * keep all variables in the model;
   partition fraction(test=0.5);  * randomly split the data in half between training & test;
   ods output fitstatistics=FitStats1;
run;
ods listing;

* estimate MSE;
* > mean((mpg-predict(lm.fit,Auto))[-train]^2);
proc print data=FitStats1 noobs;
   where substr(Label1,1,3) in ('ASE');
run;

* repeat for quadratic model;
* > lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train);
ods listing close;
proc glmselect data=Auto seed=1;
   title2 'Seed = 1, 2nd Order Model';
   model mpg = horsepower horsepowersq / selection=none;
   partition fraction(test=0.5);
   ods output fitstatistics=FitStats2;
run;
ods listing;

* > mean((mpg-predict(lm.fit2,Auto))[-train]^2);
proc print data=FitStats2 noobs;
   where substr(Label1,1,3) in ('ASE');
run;

* repeat for cubic model;
* > lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train);
ods listing close;
proc glmselect data=Auto seed=1;
   title2 'Seed = 1, 3rd Order Model';
   model mpg = horsepower horsepowersq horsepowercb / selection=none;
   partition fraction(test=0.5);
   ods output fitstatistics=FitStats3;
run;
ods listin>
118:<* I created SAS programs to duplicate the output from the R code in the Chapter 5 Lab.;
* No warranty.  :-);
* Sorry about the formatting. It appears to be automatic.;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

***** 5.3.1 The Validation Set Approach *****;
title '5.3.1 The Validation Set Approach';

* csv file created from Auto dataset (> write.csv(Auto,file="Auto.csv");
* note: need to delete 1st column in resulting csv file before importing into sas;
* note: sas imports horsepower as character;
proc import out=Auto(rename=(horsepower=horsepowerchar)) dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookAuto.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=1000;
run;

* randomly sample 196 obs for the training and test datasets;
* > set.seed(1)
  > train=sample(392,196);
proc surveyselect data=Auto method=srs sampsize=196 rep=1 seed=1 out=AutoSelect1 outall noprint;
   * 'method=srs' specifies without replacement;
   * 'outall' option keeps all data in output dataset and adds 'selected' variable (0,1);
run;

data AutoTraining1 AutoTest1;
   set AutoSelect1;
   horsepower = horsepowerchar + 0;
   drop horsepowerchar;
   horsepowersq = horsepower**2;
   horsepowercb = horsepower**3;
   if Selected = 1 then output AutoTraining1;
     else output AutoTest1;
run;

* fit first order model to training data;
* > lm.fit=lm(mpg~horsepower,data=Auto,subset=train);
proc reg data=AutoTraining1 outest=RegEst1 noprint;
   title2 'Seed = 1, 1st Order Model';
   mpghat: model mpg = horsepower;
   * label the model 'mpghat' for naming predicted column in score outout dataset;
run;

* apply model to test data and estimate MSE;
* > mean((mpg-predict(lm.fit,Auto))[-train]^2);
proc score data=AutoTest1 score=RegEst1 type=parms out=ScoreOut1;
   var horsepower;
run;

data ScoreOut1;
   set ScoreOut1;
   SqError = (mpg - mpghat)**2;
run;

proc means data=ScoreOut1 mean;
   var SqError;
run;

* repeat for quadratic model;
* > lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train);
proc reg data=AutoTraining1 outest=RegEst2 noprint;
   title2 'Seed = 1, 2nd Order Model';
   mpghat: model mpg = horsepower horsepowersq;
run;

* > mean((mpg-predict(lm.fit2,Auto))[-train]^2);
proc score data=AutoTest1 score=RegEst2 type=parms out=ScoreOut2;
   var horsepower horsepowersq;
run;

data ScoreOut2;
   set ScoreO>
119:<* I created SAS programs to duplicate the output from the R code in the Chapter 5 Lab.;
* No warranty.  :-);
* Sorry about the formatting. It appears to be automatic.;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

***** 5.3.2 Leave-One-Out Cross-Validation *****;
title '5.3.2 Leave-One-Out Cross-Validation';

* csv file created from Auto dataset (> write.csv(Auto,file="Auto.csv");
* note: need to delete 1st column in resulting csv file before importing into sas;
* note: sas imports horsepower as character;
proc import out=Auto(rename=(horsepower=horsepowerchar)) dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookAuto.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=1000;
run;

data Auto;
   set Auto;
   horsepower = horsepowerchar + 0;
   horsepowersq = horsepower**2;
   horsepowercb = horsepower**3;
   horsepower4th = horsepower**4;
   horsepower5th = horsepower**5;
run;

* use glm to perform linear regression;
* > glm.fit=glm(mpg~horsepower,data=Auto)
  > coef(glm.fit);
proc glm data=Auto;
   title2 'GLM';
   model mpg = horsepower;
run;

* > lm.fit =lm(mpg~horsepower,data=Auto)
  > coef(lm.fit);
proc reg data=Auto;
   title2 'Regression';
   model mpg = horsepower;
run;

* fit first order model using LOOCV;
* > library=(boot)
  > glm.fit=glm(mpg~horsepower,data=Auto)
  > cv.err=cv.glm(Auto,glm.fit)
  > cv.err$delta;
proc pls data=Auto cv=one noprint;  * 'cv=one' requests one-at-a-time cross validation; 
   title2 '1st Order Model';
   model mpg = horsepower;
   output out=plsout1 press=ApproxPredResid;
run;

data plsout1;
   set plsout1;
   SqPredResid = ApproxPredResid**2;
run;

proc means data=plsout1 mean;
   var SqPredResid;
run;

* fit models using LOOCV up to order 5;
* > cv.error=rep (0,5)
  > for (i in 1:5){
  + glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
  + cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
  + }
  > cv.error;
proc pls data=Auto cv=one noprint;
   title2 '2nd Order Model';
   model mpg = horsepower horsepowersq;
   output out=plsout2 press=ApproxPredResid;
run;

proc pls data=Auto cv=one noprint;
   title2 '3rd Order Model';
   model mpg = horsepower horsepowersq horsepowercb;
   output out=plsout3 press=ApproxPredResid;
run;

proc pls data=Auto cv=one noprint;
   title2 '4th Order Model';
   model mpg = horsepower horsepowersq horsepowercb horsepower4th;
>
120:<* I created SAS programs to duplicate the output from the R code in the Chapter 5 Lab.;
* No warranty.  :-);
* Sorry about the formatting. It appears to be automatic.;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

***** 5.3.3 k-Fold Cross-Validation *****;
title '5.3.3 k-Fold Cross-Validation';

* csv file created from Auto dataset (> write.csv(Auto,file="Auto.csv");
* note: need to delete 1st column in resulting csv file before importing into sas;
* note: sas imports horsepower as character;
proc import out=Auto(rename=(horsepower=horsepowerchar)) dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookAuto.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=1000;
run;

* > set.seed(17)
  > cv.error.10= rep(0,10)
  > for(i in 1:10){
  + glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
  + cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
  + }
  > cv.error .10;

data Auto;
   set Auto;
   horsepower1 = horsepowerchar + 0;  * convert horsepower to numeric;
   * create horsepower squared, cubed ... 10th;
   array h{9} horsepower2-horsepower10;
   do j = 2 to 10;
      h{j-1} = horsepower1**j;
   end;
   drop j;
   fold = ranuni(1);  * needed to create folds using proc rank below;
run;

* create dataset for k=10;
* note: use of proc surveymeans with rep option will create datasets for RANDOM k-fold cross validation;
proc rank data=Auto out=AutoRanked groups=10; * divides data into k=10 folds;
   var fold;  * fold is assigned a value from 0 to 9;
run;

%macro KFCV;  * macro to loop thru each of the k=10 validation datasets and  increasing complex polynomial fits;

%do PolyOrder = 1 %to 10;  * loop thru increasing complex polynomial fits;

data MSEs&PolyOrder;  * blank dataset to append MSEs (from each of the k=10 validation datasets) to;
run;

   %do mfold = 0 %to 9;  * loop thru each of the k=10 validation (held-out) datasets;

* ensure that data from a previous loop is not used;
proc datasets memtype=data library=work; 
   save AutoRanked MSEs&PolyOrder;
run;

* fit model on non-held-out folds;
proc reg data=AutoRanked outest=RegEst noprint;
   where fold ne &mfold;
   title2 "Order &PolyOrder Model";
   mpghat: model mpg = %do i = 1 %to &PolyOrder; horsepower&i %end;;  * loop adds higher order variables each time thru;
   * label the model 'mpghat' for naming predicted column in score outout da>
121:<* I created SAS programs to duplicate the output from the R code in the Chapter 5 Lab.;
* No warranty.  :-);
* Sorry about the formatting. It appears to be automatic.;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

***** 5.3.4 The Bootstrap *****;
title '5.3.4 The Bootstrap';

* csv file created from Portfolio dataset (> write.csv(Portfolio,file="Portfolio.csv");
* note: need to add "index" to 1st row in resulting csv file before importing into sas;
proc import out=Portfolio dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookPortfolio.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=1000;
run;

* > alpha.fn=function (data ,index){
  + X=data$X [index]
  + Y=data$Y [index]
  + return ((var(Y)-cov (X,Y))/(var(X)+var(Y) -2* cov(X,Y)))
  + };
* > alpha.fn(Portfolio ,1:100);
* get variances & covariances;
ods listing close;
proc corr data=Portfolio cov;
   var X Y;
   ods output cov=CovMatrix1;
run;
ods listing;

* calculate alpha;
data Alphas1(keep=alpha where=(alpha ne .));
   set CovMatrix1;
   lagX = lag(X);  * lag function cant be used after a if-then;
   if Variable = 'Y' then do;
      VarX = lagX; VarY = Y; CovXY = X;
      alpha = (VarY - CovXY)/((VarX + VarY) - (2*CovXY));
   end;
run;

proc print data=Alphas1 noobs;
   title2 'Portfolio Data: All Data';
run;

* > set.seed(1)
  > alpha.fn(Portfolio,sample(100,100,replace=T));
* create sample of 100 obs w/ replacement;
proc surveyselect data=Portfolio method=urs sampsize=100 rep=1 seed=1 out=PortfolioSelect2 noprint;
   * 'method=urs' specifies with replacement and adds 'NumberHits' variable which MUST BE USED IN ALL SUBSEQUENT PROCS;
run;

ods listing close;
proc corr data=PortfolioSelect2 cov;
   var X Y;
   freq NumberHits;  * MUST BE USED IN ALL PROCS;
   ods output cov=CovMatrix2;
run;
ods listing;

* calculate alpha;
data Alphas2(keep=alpha where=(alpha ne .));
   set CovMatrix2;
   lagX = lag(X);  * lag function cant be used after a if-then;
   if Variable = 'Y' then do;
      VarX = lagX; VarY = Y; CovXY = X;
      alpha = (VarY - CovXY)/((VarX + VarY) - (2*CovXY));
   end;
run;

proc print data=Alphas2 noobs;
   title2 'Portfolio Data: Single Bootstrap Data';
run;

* > boot(Portfolio,alpha.fn,R=1000);
* sample 100 ods w/ replacement 1000 times;
proc surveyselect data=Portfolio method=urs sampsize=100 rep=1000 seed=1 out=P>
122:<completely agree @jjones123 and too felt the quizzes are much more lighter and less selective than other courses (c'mon... 5 tries for EACH question instead than 3 tries max for a whole set... that is in my vocabulary considered easy) and quite on spot.

I'm really enjoying the course, it is really well built without sucking too much time (I need to work for a living ;): it provides a conceptual "backbone" and you can easily go to more depth if needed.

good work @ profs and crew>
123:<Hello everyone,
I solve all the question, and went step by step this section, and I got std.Error <zipRedac>.<zipRedac>139, and it was wrong!
These are my codes.
any idea what is wrong with it?
till now I solved all the questions except this one!
and I spend lots of time to complete this course 1<zipRedac><zipRedac>% :D

alpha=function(X1,y){
+ vX1=var(X1)
+ vy=var(y)
+ cX1y=cov(X1,y)
+ (vy-cX1y)/(vX1+vy-2*cX1y)
+ }
> alpha(Xy$X1,Xy$y)
[1] <zipRedac>.<phoneRedac>
> alpha.fn=function(data,index){
+   
+ + with(data[index,],alpha(X,Y))
+ }
> alpha.fn(Xy,1:1<zipRedac><zipRedac>)
 Show Traceback
 
 Rerun with Debug
 Error in is.data.frame(x) : object 'X' not found > alpha.fn=function(data,index){
+   
+ with(data[index,],alpha(X1,y))
+ }
> alpha.fn(Xy,1:1<zipRedac><zipRedac>)
[1] <zipRedac>.<phoneRedac>
> boot.out=boot(Xy,alpha.fn,R=1<zipRedac><zipRedac><zipRedac>)
> boot.out

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = Xy, statistic = alpha.fn, R = 1<zipRedac><zipRedac><zipRedac>)


Bootstrap Statistics :
     original        bias    std. error
t1* <zipRedac>.<phoneRedac> -<zipRedac>.<zipRedac><zipRedac><zipRedac><phoneRedac>  <zipRedac>.<zipRedac>14<zipRedac>4689
> set.seed(1)
> boot.out

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = Xy, statistic = alpha.fn, R = 1<zipRedac><zipRedac><zipRedac>)


Bootstrap Statistics :
     original        bias    std. error
t1* <zipRedac>.<phoneRedac> -<zipRedac>.<zipRedac><zipRedac><zipRedac><phoneRedac>  <zipRedac>.<zipRedac>14<zipRedac>4689
> set.seed(1)
> alpha.fn(Xy,sample(1:1<zipRedac><zipRedac>,1<zipRedac><zipRedac>,replace=TRUE))
[1] <zipRedac>.<phoneRedac>
> boot.out=boot(Xy,alpha.fn,R=1<zipRedac><zipRedac><zipRedac>)
> boot.out

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = Xy, statistic = alpha.fn, R = 1<zipRedac><zipRedac><zipRedac>)


Bootstrap Statistics :
     original       bias    std. error
t1* <zipRedac>.<phoneRedac> <zipRedac>.<zipRedac><zipRedac><zipRedac><phoneRedac>  <zipRedac>.<zipRedac>1397327
> alpha.fn(Xy,sample(1:1<zipRedac><zipRedac><zipRedac>,1<zipRedac><zipRedac><zipRedac>,replace=TRUE))
[1] <zipRedac>.<phoneRedac>
> boot.out

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = Xy, statistic = alpha.fn, R = 1<zipRedac><zipRedac><zipRedac>)


Bootstrap Statistics :
     original       bias    std. error
t1* <zipRedac>.<phoneRedac> <zipRedac>.<zipRedac><zipRedac><zipRedac><phoneRedac>  <zipRedac>.<zipRedac>1397327
> plot(boot.out)>
124:<Staff EDIT:  This post is misleading because of an error, see Staff response below for more.

@alhf:

I know your alien abduction example was a made up one. Still, I can't resist to draw your attention to the fact that Frequentists (under whose domain p-value falls) would object to model the event that {Prof. Hastie has been abducted by aliens}. For them, the probability of that is either 0 or 1, i.e either he has been abducted or not. The event is not something which can be trialed/experimented/repeated under identical conditions a lot of times.

Of course, Bayesians would take a different course :-)

BTW, good overall explanation!>
125:<@sirahman:

> @<nameRedac_anon_screen_name_redacted>:
> 
> I know your alien abduction example was a made up
> one. 

I hope so :-)

> Still, I can't resist to draw your attention to the
> fact that Frequentists (under whose domain p-value falls)
> would object to model the event that {Prof. Hastie has been
> abducted by aliens}. For them, the probability of that is
> either 0 or 1, i.e either he has been abducted or not. The
> event is not something which can be
> trialed/experimented/repeated under identical conditions a
> lot of times.

You're quite right of course.  

In setting up the scenario, I tried to be pretty careful not
to mention anything about the probability of abduction.

I just put up "non-abduction" as a null hypothesis which is
OK for a frequentist.  As you say, the hypothesis is either
true or not  there is no probability associated with it.

But I guess using wording like "evidence ... abducted by
aliens" is slipping into a Bayesian frame of mind.

And when I said

> > In particular, does it mean that there's only a 4 percent
> > chance the null is true ?
> > 
> > No.

The answer *is* "no".

But, for the frequentist, this is *not*
because the numerical value is incorrect (as a Bayesian
might argue).

It's because the very idea of the "chance the null is true"
is not a meaningful idea.[*]

> Of course, Bayesians would take a different course :-)

I imagine they'd say that the sentence above (*), although
being part of a consistent position, is not very helpful :-)


By the end of the story, with the mention of "priors" I
guess I'm being blatently Bayesian.

> BTW, good overall explanation!

Thanks.

And thanks for making the frequentist point clearer.

It's been interesting to reflect on my original wording in
light of your comment.>
126:<@alhf:

> I hope so :-)

I can't imagine Dear Hastie giving lecture on LASSO to Martians:-) Let we earthlings continue to "suffer", rather than our poor cousins somewhere out there:-)

> And thanks for making the frequentist point clearer.

My pleasure.

> Of course, Bayesians would take a different course :-)

I guess, they will translate their "degree of belief" on the event by putting a U(0, 1) prior on it, then update it with the observed data (Prof. Hastie's postings on the forum) and arrive at some posterior distribution from which they can answer questions like Pr{Prof. Hastie has not been abducted by aliens}.

Different priors will yield different results, of course:-). But that is another story.

@JulioKuplinsky:

> For hypothesis testing you need random variables.

Very well put. In one single sentence, you have captured the essence of my post.

To summarise, this thread brings to light the point that in Statistics how much more important it is to frame the research question/problem, give the context, then  decide on the appropriate course of action depending on the formulated Q in the given setting, than to just jump to analyse the data.

Good discussion, mates. I think, OP has had enough!>
127:<1) In prediction, maybe is not a good idea to guide your decision exclusively by MSEP. Simplicity of the model (interpretability) and results are great and complementary guides. The use of a model (results) gives you a very good sense about its quality and provides data to post-evaluation.

2) I think so, but taking in account my response to question 1... :)

In general, modeling for prediction, I prefer to combine (average) few methods (different methodology) than to use exclusively the method having the best MSEP.>
128:<I wrote code to calculate the LOOCV error for this problem, then checked it out by using the cv.glm() function.  They give dramatically different answers (my code = 0.45, cv.glm = 0.25), but I can't see anything wrong with my code.  Can anybody help?

    library(ISLR)
    
    #initialize mistakes counter
    wrong = 0;
    #loop thru the cases
    for (i in 1:nrow(Weekly)){
       #fit a glm for all but case i
       butone=glm(Direction~Lag1+Lag2,data=Weekly[-i,],family=binomial)
       #use this glm to calc a probability of point i
       probone=predict(butone,newdata=Weekly[i,],type="response")
       #predict point i = Up if probability > .5
       classone=ifelse(probone>0.5,"Up","Down")
       #calculate 1 if predicted class does not equal observed
       err=ifelse(classone==Weekly$Direction[i],0,1)
       #cumulate errors
       wrong= wrong + err
       }
    # calculate average error rate
    rate=wrong/nrow(Weekly)
    rate
    
    library(boot)
    glmfit=glm(Direction~Lag1+Lag2,data=Weekly,family=binomial)
    cv.err=cv.glm(Weekly,glmfit)
    cv.err$delta>
129:<Hi, I'm trying to solve question 5.R.R3, in a similar way as lesson:

alpha<-function(d){
glm(y~X1+X2,data=d)}

alpha.fn<-function(data,index){
with(data[index,],alpha(data))
}

boot.out<-boot(Xy,alpha.fn,R=1000)

And i get this error: Error in boot(Xy, alpha.fn, R = 1000) : 
  incorrect number of subscripts on matrix

I can't figure out what's happening here. Any suggestions?

Thanks!!>
130:<Hi,

Thanks for your extensive answer :)
But OP hasn't had enough just yet.

RE: Firstly

Sure, that also makes sense when seen from a purely logical perspective.
$H_0 implies Observation$
Now, when $P(Observation)$ is not low, i.e., not likely false, then we can't conclude anything.

Re: Secondly

I don't think that's all what they wanted to say. I quote:

> One result is an abundance of confusion about what the P value means.
> Consider Motyl's study about political extremists. Most scientists
> would look at his original P value of 0.01 and say that there was just
> a 1% chance of his result being a false alarm.
>  The P value cannot say this: all it can do is summarize the data
> assuming a specific null hypothesis. It cannot work backwards and make
> statements about the underlying reality. ***That requires another piece
> of information: the odds that a real effect was there in the first
> place.*** To ignore this would be like waking up with a headache and
> concluding that you have a rare brain tumour  possible, but so
> unlikely that it requires a lot more evidence to supersede an everyday
> explanation such as an allergic reaction. The more implausible the
> hypothesis  telepathy, aliens, homeopathy  the greater the chance
> that an exciting finding is a false alarm, no matter what the P value
> is.
> 
> 
> Challenges in irreproducible research These are sticky
> concepts, but some statisticians have tried to provide general
> rule-of-thumb conversions (see 'Probable cause'). According to one
> widely used calculation5, a P value of 0.01 corresponds to a
> false-alarm probability of at least 11%, depending on the underlying
> probability that there is a true effect; a P value of 0.05 raises that
> chance to at least 29%.

And as an aside:

> Critics also bemoan the way that P values can encourage muddled
> thinking. A prime example is their tendency to deflect attention from
> the actual size of an effect. Last year, for example, a study of more
> than 19,000 people showed8 that those who meet their spouses online
> are less likely to divorce (p < 0.002) and more likely to have high
> marital satisfaction (p < 0.001) than those who meet offline (see
> Nature http://doi.org/rcg; 2013). That might have sounded impressive,
> but the effects were actually tiny: meeting online nudged the divorce
> rate from 7.67% down to 5.96%, and barely budged happiness from 5.48
> to 5.64 on a 7-point scale. To pounce on tiny P values and ignore the
> larger que>
131:<I think that the "with" is messing you up in this case, the alpha function is expecting the entire data frame, using with lets you access the columns within the data frame.

Also the boot function is expecting a vector as the return value, your alpha function returns an object, not just a vector, that is what is generating the error.  

The following modifications work for me:

    alpha<-function(d){ coef(glm(y~X1+X2,data=d))}
    alpha.fn<-function(data,index){ alpha(data[index,]) }
    boot.out<-boot(Xy,alpha.fn,R=1000)>
132:<I think the course works well. Yes it's more 'mathy' than some, but it's good to have a mix, and it's not too much. Teaching the R at the same time as the theory would be good.

And kiwis? lol :)>
133:<Mark79,

>The only difference is that y_hat is estimated with more data in the case of LOOCV. 
This makes sense. If you sum different numbers there is a possibility that you will get the different answer.

Also you did not answer about formula from page 12 where the leverage is considered. So it seems you mean only the formula from page 11. Ok, lets consider this case and the function
y = 0 for x = [1,..,10]
y = 1 for x = [11,..,20]
y = 0 for x = [21,..,30]
y = 1 for x = [31,..,40]
y = 0 for x = [41,..,50]

You can try the code below running it in Octave or Matlab if you would like to. It's very straightforward. 

Here are the CVs and metrics for y_hat_i: 
CV for 5-folds : 0.5298
[5-folds] y_hat_i : mean 0.4406, median 0.3420, min 0.1096, max 1.0535, std 0.3433

CV for N-folds : 0.2589
[N-folds] y_hat_i : mean 0.4043, median 0.4084, min 0.3749, max 0.4337, std 0.0204

There is a difference in 2 times. Because of reasons I mentioned in the previous post. I did not tell that I can proof that "the test error estimate using LOOCV is exactly equal to the variance of the test error estaimate using five- or tenfold averaging". This is your point. Could you please show your analytical proof if you have already done this. If your proof that you have changed K to N this is not enough. You should show that sums are equal. Perhaps you need to consider formulas for y_hat_i also. 



clc; close all; clear all;

MULT = 10;
x1 = 1:1*MULT;
x2 = 1*MULT+1:2*MULT;
x3 = 2*MULT+1:3*MULT;
x4 = 3*MULT+1:4*MULT;
x5 = 4*MULT+1:5*MULT;

y = zeros(1,5*MULT);
y(x1) = 0;
y(x2) = 1;
y(x3) = 0;
y(x4) = 1;
y(x5) = 0;

n = length([x1 x2 x3 x4 x5]);


nk = length(x1); % for 5-fold
cvs = [];
mses = [];
yhatis = [];
for K = 1:4

    if K == 1, x = [x2, x3, x4, x5]; xtest = x1;
    elseif K == 2, x = [x1, x3, x4, x5]; xtest = x2;
    elseif K == 3, x = [x1, x2, x4, x5]; xtest = x3;
    elseif K == 4, x = [x1, x2, x3, x5]; xtest = x4;
    elseif K == 5, x = [x1, x2, x3, x4]; xtest = x5;
    end

    ylearn = y(x);
    ytest = y(xtest);

    p = polyfit(x,ylearn,1);

    fprintf('K = %d
', K);
    fprintf('X : %d, ', x);
    fprintf('
');
    fprintf('Y : %d, ', ylearn);
    fprintf('
Y = %.4f*X + %.4f
', p(1), p(2));
    

    mse = 0;
    for i = 1:length(xtest)
        xi = xtest(i);
        yi = ytest(i);
        yhati = polyval(p, xi);
        yhatis = [yhatis; yhati;];
        mse = mse + (yi-yhati)*(yi-yhati)/nk;
    end
    mses = [mses; mse;];
    
end

CV = sum(mses)*nk/n;
fprintf('


>
134:<@smgross:

I did not say Pr{Hypothesis|Data}=p-value=0 or 1. I am aware that p-value=Pr{Data|Hypothesis}. That is why in my second post I give a outline about how would a Bayesian typically go about finding Pr{Hypotheis|Data}.>
135:<I managed to get the right answer, via a very simple implementation of tsboot.

This is my code for the standard bootstrap (5.R.R3)

set.seed(1)

bs <- function(formula, data, indices) {

  d <- data[indices,] 

  fit <- lm(formula, data=d)

  return(coef(fit))

}

results <- boot(data=Xy, statistic=bs,
   R=1000, formula=y~X1+X2)

results

Note that this gives you all of the coefficiencts (the X1 coefficient is called t2; t1 is the intercept).  You can pull out just the coefficient for X1 by putting "summary(fit)$coef[2]" in the return() line above, in place of "coef(fit)" 

And all I needed to do for 5.R.R4 was change boot() to tsboot(), making the requisite changes inside the brackets to match what that function wants:
set.seed(1)

bs2 <- function(formula, data, indices) {

  d <- data[indices,] 

  fit <- lm(formula, data=d)

  return(coef(fit))

}

results2 <- tsboot(tseries=Xy, statistic=bs2,
   R=1000, l=100, sim="fixed",formula=y~X1+X2)

results2

The only real difference between boot() and tsboot() is that you call the data "tseries" in tsboot, and you need to specify the type of block (sim="fixed") and the length of the block ("l=100").

This is much simpler that Liyu's code above, and thank heavens, because I couldn't code my way out of a wet paper bag, and was having a great deal of trouble following it.  I don't know what grantbrown used within tsboot, but could the problem be that you were missing set.seed(1) at the top?  I missed it the first try, and got the wrong result.>
136:<For time series data, you should not use the ordinary bootstrap, because unlike cross-sectional data, order of the observations is very important here. That is why block bootstrapping is used as at least you will get something like a week or fortnight or month etc. (depending on the length of the block and *n*) **together as a block** in your **resampled data** assuming your original time series was a daily one. Otherwise due to Monte Carlo sampling, observations will go haywire.

As an aside, you could have framed the Question without giving the numbers:-).>
137:<If you're interested, I looked for a solution manual for ESL, and came across this one: [http://waxworksmath.com/Authors/G_M/Hastie/hastie.html][1]



  [1]: http://waxworksmath.com/Authors/G_M/Hastie/hastie.html

At first, I was stumped looking at the exercises. Then when I looked at the answers, I got even more confused !!!!!!! No wonder I couldn't figure them out or even know where to start, it's must be some high level of math. :p>
138:<Thank you everyone for your posts. It has helped me greatly. I also got to thinking why is this an important question for us to answer. I think this question would like us to think about how many times we resample the sample data set is enough times to estimate the standard error of a coefficient and thus a confidence interval. I have two thoughts come to mind. The first is how to trade of computational cost with accuracy. *Quantum Leap* is an article in the February 17, 2014 issue of Time magazine. The article discusses adiabatic quantum computing. If this comes to fruition, the issue of computational cost may be mute. For now, though, I'm thinking we will learn more of how to do this next. I am also thinking we are beginning to delve into the issues with *Big Data*. The lecture gave us some insight in that one example included 5,000 predictors and 50 observations. Working as a "traditional" statistician, this problem would not be doable using methods we've discussed thus far. Traditionally, I would have asked for more observations and would have required the client to provide a small number of predictors based on the physics backing the problem. I may become a Data Scientist yet :)>
139:<I have compared your results to mine.. They were similar .. Nevertheless, 5.R.R.3 was giving me a correct answer, but it was not the case with 5.R.R.4. I am not sure what is the problem .. Is it **with** and **summary** that make it so ? Hmmph or the  approach is incorrect .. I hope who got it right answers =)>
140:<I found it means the tolerable error in my answer (might be caused by different random seed setting or different number of bootstrapping iterations used by students and the teachers).

Anyway, "to within xx%" is a little bit difficult for a non-English speaking student to understand. :)>
141:<Ch1 Introduction

1.1 Opening Remarks
(https://www.youtube.com/watch?v=2wLfFB_6SKI)
1.2 Examples and Framework
(https://www.youtube.com/watch?v=LvaTokhYnDw)

Ch 02 Overview of Statistical Learning

Introduction to Regression Model
(https://www.youtube.com/watch?v=WjyuiK5taS8)
2.2 Dimensionality and Structured Models
(https://www.youtube.com/watch?v=UvxHOkYQl8g)
2.3 Model Selection and Bias-Variance Tradeoff
(https://www.youtube.com/watch?v=VusKAosxxyk)
2.4 Classification
(https://www.youtube.com/watch?v=vVj2itVNku4)
2.R Introduction to R
(https://www.youtube.com/watch?v=jwBgGS_4RQA)
Interview with John Chambers
(https://www.youtube.com/watch?v=jk9S3RTAl38)

Ch 03 Linear Regression

3.1 Simple Linear Regression
(https://www.youtube.com/watch?v=PsE9UqoWtS4)
3.2 Hypothesis Testing and Interval Confidence
(https://www.youtube.com/watch?v=J6AdoiNUyWI)
3.3 Multiple Linear Regression
(https://www.youtube.com/watch?v=1hbCJyM9ccs)
3.4 Some Important Questions
(https://www.youtube.com/watch?v=3T6RXmIHbJ4)
3.5 Extensions of the linear models
(https://www.youtube.com/watch?v=IFzVxLv0TKQ)
3.R Linear Regression in R
(https://www.youtube.com/watch?v=5ONFqIk3RFg)

Ch4 Classification

4.1 Introduction to Classification Problems
(https://www.youtube.com/watch?v=sqq21-VIa1c)
4.2 Logistic Regression
(https://www.youtube.com/watch?v=31Q5FGRnxt4)
4.3 Multivariate Logistic Regression
(https://www.youtube.com/watch?v=MpX8rVv_u4E)
4.4 Logistic Regression - Case Control Sampling and Multiclass
(https://www.youtube.com/watch?v=GavRXXEHGqU)
4.5 Discriminant Analysis
(https://www.youtube.com/watch?v=RfrGiG1Hm3M)
4.6 Gaussian Discriminant Analysis - One Variable
(https://www.youtube.com/watch?v=QG0pVJXT6EU)
4.7 Gaussian Discriminant Analysis - Many Variable
(https://www.youtube.com/watch?v=X4VDZDp2vqw)
4.8 Quadratic Discriminant Analysis and Naive Bayes
(https://www.youtube.com/watch?v=6FiNGTYAOAA)
4.R Classification in R
(https://www.youtube.com/watch?v=TxvEVc8YNlU)

Ch05 Resampling Methods

Interview with Bradley Efron
(https://www.youtube.com/watch?v=6l9V1sINzhE)
5.1 Cross-Validation
(https://www.youtube.com/watch?v=_2ij6eaaSl0)
5.2 K-fold Cross-Validation
(https://www.youtube.com/watch?v=nZAM5OXrktY)
5.3 Cross-Validation: the wong and right way
(https://www.youtube.com/watch?v=S06JpVoNaA0)
5.4 The Bootstrap
(https://www.youtube.com/watch?v=p4BYWX7PTBM)
5.5 More on the Bootstrap
(https://www.youtube.com/watch?v=BzHz0J9a6k0)
5.R Resampling in R
(https://www.youtube.com/watch?v=6dSXlq>
142:<I ran simple bootstrapping and then block bootstrapping on the Portfolio data available with the ISLR package (R). Although the value of alpha came out same in both cases, I am getting a difference in standard errors (0.0<phoneRedac> for the simple case, 0.<zipRedac>068888 for the 2nd case). I thought they should be almost identical in this particular case. 

I have couple of questions:
<zipRedac>. Is this difference can be considered significant or not ? 
2. If we apply block bootstrapping to a data sets which doesn't look like a time series, how much difference should we expect from the simple bootstrapping ? 

I must warn you that my background is in Algebra (not applied sciences). So to me any difference is significant (hence the source of the question).:)

Thanks.>
143:<***Ch01 Introduction***

1.1 Opening Remarks
([https://www.youtube.com/watch?v=2wLfFB_6SKI][1])

1.2 Examples and Framework
([https://www.youtube.com/watch?v=LvaTokhYnDw][2])


***Ch02 Overview of Statistical Learning***

Introduction to Regression Model
([https://www.youtube.com/watch?v=WjyuiK5taS8][3])

2.2 Dimensionality and Structured Models
([https://www.youtube.com/watch?v=UvxHOkYQl8g][4])

2.3 Model Selection and Bias-Variance Tradeoff
([https://www.youtube.com/watch?v=VusKAosxxyk][5])

2.4 Classification
([https://www.youtube.com/watch?v=vVj2itVNku4][6])

2.R Introduction to R
([https://www.youtube.com/watch?v=jwBgGS_4RQA][7])

Interview with John Chambers
([https://www.youtube.com/watch?v=jk9S3RTAl38][8])


***Ch03 Linear Regression***

3.1 Simple Linear Regression
([https://www.youtube.com/watch?v=PsE9UqoWtS4][9])

3.2 Hypothesis Testing and Interval Confidence
([https://www.youtube.com/watch?v=J6AdoiNUyWI][10])

3.3 Multiple Linear Regression
([https://www.youtube.com/watch?v=1hbCJyM9ccs][11])

3.4 Some Important Questions
([https://www.youtube.com/watch?v=3T6RXmIHbJ4][12])

3.5 Extensions of the linear models
([https://www.youtube.com/watch?v=IFzVxLv0TKQ][13])

3.R Linear Regression in R
([https://www.youtube.com/watch?v=5ONFqIk3RFg][14])


***Ch04 Classification***

4.1 Introduction to Classification Problems
([https://www.youtube.com/watch?v=sqq21-VIa1c][15])

4.2 Logistic Regression
([https://www.youtube.com/watch?v=31Q5FGRnxt4][16])

4.3 Multivariate Logistic Regression
([https://www.youtube.com/watch?v=MpX8rVv_u4E][17])

4.4 Logistic Regression - Case Control Sampling and Multiclass
([https://www.youtube.com/watch?v=GavRXXEHGqU][18])

4.5 Discriminant Analysis
([https://www.youtube.com/watch?v=RfrGiG1Hm3M][19])

4.6 Gaussian Discriminant Analysis - One Variable
([https://www.youtube.com/watch?v=QG0pVJXT6EU][20])

4.7 Gaussian Discriminant Analysis - Many Variable
([https://www.youtube.com/watch?v=X4VDZDp2vqw][21])

4.8 Quadratic Discriminant Analysis and Naive Bayes
([https://www.youtube.com/watch?v=6FiNGTYAOAA][22])

4.R Classification in R
([https://www.youtube.com/watch?v=TxvEVc8YNlU][23])


***Ch05 Resampling Methods***

Interview with Bradley Efron
([https://www.youtube.com/watch?v=6l9V1sINzhE][24])

5.1 Cross-Validation
([https://www.youtube.com/watch?v=_2ij6eaaSl0][25])

5.2 K-fold Cross-Validation
([https://www.youtube.com/watch?v=nZAM5OXrktY][26])

5.3 Cross-Validation: the wong and right way
([https://www.youtube.com/watch?v=S06J>
144:<hi, may someone shed some lights on this R code:

     predict(fit1,data.frame(lstat=c(5,10,15)),interval="confidence")

what does `data.frame(lstat=c(5,10,15)` mean ?

When I try

    > test1=data.frame(lstat=c(5,10,15))
    > test1
      lstat
    1     5
    2    10
    3    15

Thanks>
145:<Hi,

I am <nameRedac_anon_screen_name_redacted>, from Geneva and live Zurich in Switzerland. I just graduated in mathematics and now am a graduate trainee in a hedge fund.

Thanks a lot for the teaching :-)

Cheers,>
146:<Hi!

What are you doing in Geneva? :-)

Cheers>
147:<Error in UseMethod("predict") : 
  no applicable method for 'predict' applied to an object of class "regsubsets"

I get this when I run:
set.seed(11)
folds=sample(rep(1:10,length=nrow(Hitters)))
folds
table(folds)
cv.errors=matrix(NA,10,19)
for(k in 1:10){
  best.fit=regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax=19,method="forward")
  for(i in 1:19){
    pred=predict(best.fit,Hitters[folds==k,],id=i)
    cv.errors[k,i]=mean( (Hitters$Salary[folds==k]-pred)^2)
  }
}
which is directly copy pasted from the R script/Rmd>
148:<I was really enjoying the lectures with Rob and Daniela when I idly googled Prof. Witten and discovered she is the daughter of Edward Witten.  As a former math student I was acquainted with the name.  They are probably tired of hearing it, but for those who don't know, Edward Witten is one of the great math/physics super geniuses of our time.  Just thought I'd pass that along.  :)>
149:<I guess "null" models are not counted. :D.>
150:<alpha=function(formula,data,indices){
  d<-data[indices,]  
  fit<-lm(formula,data=d)  
  return(fit$coef[2])  
}


result<-boot(data=Xy,statistic=alpha,R=100,formula=y~X1+X2)>
151:<alpha=function(formula,data,indices){
  
  d<-data[indices,]  
  fit<-lm(formula,data=d)  
  return(fit$coef[2])  
}


result<-tsboot(tseries=Xy,statistic=alpha,R=100,l=100,formula=y~X1+X2,sim = "fixed")>
152:<Here are some rough ideas.

----  

This should do the scatter plot matrix, similar to lecture 4, slide 29.

    plot(~., data=iris[,-5], col=iris$Species)

----  
    
This gives a linear discriminant plot like in lecture 4,
slide 30.  However, the points are plotted with text labels
instead of coloured circles. Also the first linear
discriminant component, LD1, seems to be going in the
opposite direction to what's on the slide.  (That doesn't
really matter because the direction is rather arbitrary anyway)

    require(MASS)
    lda.fit <- lda(Species ~ ., data=iris)
    plot(lda.fit)

----  
    
If we want coloured circles, I guess we'd have to try to get
at the linear discriminant numbers and do it for ourselves.

This is a rough guess, but the scales of the linear
discriminants don't seem to be quite right.

    discr <- as.matrix(iris[,-5]) %*% as.matrix(lda.fit$scaling)
    discrim.data <- cbind(Species=iris$Species, discr)
    plot(LD2 ~ LD1, col=Species, data=discrim.data)

A better answer would come from someone who understands
exactly what the different parts the 'lda' object are.

---->
153:<I rebooted RStudio and the problem went away.   Never mind!   :-)>
154:<Awesome to hear that! :)>
155:<I did a simulation with <zipRedac> response variable and <zipRedac>00 explanatory variables. I created a true model with 3 explanatory variables. A stepwise regression with AIC chose many noise variables and the true variables. A stepwise regression with BIC chose one noise variable and the true variables. Therefore, it appears that BIC outperforms AIC when you have many noise variables.

    ############## simulate training data
    
    df <- matrix(NA, nrow=<zipRedac>0000, ncol=<zipRedac>00)
    
    set.seed(<zipRedac>)
    
    for (i in <zipRedac>:ncol(df)){
      df[,i] <- rnorm(n=nrow(df), mean=i, sd=i/<zipRedac>0)
    }
    
    df <- as.data.frame(df)
    
    ############## simulate test data
    
    df.test <- matrix(NA, nrow=<zipRedac>0000, ncol=<zipRedac>00)
    
    set.seed(2)
    
    for (i in <zipRedac>:ncol(df.test)){
      df.test[,i] <- rnorm(n=nrow(df.test), mean=i, sd=i/<zipRedac>0)
    }
    
    df.test <- as.data.frame(df.test)
    
    ####### TRUE MODEL: Linear with 3 explanatory variables #######
    
    df$y <- with(df, 50 + -2*V<zipRedac> - 0.2*V50 + 0.25*V<zipRedac>00 + rnorm(nrow(df), mean=0, sd=2)) 
    
    df.test$ytest <- with(df.test, 50 + -2*V<zipRedac> - 0.2*V50 + 0.25*V<zipRedac>00 + rnorm(nrow(df), mean=0, sd=2))
    
    ######################################################################
    
    
    
    ###############  fit the true model
    
    reg.fit <- lm(y~V<zipRedac>+V50+V<zipRedac>00, df) # two true variables
    
    summary(reg.fit)
    
    reg.pred <- predict(reg.fit, newdata=df.test)
    
    MSE_null <- mean((mean(df.test$ytest) - df.test$ytest)^2)
    
    MSE_true <- mean((reg.pred - df.test$ytest)^2)
    
    R2_true <- (MSE_null - MSE_true) / MSE_null
    
    R2_true
    
    ############### put all variables into stepwise using AIC and BIC penalties
    
    library(MASS)
    
    reg.full <- lm(y~., df)
    
    reg.step.aic <- stepAIC(reg.full, direction="both", trace=0) # Impose AIC crit
    
    reg.step.bic <- stepAIC(reg.full, direction="both", trace=0, k=log(nrow(df))) # Impose BIC crit
    
    summary(reg.step.aic)
    summary(reg.step.bic)
    
    reg.aic.pred <- predict(reg.step.aic, newdata=df.test)
    reg.bic.pred <- predict(reg.step.bic, newdata=df.test)
    
    MSE_aic <- mean((reg.aic.pred - df.test$ytest)^2)
    MSE_bic <- mean((reg.bic.pred - df.test$ytest)^2)
    
    R2_aic <- (MSE_null - MSE_aic) / MSE_null
      
    R2_bic <- (MSE_null - MSE_bic) / MSE_null
 >
156:<will this course be offered again this year ?
I am unable to finish this right now as I have exams coming up. 

I would love for it to be offered again. Say, July ;)

Thanks.>
157:<"Obviously not right" is wrong. :-)

Look at the equations on slide 6.>
158:<I found the Lasso picture (slide 6.37 or Figure 6.7 on page <zipRedac><zipRedac><zipRedac>) illuminating.  Having seen that, the question that occurred was: what if the geometry was changed to be even more extreme than the diamond?  We have the constraints:  
Ridge Regression: $$Sigma_{j=<zipRedac>}^{p} 
eta_j^<zipRedac> le s$$
Lasso: $$Sigma_{j=<zipRedac>}^{p} 
|eta_j| le s$$
What if we were to do something like: $$Sigma_{j=<zipRedac>}^{p} 
log(|eta_j|+<zipRedac>) le s$$
For those who find visualization helpful (like me :) here is an example contour from [Wolfram Alpha][<zipRedac>]:  
![Example contour][<zipRedac>]

My first observation is that the curve is no longer convex so I believe (perhaps I should't have stopped following Stephen Boyd's [CVX<zipRedac>0<zipRedac>][3]) we lose the nice convex optimization property of the Lasso and Ridge Regression.  Thus this idea is of at best intellectual interest only.  A geometrical argument indicates this constraint would even more strongly force coefficients to zero, but I don't know how to quantify this.

More interesting is thinking about this in the context of exercise 6.5 on page <zipRedac>6<zipRedac>.  If Ridge Regression tends to give similar values to the coefficients of correlated variables (sensible since minimizing sum of squares) while the Lasso gives a variety of possibilities (sensible since multiple solutions to minimizing the sum) it seems to me that the constraint above would tend to give two possible solutions (each with one coefficient 0) which seems like an interesting computational property.

Still more interesting is thinking about this in terms of "A Simple Special Case for Ridge Regression and the Lasso" and "Bayesian Interpretation for Ridge Regression and the Lasso" from pp. <zipRedac><zipRedac>4-<zipRedac><zipRedac>7 of ISLR.  But I don't see how to do similar analyses for this case.

Any thoughts?


  [<zipRedac>]: http://www.wolframalpha.com/input/?i=ln%<zipRedac>8abs%<zipRedac>8x%<zipRedac>9%<zipRedac>b<zipRedac>%<zipRedac>9%<zipRedac>0%<zipRedac>b%<zipRedac>0ln%<zipRedac>8abs%<zipRedac>8y%<zipRedac>9%<zipRedac>b<zipRedac>%<zipRedac>9%<zipRedac>0=%<zipRedac>0<zipRedac>
  [<zipRedac>]: http://www3.wolframalpha.com/Calculate/MSP/MSP<zipRedac>964<zipRedac>ei<zipRedac>8<zipRedac>8<zipRedac><zipRedac>4bggh<phoneRedac><zipRedac>ga657befch3i7b?MSPStoreType=image/gif&s=<zipRedac>7&w=<zipRedac>00.&h=<zipRedac>04.&cdf=RangeControl
  [3]: https://class.stanford.edu/courses/Engineering/CVX<z>
159:<Hi! I'm really in love with this course :) ! I never laughed so much before while learning statistics.
I appreciate that you introduce the concepts first and put a bigger stress on understanding instead of inondating the audience with all the mathy notations right away. Also I find interesting all the historical and inter-subject references : they really put thing into perspective for me. The massive use of plots and illustrations is really helpful too.
You really make studing statistics sexy :)  !
Thank you for such a wonderful and exciting learning experience!>
160:<Due to the random nature of the technique being used, you might not get exactly the same, precise answer that the instructor did. "Within 10%" means that you can deviate from the quiz's official answer up to 10% of that answer *higher or lower* and still get credit for the answer.

It's sort of like if I bet you that you couldn't guess the year that Napoleon died, "within five years". He really died in 1821, but you could guess as early as 1816 or as late as 1826 and still win the bet. :-)

And you're right... using linear regression, you should get the same answer and the 10% range isn't really needed. Although you still might vary a bit if you rounded to a lower level of precision.>
161:<FWIW, a pad of paper and a pen works great.

I've upgraded my setup to include several different colors of pen, but you don't have to get that fancy if you don't want to. :-)>
162:<I find it interesting that linear regression can be considered a flexible model. In one sense linear regression is not considered a flexible model and as such, is more interpretable. This is traditional thought based on the availability of a large number of observations in the data set with a relatively small number of predictors. In the sense of chapter six, the linear regression model becomes flexible due to the number of observations in the data set being not much larger than the number of predictors. So now the linear regression model suffers from coefficient estimates with large variance and is at risk of over-fitting. In the case of p > n, we have multiple solutions fitting the least squares and must consider how to choose between them. For this high-dimensional case, we also fall short on assessing model fit with traditional measures such as RSS, p-values, and R^2. In spite of all this, the linear model continues to be a simple, interpretable model and worth this study. In the end, we know not the true form of the relationship or even if there is a relationship. However, we can do our best to choose the appropriate method to answer the question. I wish I had these tools in my statistical belt 10 years ago :)>
163:<Hi
I prefer to ask and to seem dumb to be quiet and to be it. So from this perspective, please be all lenient as you patient let you :-P

Thank you for your explanation, Alhf. I understand you look for softness in the plot of Errors derived from a lm.
resid <- residuals(lm.fit)
plot(resid, type="l")

And I "almost" understand too correlation is present when you plot errors from X1 Vs X2. Y say almost because it is clear in the first case but not too much in the second plot, 10 measures before.

But... plot suggested in the quizz is over Xy directly not over errors resulting from a fitted lm

Well, i think you can figure there is correlations because both plots are "continues" (smooth and can be derived in all its points) ¿right? ¿It is enough condition really?

i have done a little test. Maybe (Indeed pretty sure) i'm wrong but... 
when you run plot(Xy$X1 ~ Xy$X2, type='l' )
You get something similar a rich man signature, I can't paste that plot but it easy to reproduce. It is enough regular to consider there are correlation between those vars X1 and X2?, Because I was looking forward to find a single line.
It is similar to you second graph but it is referred to samples not to errors.

And finally, if you enter in R plot(acf(Xy)) you get a lot of plots. Sincerely I do not understand it, but they are similar to your last plot

Thank you, Again>
164:<@kieranmace:  (and anyone else who's interested)

Yes  of course.

I should have posted code earlier, but the "code" was just a
bunch of not-that-well-organized stuff from an interactive
session.

Anyway, I've tidied it up a bit and included it below.
(Still maybe not the prettiest, but it works.)
  
Once the early lines have been run, it's fun to run the last
two lines of code multiple times while looking at the plots.
This gives an "animation effect" and the difference between
the plain bootstrap and block bootstrap is even clearer than
looking at the sets of plots in the original post.

Hope you find it interesting.

Thanks for prompting me to do this.

    ##--------------------------------------------------
    ## generate some data
    ## It's deterministic (like the data in the problem seemed to be)
    ## It's definately autocorrelated
    t <- seq(0,0.99,by=0.01)
    x <- sin(2*pi*t)
    y <- sin(4*pi*t) + x
    data <- data.frame(x=x, y=y)
    
    plot(y ~ x, data=data)
    
    ##--------------------------------------------------
    ## Set up some functions
    
    block.idxs <- function(data,l=1) {
        data.length <- dim(data)[1]
        blockstart <- seq(1,data.length,by=l)
        blockend <- c(blockstart[-1]-1,data.length)
        data.frame(blockstart,blockend)
    }
    
    bbsamp.one.block.idxs <- function(data,l=1) {
        block.idxs <- block.idxs(data,l)
        nblocks <- dim(block.idxs)[1]
        bsamp <- sample(1:nblocks, 1)
        bsamp.idxs <- with(block.idxs[bsamp,], seq(blockstart, blockend))
        bsamp.idxs
    }
    
    bbsamp.idxs.with.block.num <- function(data,l=1) {
        data.length <- dim(data)[1]
        num.blocks <- round(data.length/l)
        idxs.with.block.num <- data.frame(block.num=c(),idx=c())
        for(i in 1:num.blocks) {
            idxs <- bbsamp.one.block.idxs(data,l)
            block.num <- rep(i, length(idxs))
            idxs.with.block.num <-
                rbind(idxs.with.block.num, data.frame(block.num, idxs))
        }
        idxs.with.block.num
    }
    
    plot.block.boostrap.with.fit <- function(data, l=20, col=TRUE) {
        idxs.bn <- bbsamp.idxs.with.block.num(data,l)
        bbsamp <- data[idxs.bn$idxs,]
        bbsamp <- transform(bbsamp, block.num=idxs.bn$block.num)
        lm.fit <- lm(y~x, bbsamp)
        pred.data <- transform(bbsamp, pred=predict(lm.fit, bbsamp))
        pred.data <- transform(pred.data, jx=jitter(x,amount=.05), jy=jitter(y,amount=.05))
       >
165:<Hello:

May I have your advice/comments about these 2 packages: bestglm, glmulti. I found out they can be used to apply all subset regression for logits, just what I need right now.

Thank you.

PD: a great great course, by the way :)>
166:<Cp, AIC, and BIC are supposed to help you find the optimal model size (d, where d could be 0, 1, 2, 3, ..., p). If all ridge models (where each model corresponds to a specific value of $lambda$) have the same d (d=p), then how would you know the optimal d? More importantly, the idea of a "model size" doesn't really apply to Ridge regression.>
167:<If we could *replace "Introduction"* with *"Applied"*, and get more points for R sessions it would be a dream come true!

P.S. *Official* solutions to the ISLR book could do the trick, as well... :-)>
168:<**Problem:** We have a data set in which many features are correlated with one another. A linear model is fit to this data set. Therefore, correlated features' estimated coefficients have high standard error (*as explained in previous lectures*). 

**Question:** Can we avoid this problem by using PCR (*Principal Components Regression*) with **M=p** (*# of principal components = # of original features*)? Since all M features in PCR are uncorrelated, coefficients will *not* have high standard error. 

I have a hunch that something is wrong with this argument. What am I missing?>
169:<In R command line run :

install.packages("ISLR")
then run:
library(ISLR)
to import this library to work space ;)>
170:<Sunny Southern California :)>
171:<> Problem: We have a data set in which many features are
> correlated with one another. A linear model is fit to this
> data set. Therefore, correlated features' estimated
> coefficients have high standard error (as explained in
> previous lectures).
> 
> Question: Can we avoid this problem by using PCR (Principal
> Components Regression) with M=p (# of principal components
> = # of original features)? Since all M features in PCR are
> uncorrelated, coefficients will not have high standard
> error.
> 
> I have a hunch that something is wrong with this
> argument. What am I missing?

A few thoughts about this:

----
You are Considering PCR with M=p.

This means we're regressing on a *full* set of principal
components.  So the model we fit will be effectively the
same as an ordinary regression on the full set of features.

(The PCR is indirectly regressing on the full set of original
features.  The ordinary regression is directly regressing on
the full set of original features.)

To be clear, the words "effectively the same" above mean:

- The fitted values will be the same.

- If we take the coefficients on the principal components
  from PCR and transform them so they are back in terms of
  original features, we will have the same coefficients as
  if we just did in ordinary regression directly on those
  original features.
  
By "avoid this problem" (in your post), are you thinking of
the following  situation?

There are two highly correlated predictor variables which
are jointly associated with response, but because of their
high correlation with each other their standard errors are
large, making it difficult to "accurately" divide up the
association (with the response) between the two predictors.
That is, it is difficult to work out "accurate" *separarate*
slope coefficients on each of the two correlated predictors.

If you are hoping that principal components will provide a
way getting "accurate" values for each of the two separate
slope coefficients, but suspect this is "too good to be
true", then your suspicion is well-founded.

Assume, for simplicity, that these two highly correlated
predictors are the only predictors and that their
correlation is positive.

This is like the plot on slide 48/57 from the model
selection lectures.

![Plot from lecture slide 48/57 from model selection lectures][1]

The first principal component will consist of some kind of
"sum" of these two correlated predictors.
In the plot from lectures, it might be

$PC_1 = 0.8(mat>
172:<I've tried mocking up the equations in Excel, but that didn't help me get to the answer.  :-)  I am missing something!>
173:<thanks, nice and helpful explanation (and excellent english btw ;))>
174:<I was also thinking about confidence intervals...sometimes questions are a bit confusing, or students overthink too much :)>
175:<Is your "why?" a question? =)
If that's so, then the answer probably would be: because we'd like to find an intersection for these two lines. By assumption right in between two formulas 4.13 and 4.14 in the book, we have K = 2. By solving it, we find the border line, which separates our classes.>
176:<From Kolkata. After 5 years of Statistics, now am working in Hyderabad. 

P.S. I must say I expected to see more bengalis. Statistics is generally crowded with bengalis. ;)>
177:<I took an econometrics course way back in the day (late 70s?).  The problem is that you run into so many economists :)>
178:<Hi,
At first you will need some additional tools: 'devtools' in R, MikTeX for documentation and for version control use Git or SVN (http://www.rstudio.com/ide/docs/version_control/overview?version=0.97.312&mode=desktop)

Then You may create your packages in RStudio by creating new project with type "package". That wizard is really helpful!

Also I did my first steps in packaging R code by looking at simple "Hello World" package
"http://notepad.patheticcockroach.com/1342/a-hello-world-r-package-or-a-quick-start-into-writing-r-extensions-with-some-c-inside/">
179:<Thanks Heidi!

I had figured it out yesterday when I had the same problem for chapter 5 :)>
180:<@WilliamChiu Thanks for pointing out thus subtle difference. Now it actually makes sense (but I reckon no management scholar cares :) )>
181:<hi,
pratically this seems to be more of a DoE problem, because there is no guarantee that the variables are spanning a reasonable part of the phase space. Plackett-Burman would come to mind :)

Regards
<nameRedac_anon_screen_name_redacted>>
182:<Excellent Reference :) Thanks!>
183:<Let say, 
Y1= Procedure A, Y2=Procedure B & Y3=Procedure C (Surgical Procedure in Hospital for a particular service)
<- f( X1= Operating time, X2=Surgeon, X3=Supporting workforce, X4=Technology). 
Y1 , Y2 & Y3 most likely will interact with each other bcos if Y1 increase then Y2 or Y3 may decrease, or Y1 increase my result in Y3 increase and so on ( I am not sure), assume Y1,2&3 have linear/non-linear relationship with X1-4, I am interest to work out how each X contribute to the changes in all Y, while still considering the potential interaction between Y, how shd such analysis be performed?>
184:<And if you didn't notice the other videos, you probably didn't notice the quizzes either. Ugh.

I'm not sure why so many people miss that each "subsection" of the course actually has multiple "units". I would've thought that the arrows at the top and at the bottom were good enough indicators that there is more content than just the first thing you see, but it's obviously not working for a lot of folks.

Granted, this was explained in the "Course Logistics" section:

> Click on a section to see its subsections, and click on a subsection
> to see its units, which contain videos, questions, etc.  Within a
> given subsection, you can move from one unit to the next by clicking
> the next icons, which appear at the top and bottom of each page.

But the fact that it even *needs* to be explained points to a flaw in the design. ([People don't read webpages][1] anyway.) They must not have tested the interface with regular humans, because that would've shown them the problem in about 5 minutes. :-)

Don't really know how you'd fix it though. Maybe have the "units" appear under each subsection in the outline view on the left, once you click on a subsection? Similar to the existing interface behavior of the subsections appearing under each section when you click on the section--which seems to work just fine for everyone.

What's probably throwing everyone off is that the interface is training them on one way of navigating the content, but then abruptly switching to a *different* way to navigate once you get to a certain point in the heirarchy.

Setting up an expectation and then subverting it is great for comedians and jazz musicians... not so good for a user interface. :-)

  [1]: http://www.nngroup.com/articles/how-users-read-on-the-web/>
185:<It's probably because the models are logistic this time.

From "Statistical Models in S", coauthored by one of our esteemed instructors :-) ...

> The **`anova()`** methods all have a **`test=`** argument. The default is "none"
> for anova.glm(), and other choices are "Chisq", "F", and "Cp". **For a
> binomial model, the changes in deviances between nested models are
> typically treated as chi-squared variables, so test="Chi" is
> appropriate here** (notice abbreviations are allowed).>
186:<Hi All,<br/><br/>
I still dont know understand how to plot the graph on the right side.<br/>
If we have the fractions of volume, how can I compute the radius and plot the graph?<br/> 
Thank You :)>
187:<this explanation makes perfect sense and is so helpful! thank you! :)>
188:<@HusVar:

It produces a model matrix without a column of '1's.

In effect it just puts the predictor variables
together as columns of a matrix.

----

`-1` specifies a model without an intercept.

From `help(formula)`, under "Details":

> The - operator removes the specified terms,
> so that (a+b+c)^2 - a:b is identical to
> a + b + c + b:c + a:c.  It can also used to
> remove the intercept term: when fitting a
> linear model y ~ x - 1 specifies a line
> through the origin.  A model with no
> intercept can be also specified as y ~ x +
> 0 or y ~ 0 + x.

The intercept is a "variable" whose value is `1`
for every observation.  The slope on this
"variable" will add a constant offset to whatever
else is going on in the model.

For example:

    > data=data.frame(predictor1=c(1,2,3), predictor2=c(4,5,2), response=c(10,9,12))
    > data
      predictor1 predictor2 response
    1          1          4       10
    2          2          5        9
    3          3          2       12
    > model.matrix(response ~ . , data=data)
      (Intercept) predictor1 predictor2
    1           1          1          4
    2           1          2          5
    3           1          3          2
    > model.matrix(response ~ .-1 , data=data)
      predictor1 predictor2
    1          1          4
    2          2          5
    3          3          2>
189:<* sorry about the formatting - appears the be the default;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

***** 6.5.1 Best Subset Selection *****;
title '6.5.1 Best Subset Selection';

* csv file created from Portfolio dataset (> write.csv(Hitters,file="Hitters.csv");
* note: need to add "Player" to 1st row in resulting csv file before importing into sas;
proc import out=HittersAll(rename=(Salary=SalaryChar)) dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookHitters.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=1000;
run;

* delete row if any variable missing;
* > Hitters=na.omit(Hitters);
data Hitters;
   set HittersAll;
   Salary = SalaryChar + 0;
   * delete row if any variable missing;
   if nmiss(of _numeric_) + cmiss(of _character_) > 0 then delete;
   * sas needs numeric variables only for best subset selection (which is available only in procs reg & phreg); 
   if League = 'A' then League01 = 0; else League01 = 1;
   if Division = 'E' then Division01 = 0; else Division01 = 1;
   if NewLeague = 'A' then NewLeague01 = 0; else NewLeague01 = 1;
run;

* use contents & sql to get list of (numeric only) predictors for model statement;
proc contents data=Hitters(keep=_numeric_) noprint out=ContentsOut(keep=Name);
run;

proc sql noprint;
   select Name into :Predictors separated by ' '
   from ContentsOut
   where Name ne 'Salary';
quit;

* > library (leaps)
  > regfit .full=regsubsets (Salary~.,Hitters )
  > summary (regfit .full);
* perform best subset selection, up to 8 predictors - note that glmselect cannot perform best subset selection;
proc reg data=Hitters;
   title2 'Hitters Dataset: Up to 8 Predictors';
   model Salary = &Predictors / selection=rsquare stop=8 sse;  * 'sse' displayed since book used RSS to quantify best;
run;quit;

* > regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19)
  > reg.summary=summary(regfit.full)
  > par(mfrow=c(2,2))
  > plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
  > plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
  > plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type=l)
  > plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type=l);
* perform best subset selection, up to 19 predictors and plot the fit statistics;
ods graphics on;
proc reg data=Hitters outest=Reg>
190:<* sorry about the formatting - it appears to be automatic;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

***** 6.5.2 Forward and Backward Stepwise Selection *****;
title '6.5.2 Forward and Backward Stepwise Selection';

* csv file created from Portfolio dataset (> write.csv(Hitters,file="Hitters.csv");
* note: need to add "Player" to 1st row in resulting csv file before importing into sas;
proc import out=HittersAll(rename=(Salary=SalaryChar)) dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookHitters.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=1000;
run;

* delete row if any variable missing;
* > Hitters=na.omit(Hitters);
data Hitters;
   set HittersAll;
   Salary = SalaryChar + 0;
   * delete row if any variable missing;
   if nmiss(of _numeric_) + cmiss(of _character_) > 0 then delete;
   * sas needs numeric variables only for best subset selection (which is available only in procs reg & phreg); 
   if League = 'A' then League01 = 0; else League01 = 1;
   if Division = 'E' then Division01 = 0; else Division01 = 1;
   if NewLeague = 'A' then NewLeague01 = 0; else NewLeague01 = 1;
run;

* use contents & sql to get list of (numeric only) predictors for model statement;
proc contents data=Hitters(keep=_numeric_) noprint out=ContentsOut(keep=Name);
run;

proc sql noprint;
   select Name into :Predictors separated by ' '
   from ContentsOut
   where Name ne 'Salary';
quit;

* > regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
  > summary(regfit.fwd);
* perform forward stepwise selection, up to 19 predictors;
proc reg data=Hitters;
   title2 'Hitters Dataset: Forward Selection';
   model Salary = &Predictors / selection=forward;
run;quit;

* > regfit.fbd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
  > summary(regfit.bwd);
* perform backward stepwise selection, up to 19 predictors;
proc reg data=Hitters;
   title2 'Hitters Dataset: Backward Selection';
   model Salary = &Predictors / selection=backward;
run;quit;

quit;>
191:<* sorry about the formatting - it appears to be automatic;

proc datasets memtype=data library=work kill; run;
options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;

***** 6.5.3 Choosing Among Models Using the Validation Set Approach and Cross-Validation *****;
title '6.5.3 Choosing Among Models Using the Validation Set Approach and Cross-Validation';

* csv file created from Portfolio dataset (> write.csv(Hitters,file="Hitters.csv");
* note: need to add "Player" to 1st row in resulting csv file before importing into sas;
proc import out=HittersAll(rename=(Salary=SalaryChar)) dbms=dlm replace
     datafile='\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookHitters.csv';
   getnames=yes;
   delimiter=',';
   guessingrows=1000;
run;

* delete row if any variable missing;
* > Hitters=na.omit(Hitters);
data Hitters;
   set HittersAll;
   Salary = SalaryChar + 0;
   * delete row if any variable missing;
   if nmiss(of _numeric_) + cmiss(of _character_) > 0 then delete;
   * sas needs numeric variables only for best subset selection (which is available only in procs reg & phreg); 
   if League = 'A' then League01 = 0; else League01 = 1;
   if Division = 'E' then Division01 = 0; else Division01 = 1;
   if NewLeague = 'A' then NewLeague01 = 0; else NewLeague01 = 1;
run;

* use contents & sql to get list of (numeric only) predictors for model statement;
proc contents data=Hitters(keep=_numeric_) noprint out=ContentsOut(keep=Name);
run;

proc sql noprint;
   select Name into :Predictors separated by ' '
   from ContentsOut
   where Name ne 'Salary';
quit;

* > set.seed(1)
  > train=sample(c(TRUE,FALSE),nrow(Hitters),rep=TRUE)
  > test=(!train);
* randomly split the data into training and test datasets;
proc surveyselect data=Hitters method=srs samprate=0.5 rep=1 seed=1 out=HittersSelect1 outall noprint;
   * 'method=srs' specifies without replacement;
   * 'outall' option keeps all data in output dataset and adds 'selected' variable (0,1);
run;

data HittersTrain1 HittersTest1;
   set HittersSelect1;
   if selected = 1 then output HittersTrain1;
   * create mutiple reps of test data to merge with models of each size in score procedure;
     else do _in_ = 1 to 19;  * _in_ = # of variables included in the model;
        output HittersTest1;
     end;
run;

* > regfit.best=regsubsets(Salary~.,data=Hitters[train,],nvmax=19)
* perform best subset selection, up to 19 predictors - note that glmsel>
192:<Hi,

the book (particularly chaps. 4.6.2 and 4.6.6) as well as the lectures (exp. end of Lec. 4.7, ROC curve) often refer to the classification error rate that one would obtain in case of random guessing, as a reference for the error rate of our classifiers. Consider for example the Caravan Insurance data set in the R package ISLR: 6% of individuals purchased a caravan insurance. It is stated on on page 166 that random guessing would give an error rate of 6%. However, it seems to me that error rate in case of random guessing should always be 50%! This would be coherent with what's written on page 160, where the error rate of random guessing (for a different data set) is said to be 50%.

My reasoning is as follows: let $Y=1$ correspond to an individual that buys insurance, and $Y=0$ to an individual that doesn't. I throw a fair coin, and I predict that Joe will buy the insurance if the coin comes out head ($X=1$), otherwise he won't ($X=0$). Clearly $X$ and $Y$ are independent, and $P(X=0)=P(X=1)=0.5$ (fair coin). Now,  

$P(Error)=P(Y=1|X=0)P(X=0)+P(Y=0|X=1)P(X=1)$                                        

Y and X are independent, so

$P(Error)=P(Y=1)P(X=0)+P(Y=0)P(X=1)$

Also, since the coin is fair, 

$P(Error)=.5P(Y=1)+.5P(Y=0)=.5*1=.5$

So the error rate is 50%! Where am I wrong?>
193:<Hi!

Thanks a lot for your answer. Yes, the book mentions always picking the majority class, and I understand this gives a 6% error rate. But, later on, they also mention random guessing. However, always guessing "No" didn't seem to me to be any random at all! :)

> It turns out that KNN with K = 1 does far better than random guessing
> among the customers that are predicted to buy insurance. Among 77 such
> customers, 9, or 11.7%, actually do purchase insurance. This is double
> the rate that one would obtain from random guessing.

But, thanks to your comment, I understand. What's really random here is the sampling of the individuals. In other words, I randomly pick n individuals out of my population. Since the percentage of individuals in the population that buy insurance is 6%, if I repeat my random sampling many times, on average 6% of the people I picked will buy insurance (Law Of Large Numbers). That's clear now!

If I can reciprocate someway, let me know!>
194:<Hi,

some weeks ago I posted the link to a proof of the bias-variance decomposition that, while nice, didn't show all of the details. I found a great one in this book:

Bishop, C.M., "Pattern Recognition and Machine Learning", Springer, 2007

It's in paragraph 3.2. I really liked the book! It's very rigorous and full of clear examples. What's more, all the advanced mathematical tools he uses are explained in the Appendixes. The book content isn't really the same as this course's, so it doesn't really fit as a textbook for the course. However, we don't need a textbook for this course since we already have two excellent ones :) But for anyone interested in extra references on specific topics, this may just cut it.>
195:<When I download RStudio and open it, I get the following error message:

ERROR r error 4 (R code execution error) [errormsg=Error in identical(call[[1L]], quote(doTryCatch)) : 
  7 arguments passed to .Internal(identical) which requires 5
, code=local(source("/Applications/RStudio.app/Contents/Resources/R/Tools.R", local=TRUE, echo=FALSE, verbose=FALSE, keep.source=FALSE, encoding='UTF-8'))]; OCCURRED AT: core::Error r::exec::<anonymous namespace>::evaluateExpressions(SEXP, SEXP, SEXP *, sexp::Protect *) /Users/rstudio/rstudio/src/cpp/r/RExec.cpp:145

Any idea what the problem is? So far I've just been running everything in R but would like to get more familiar with RStudio.

Thanks.>
196:<MooMoo, thank you for explaining it so well! This part of the course isn't explained very clearly, but your description is perfect. I think the course team should bring this explanation into the material itself. (There's no poor English in your post by the way :-)>
197:<@alhf, good point, you've demonstrated numerically that $lim_{p_{mk} ightarrow 0^+} D(p_{mk}) = 0$

My observation is that in both a mathematical and programming sense $D$ is currently only defined when each $p_{mk} in (0,1]$. It would be much more convenient if it were defined on the closed interval $[0, 1]$. To do this we would need to add a case statement to each term in the sum. But that would really clutter up the simple equation :)>
198:<Are you using RStudio? It makes it painless to get started with Markdown - RStudio Notebooks hide the markdown completely. An introduction is here:
http://www.rstudio.com/ide/docs/authoring/markdown_notebooks?version=0.98.501&mode=desktop>
199:<Personally I watch the videos, read the book and only then do the quizzes. In a couple occasions I noticed that lectures weren't sufficient to answer quizzes, so I decided to read the book before attempting answers. Also, reading the book is great to fix the key points in my mind. I never read the pdfs, since I watched the videos already. If I have any spare time, I prefer to study the book or navigate the forum. Doing the book exercises would be a great idea, but I've got a full-time job and I I'd never find time for that. 

<nameRedac_anon_screen_name_redacted>

ps I'm very curious, so I like to read extra references, such as ESL, and the book by Bishop I cited some time ago. The few times I do that, I regularly fall behind schedule (but I feel satisfied :)>
200:<Hello everyone!

I have some doubts about which is called the "right cv" and the "wrong cv". I'm actually using cross validation at my bachelor project, and I got a big doubt, since seems I was using it the wrong way... 

What I did is, I developed some linear regression models (which actually are quite complicated, with some predictors and interactions), BUT I updated them before using CV on them. I mean, I deleted already such variables/interactions that were non-significant for the models. And with my "clean" model, then I apply cross validation, in order to select the "best" models. But seems I did it wrong, or?? 

I don't really understand which is the correct way to apply CV to my models. Should I leave EVERY predictor and interaction (which would be almost impossible, there would be hundreads of them) before applying CV? And them develope (update) the models with the best CV results? 

Can anyone please explain me how does it work? That would be really helpful... I'm new at this topic, and I'm still pretty lost!!!

Thank you very much to every information :)>
201:<This is a little mathematical, but here it goes: a non-regularized solution to an ordinary LS (OLS) is given by: inv(A'A)A'Y. ':Transpose, Y: observations. A weighted LS solution takes the form inv(A'WA)A'WY, where W is the weighting matrix. Now a regularized solution takes the form (without weighting to simplify the formula) inv(A'A + lambda*I)A'Y. So the Regularizer here is lambda*I where I is the identity matrix of appropriate size. So now here's my question: If I knew the correlation between the answers, how can I use that information to select a better Regularizer than lambda*I? 

as an example, take the problem of fitting a 1D function to a set of observations, say 100 observations, and I want to evaluate the function at 1000 query points. If I knew or can compute the correlation between the function values at these query (unknown) locations, how can use this information to construct a better Regularizer?

hope you'll enjoy this one :)
many thanks,>
202:<The video quality of .mov file available for download through download link is quite poor (slide text shown in the video are often blur).

Additionally since youtube in our country is blocked so it is not possible to view the course video directly.

It would be great help if someone can list down all the youtube video url for each lecture. For example the video url for lecture (Ch5-Resampling in R) is 

   https://www.youtube.com/watch?feature=player_embedded&v=YVSmsWoBKnA 

Once this url is available, i can then download the video using www.clipconverter.cc>
203:<Hi,

I guess the main problem with being unable to load the right data is that your search directory in R is searching in the wrong place. 
Before doing anything else, do the following:
1) getwd() # This will tell you in which directory R is searching. 
2) Find the directory in which you have saved the focal file (if you use windows like me than just open the righ map and double click on the topline in the map
3) setwd("C:UsersDropboxMethodsRStanford") # That is my directory (make sure to either use double \ or to change them to forward slash /
4) load("7.R.RData") # Normaly not case sensITIve but take no chances :)  

That should do the trick>
204:<Here you go.  You'll have to remove the blank lines yourself, as, for some reason, the only other with this web-based editor is to paste them in one continuous paragraph w/o any cr/lf's.

https://www.youtube.com/watch?v=6l9V1sINzhE

https://www.youtube.com/watch?v=jk9S3RTAl38

https://www.youtube.com/watch?v=uQBnDGu6TYU

https://www.youtube.com/watch?v=DCn83aXXuHc

https://www.youtube.com/watch?v=jwBgGS_4RQA

https://www.youtube.com/watch?v=5ONFqIk3RFg

https://www.youtube.com/watch?v=6ENTbK3yQUQ

https://www.youtube.com/watch?v=GfPR7Xhdokc

https://www.youtube.com/watch?v=hPEJoITBbQ4

https://www.youtube.com/watch?v=lq_xzBRIWm4

https://www.youtube.com/watch?v=U3MdBNysk9w

https://www.youtube.com/watch?v=2cl7JiPzkBY

https://www.youtube.com/watch?v=9TVVF7CS3F4

https://www.youtube.com/watch?v=TxvEVc8YNlU

https://www.youtube.com/watch?v=6dSXlqHAoMk

https://www.youtube.com/watch?v=YVSmsWoBKnA

https://www.youtube.com/watch?v=3kwdDGnV8MM

https://www.youtube.com/watch?v=mv-vdysZIb4

https://www.youtube.com/watch?v=F8MMHCCoALU

https://www.youtube.com/watch?v=1REe3qSotx8

https://www.youtube.com/watch?v=1REe3qSotx8

https://www.youtube.com/watch?v=0wZUXtvAtDc

https://www.youtube.com/watch?v=IY7oWGXb77o

https://www.youtube.com/watch?v=QpbynqiTCsY

https://www.youtube.com/watch?v=xKsTsGE7KpI

https://www.youtube.com/watch?v=xKsTsGE7KpI

https://www.youtube.com/watch?v=dm32QvCW7wE

https://www.youtube.com/watch?v=mI18GD4_ysE

https://www.youtube.com/watch?v=2wLfFB_6SKI

https://www.youtube.com/watch?v=LvaTokhYnDw

https://www.youtube.com/watch?v=L3n2VF7yKkk

https://www.youtube.com/watch?v=L3n2VF7yKkk

https://www.youtube.com/watch?v=WjyuiK5taS8

https://www.youtube.com/watch?v=UvxHOkYQl8g

https://www.youtube.com/watch?v=VusKAosxxyk

https://www.youtube.com/watch?v=vVj2itVNku4

https://www.youtube.com/watch?v=PsE9UqoWtS4

https://www.youtube.com/watch?v=J6AdoiNUyWI

https://www.youtube.com/watch?v=1hbCJyM9ccs

https://www.youtube.com/watch?v=3T6RXmIHbJ4

https://www.youtube.com/watch?v=IFzVxLv0TKQ

https://www.youtube.com/watch?v=sqq21-VIa1c

https://www.youtube.com/watch?v=31Q5FGRnxt4

https://www.youtube.com/watch?v=MpX8rVv_u4E

https://www.youtube.com/watch?v=GavRXXEHGqU

https://www.youtube.com/watch?v=RfrGiG1Hm3M

https://www.youtube.com/watch?v=QG0pVJXT6EU

https://www.youtube.com/watch?v=X4VDZDp2vqw

https://www.youtube.com/watch?v=6FiNGTYAOAA

https://www.youtube.com/watch?v=_2ij6eaaSl0

https://www.youtube.com/watch?v=nZAM5OXrktY

https://www.youtu>
205:<@WilliamChiu:

> Lasso. However, as mentioned in the textbook, even the Lasso has
> problems when faced with a large number of variables.

Could I ask, which part of the book was that ?
I got the impression that this was the sort of situation where lasso (and
ridge) could be useful.

> Lasso cannot deal with p > n problems. Elastic net can deal with
> that. Lasso is a special case of elastic net.

I don't see why, in principle, lasso couldn't be used when p>n
(especially if elastic net for $alpha<1$ can be).  The optimization
problem seems to be reasonably well specified.

For example, the following code, with p>n, seems to be behaving roughly as
expected[*]:

    require(glmnet)
    n <- 10
    p <- 20
    set.seed(10)
    X <- matrix(rnorm(n*p), ncol=p)
    y <- rnorm(n)
    fit.lasso=glmnet(X,y, alpha=1)
    plot(fit.lasso,xvar="lambda",label=TRUE)

As $lambda$ increases, more coefficients are set to zero.

And there don't seem to be any complaints from the `glmnet` function.

I guess if p was really large (and maybe n too, for that matter), the
computational cost might become large (but that wouldn't be a p>n
problem).

----

Aside:

Interestingly, even the `lm` function "gives a fit" here.

    fit.lm <- lm(y ~ X)

But 

    summary(fit.lm)

shows that it seems to have only used (p-1) of the columns of X
(plus the intercept, that makes p predictors), which I guess is
a reasonable thing to do under the circumstances.

----

[*] Of course any model coming out of this would have terrible test
error, because it's just been fitted to noise.>
206:<I've made this code to solve the 9.R.1 quiz but the value that's giving me it's wrong and I can't find where I messed up, does anybody know what's wrong here?

    svm_error <- function() {
      # 1) generate a random training sample to train on + fit
      
      # build training set
      x0 = mvrnorm(50,rep(0,10),diag(10))
      x1 = mvrnorm(50,rep(c(1,0),c(5,5)),diag(10))
      train = rbind(x0,x1)
      classes = rep(c(0,1),c(50,50))
      dat=data.frame(train,classes=as.factor(classes))

      # fit
      svmfit=svm(classes~.,data=dat,cost=10,scale=FALSE)
      
      # 2) evaluate the number of mistakes we make on a large test set = 1000 samples
      test_x0 = mvrnorm(500,rep(0,10),diag(10))
      test_x1 = mvrnorm(500,rep(c(1,0),c(5,5)),diag(10))
      test = rbind(test_x0,test_x1)
      test_classes = rep(c(0,1),c(500,500))
      test_dat = data.frame(test,test_classes=as.factor(test_classes))
      fit = predict(svmfit,test_dat)
      error = abs(500 - length(which(fit == 0)))/1000
      
      return(error)
    }
    
    # 3) repeat (1-2) many times and averaging the error rate for each trial
    errors = replicate(100, svm_error())
    
    print(errors)
    print(mean(errors))

Explanation:

 - The fit is made using 100 samples coming from X0 & X1 (each w/50 gaussian samples)
 - The test set is built using 1000 samples of the same distributions (500 of each class)
 - Error = (500 - #Y=0 predicted) / 1000 (by construction test set had 500 Y=0 + 500 Y=1)

Any feedback appreciated, thanks!>
207:<Question 9 R1 starts "Use svm in the e1071 package with the default settings..."

By default cost=1 !

I think that this line in your code is not correct:
svmfit=svm(classes~.,data=dat,cost=10,scale=FALSE)

try just 

svmfit=svm(classes~.,data=dat)

Hope it will help. 

Let me know whether it was the correct answers.>
208:<I've tried using the code in lecture and the book and here is what I have come up with:

       x=matrix(rnorm(1000),100,10)
       y=rep(c(0,1),c(50,50))
       x[y==1,1:5]=x[y==1,1:5]+1
    
       library(e1071)
       dat=data.frame(x,y=as.factor(y))
       svmfit=svm(y~.,data=dat,kernel="radial",scale=FALSE)
    
       xtest=matrix(rnorm(1000), 100,10)
       ytest=sample(c(0,1), 100, rep=TRUE)
       xtest[ytest==1,1:5]=xtest[ytest==1,1:5] + 1
       testdat=data.frame(x=xtest, y=as.factor(ytest))
       ypred=predict(svmfit, testdat)
       table(predict=ypred, truth=testdat$y)

it seems to work until I get to the next to last line, the ypred statement.  R will spit out "Error in eval(expr, envir, enclos) : object 'X1' not found"  Is there any idea what might be wrong?  

Also, once I get this idea down, I will need to find the error and run it in a loop say 1000 times?

Thanks for any help!>
209:<Sorry I should have mentioned that the results I was describing are with the argument FUN=prune.misclass, which does return the sum of the number misclassed as the value of "dev" (and as is shown in the book example). 

The discrepancy still remains and seems puzzling, any other ideas would be appreciated.>
210:<The data.frame testdat has not the same colnames as dat, so the svmfit doesn't recognize it when making predictions. 

Check colnames(testdat), colnames(dat). You can simply use 

    testdat=data.frame(xtest, y = as.factor(ytest))

or directly

    colnames(testdat) <- colnames(dat)>
211:<I'm wondering the same thing.  I checked the Course Info page and the Course Logistics section but I don't see the requirements listed clearly anywhere.  Was it in one of the videos?  Is it just the quizzes after the videos or are there programming assignments and an exam?  It would be great if someone could clarify this ASAP since I (and maybe some others too) would have to get caught up soon :D

Thanks>
212:<@PhilMunck:

You could subtract the desired intercept from the response
then fit the model without an intercept (`-1` in the formula)
to get the slope.

For example

    x <- seq(0.1,1,by=0.1)
    set.seed(10)
    y <- 2 + 3*x + 0.1*rnorm(x)
    data <- data.frame(x,y)
    
    ## unconstrained model
    lm(y ~ x, data=data)
    
    ## In effect, fix intercept to be 2, see what slope is
    lm(y-2 ~  x - 1  , data=data)

Similarly, you could fix the slope:
    
    ## In effect, fix slope to be 3, see what intercept is
    lm(y - 3*x ~ 1 , data=data)>
213:<**Ch01 Introduction**

1.1 Opening Remarks (https://www.youtube.com/watch?v=2wLfFB_6SKI)

1.2 Examples and Framework (https://www.youtube.com/watch?v=LvaTokhYnDw)


**Ch02 Overview of Statistical Learning**

Introduction to Regression Model (https://www.youtube.com/watch?v=WjyuiK5taS8)

2.2 Dimensionality and Structured Models (https://www.youtube.com/watch?v=UvxHOkYQl8g)

2.3 Model Selection and Bias-Variance Tradeoff (https://www.youtube.com/watch?v=VusKAosxxyk)

2.4 Classification (https://www.youtube.com/watch?v=vVj2itVNku4)

2.R Introduction to R (https://www.youtube.com/watch?v=jwBgGS_4RQA)
Interview with John Chambers (https://www.youtube.com/watch?v=jk9S3RTAl38)


**Ch03 Linear Regression**

3.1 Simple Linear Regression (https://www.youtube.com/watch?v=PsE9UqoWtS4)

3.2 Hypothesis Testing and Interval Confidence (https://www.youtube.com/watch?v=J6AdoiNUyWI)

3.3 Multiple Linear Regression (https://www.youtube.com/watch?v=1hbCJyM9ccs)

3.4 Some Important Questions (https://www.youtube.com/watch?v=3T6RXmIHbJ4)

3.5 Extensions of the linear models (https://www.youtube.com/watch?v=IFzVxLv0TKQ)

3.R Linear Regression in R (https://www.youtube.com/watch?v=5ONFqIk3RFg)


**Ch04 Classification**

4.1 Introduction to Classification Problems (https://www.youtube.com/watch?v=sqq21-VIa1c)

4.2 Logistic Regression (https://www.youtube.com/watch?v=31Q5FGRnxt4)

4.3 Multivariate Logistic Regression (https://www.youtube.com/watch?v=MpX8rVv_u4E)

4.4 Logistic Regression - Case Control Sampling and Multiclass (https://www.youtube.com/watch?v=GavRXXEHGqU)

4.5 Discriminant Analysis (https://www.youtube.com/watch?v=RfrGiG1Hm3M)

4.6 Gaussian Discriminant Analysis - One Variable (https://www.youtube.com/watch?v=QG0pVJXT6EU)

4.7 Gaussian Discriminant Analysis - Many Variable (https://www.youtube.com/watch?v=X4VDZDp2vqw)

4.8 Quadratic Discriminant Analysis and Naive Bayes (https://www.youtube.com/watch?v=6FiNGTYAOAA)

4.R Classification in R (https://www.youtube.com/watch?v=TxvEVc8YNlU)


**Ch05 Resampling Methods**

Interview with Bradley Efron (https://www.youtube.com/watch?v=6l9V1sINzhE)

5.1 Cross-Validation (https://www.youtube.com/watch?v=_2ij6eaaSl0)

5.2 K-fold Cross-Validation (https://www.youtube.com/watch?v=nZAM5OXrktY)

5.3 Cross-Validation: the wong and right way (https://www.youtube.com/watch?v=S06JpVoNaA0)

5.4 The Bootstrap (https://www.youtube.com/watch?v=p4BYWX7PTBM)

5.5 More on the Bootstrap (https://www.youtube.com/watch?v=BzHz0J9a6k0)

5.R Resampl>
214:<Dear all,

At present, I am struggling with the final question of quiz 9.R on how to apply logistic regression in order to calculate the error rate within <zipRedac>0%.

Below is the script, which I have managed to set up. 
However, I am unable to obtain the correct answer.
Your feedback is appreciated.

Best wishes,

<nameRedac_anon_screen_name_redacted>

lg_error <- function() {

set.seed(<zipRedac>0<zipRedac><zipRedac><zipRedac>)

x0 = mvrnorm(50,rep(0,<zipRedac>0),diag(<zipRedac>0))

x<zipRedac> = mvrnorm(50,rep(c(<zipRedac>,0),c(5,5)),diag(<zipRedac>0))

train = rbind(x0,x<zipRedac>)

classes = rep(c(0,<zipRedac>),c(50,50))

dat=data.frame(train,classes=as.factor(classes))


glmfit=glm(classes~.,data=dat,family=binomial)

test_x0 = mvrnorm(500,rep(0,<zipRedac>0),diag(<zipRedac>0))

test_x<zipRedac> = mvrnorm(500,rep(c(<zipRedac>,0),c(5,5)),diag(<zipRedac>0))

test = rbind(test_x0,test_x<zipRedac>)

test_classes = rep(c(0,<zipRedac>),c(500,500))

test_dat = data.frame(test,test_classes=as.factor(test_classes))

fit = predict(glmfit,test_dat)

error = sum(fit !=test_dat$test_classes)

return(error)

}>
215:<I've enjoyed the exchange as well.  You've forced me to attempt to state clearly some things that I had previously only half thought about (I've never formally studied statistics; I'm self-taught; so this has been very educational for me).  You're right that we agree on most things now.  I think we still disagree on the fitted values, though.  As stated in the stackexchange quote in your first post in this thread, if you're willing to assume that the coefficients are normally distributed (say, by appealing to the CLT) then the fitted values will be normally distributed, as well, and pointwise confidence intervals formed by standard errors are a reasonable thing (of course, that's different than saying that the prediction interval will given by the predict function will be valid; that relies on the normality of the irreducible error).  So, rather than arguing that point with more math, I went ahead and continued the R code example you gave in your first post.  I found it educational, so I'll try posting it here in case you do too:

    require(MASS)
    fit1<-lm(medv~lstat,data=Boston)
    summary(fit1)
    hist(resid(fit1))
    qqnorm(resid(fit1))
    qqline(resid(fit1))
    # residuals do not look at all normal
    #
    # now, let's extract some stuff from the summary object
    (beta<-coef(fit1))
    # coefficients
    (sigma<-summary(fit1)$sigma)
    # residual standard deviation
    (dof<-summary(fit1)$df[2])
    # residual degrees of freedom
    #
    # Now, let's get an estimate on a new point
    #
    sum(c(1,4.98)*beta)
    # estimate on new datum lstat=4.98
    sqrt(t(c(1,4.98)%*%vcov(fit1)%*%c(1,4.98)))
    # SE of estimate, based on normality of betas
    #
    # first, let's look at pointwise condfidence interval:
    #
    predConf<-predict(fit1,newdata=data.frame(lstat=4.98),se.fit=TRUE,interval="confidence")
    predConf$fit
    # agrees with above estimate; see below for CI
    predConf$se.fit
    # agrees with above SE
    predConf$fit[1]+qt(c(0.025,0.975),dof)*predConf$se.fit
    # agrees with CI; should be good if betas approximately normal 
    #
    # Now, let's look at prediction intervals:
    #
    predPred<-predict(fit1,newdata=data.frame(lstat=4.98),se.fit=TRUE,interval="prediction")
    predPred$fit
    # estimate is the same; see below for interval
    predPred$se.fit
    # SE of estimate is the same
    predPred$residual.scale
    # this is the same as sigma
    predPred$df
    # this is the same as dof
    predPred$fit[1]+qt>
216:<Try fit2 = ifelse(fit>.5,1,0) then
error=sum(fit2!=data$test_classes)/1000>
217:<(not part of assignment or quiz)
I am trying to run the predict() function on the boost variable coming from:
boost.carseats=gbm(Sales~.,data=Carseats[train,],distribution="gaussian",n.trees=<zipRedac>0000,shrinkage=0.0<zipRedac>,interaction.depth=4)

what I've tried is: boost.pred=predict(boost.carseats,Carseats[-train,],type="class")
and get errors:
Error in paste("Using", n.trees, "trees...
") : 
  argument "n.trees" is missing, with no default

I'd like to compare the results of the Boost predictive in a Confusion Matrix to compare how well it does against the Tree and Random Forest methods. Any tips on this?>
218:<I agree with ChrisLawton about the confusing terminology in the wording of the question.  The textbook itself contains no mention of `mvrnorm()`, nor do any of the lectures.  Though I do appreciate zhq's explanation, presenting the std. dev. in the problem set-up as an identity matrix just obscures the fact that the data sets can be generated by simply manipulating segments of the data matrix, as was done in the videos.  Also, simply knowing of the existence of `mvrnorm()` might not be a great deal of help in generating the data, since going this route requires creating the data matrix in halves and then splicing them together with `rbind()` (afaik).

But worse, for me, is this:  even once we can follow the instructions and "get" the answers, what does it mean?  We have error estimates for linear and radial kernels, but how are we to interpret their respective values?  This is probably the most involved question we've had to answer in the course, but I don't really know what I learned from the exercise, beyond deciphering unfamiliar instructions and making sense out of mysterious tools?

The Chapter 9 R session required downloading ESL.mixture.rda, but the link to that file was incorrect; the instructions for the R session Quiz make use of a terminology not previously introduced in the course; that terminology strongly suggests use of a tool (`mvrnorm`) never demonstrated in the videos nor covered anywhere in the text.  I suspect these are symptoms of a "cut-and-paste" approach to creating material.  It would seem that this material itself was never tried out or even proofread.  I can only hope that comments from the participants in the course will be used to improve the next iteration. :)>
219:<Edit: I realized my response did not address your primary problem.  You just need to specify n.trees (it seems odd to me predict does not default to what was computed by the model).  For example, from the ch8.Rmd sample code:

    n.trees=seq(from=<zipRedac><zipRedac><zipRedac>,to=<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>,by=<zipRedac><zipRedac><zipRedac>)
    predmat=predict(boost.boston,newdata=Boston[-train,],n.trees=n.trees)
    dim(predmat)
    berr=with(Boston[-train,],apply( (predmat-medv)^2,2,mean))
    plot(n.trees,berr,pch=<zipRedac>9,ylab="Mean Squared Error", xlab="# Trees",main="Boosting Test Error")
    abline(h=min(test.err),col="red")
    min(berr)

I'm using caret to do confusion matrix comparisons for a variety of algorithms.  Attached is some example output.  I think you can just use the caret confusionMatrix function by itself.  For example:

    confusionMatrix(caretPreds$pred, caretPreds$obs)

    ## [<zipRedac>] "====================================="
    ## [<zipRedac>] "Model earthFit<zipRedac> of type earth"
    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction Male Female
    ##     Male    523     7<zipRedac>
    ##     Female   59    56<zipRedac>
    ##                                        
    ##                Accuracy : <zipRedac>.894        
    ##                  95% CI : (<zipRedac>.875, <zipRedac>.9<zipRedac>)
    ##     No Information Rate : <zipRedac>.52         
    ##     P-Value [Acc > NIR] : <2e-<zipRedac>6       
    ##                                        
    ##                   Kappa : <zipRedac>.787        
    ##  Mcnemar's Test P-Value : <zipRedac>.379        
    ##                                        
    ##             Sensitivity : <zipRedac>.899        
    ##             Specificity : <zipRedac>.889        
    ##          Pos Pred Value : <zipRedac>.882        
    ##          Neg Pred Value : <zipRedac>.9<zipRedac>5        
    ##              Prevalence : <zipRedac>.48<zipRedac>        
    ##          Detection Rate : <zipRedac>.43<zipRedac>        
    ##    Detection Prevalence : <zipRedac>.489        
    ##       Balanced Accuracy : <zipRedac>.894        
    ##                                        
    ##        'Positive' Class : Male         
    ##                                        
    ## [<zipRedac>] "====================================="
    ## [<zipRedac>] "Model gbmFit<zipRedac> of type gbm"
    ## Confusion Matrix and Sta>
220:<On Chapter 8, slide 4 of 51, how did the teachers color-code the Salary variable. It is a continuous variable and I've Googled several ways of doing it, but none I liked. Figured I'd ask. :) 

Any successes on this? It's a cool technique.>
221:<During one of the lectures for Chapter 6, Daniela mentioned that, during cross-validation, you can use a "one standard error" rule to select model size.  If I understood the point that was being made correctly, to apply this rule all you would need to do is to identify the model size that provides the minimum test rmse, then consider any models of smaller size that fell within 1 SE of minimum.  Based on what was shared I mocked up some code that takes this approach extending the Hitters example from class.  

I've actually got 2 questions.  First, can someone give this a look and make sure that I've understood and applied the rule correctly.  Second, if I am to understand the interpretation properly, one might actually consider the model of size 4 as "preferable" (if that is a correct characterization) due to its parsimony?  The reason I ask is that this just doesn't look right to me.  My gut tells me that a model of size 6 would be a better cut point, as this appears to be where the RMSE plot really starts to flatten out.

Any thoughts?


    require(ISLR)
    require(leaps)
    
    Hitters=na.omit(Hitters)
    
    predict.regsubsets=function(object,newdata,id,...){
      form=as.formula(object$call[[2]])
      mat=model.matrix(form,newdata)
      coefi=coef(object,id=id)
      mat[,names(coefi)]%*%coefi
    }
    set.seed(11)
    folds=sample(rep(1:10,length=nrow(Hitters)))
    folds
    table(folds)
    cv.errors=matrix(NA,10,19)
    for(k in 1:10){
      best.fit=regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax=19,method="forward")
      for(i in 1:19){
        pred=predict(best.fit,Hitters[folds==k,],id=i)
        cv.errors[k,i]=mean( (Hitters$Salary[folds==k]-pred)^2)
      }
    }
    rmse.cv=sqrt(apply(cv.errors,2,mean))
    plot(rmse.cv,pch=19,type="b")
    points(which.min(rmse.cv),rmse.cv[which.min(rmse.cv)],pch=20,col="red")
    
    ## now draw line for 1 SE around the min
    rmse.mat <- sqrt(cv.errors)
    rmse.sd <- apply(sqrt(cv.errors), 2, sd)[which.min(rmse.cv)]/sqrt(10)
    rmse.cv[which.min(rmse.cv)]
    se.bands=c(rmse.cv[which.min(rmse.cv)]+rmse.sd,rmse.cv[which.min(rmse.cv)]-rmse.sd)
    abline(h=se.bands,col="blue",lty=2)
    legend("topright",legend=c("Min Test RMSE","1 SE line"),col=c("red","blue"),pch=c(19, NA), lty=c(NA, 2))>
222:<I think there wasn't enough instruction to accomplish this task. Maybe during the course would be great to have more programming assingments, as the students could be more prepared to work with exercices like these. 

I'm a seasoned R user, and knew the book. So i had no problems to do the task. But i think that a course like this still is lacking program assignments.  

Now i will provide some help. To generate the data set use mvrnorm() that generate a sample from a multivariate normal distribution. You will need the MASS package, so:

> library(MASS)
> sample <- mvrnorm(n=50, mu=rep(0,10), Sigma=diag(10))

you just have to provide n=number of samples, mu=vector mean and Sigma that in this case is a identity matrix. I'm using diag(n) that is a easy way to generate this matrix.

Once you generate the samples, two matrixes with 10 columns each, put both together with rbind. For example:

> train_data <- rbind(sample1, sample2)  # a 100 x 11 matrix

To finish just generate y, a 100 length vector.

> y <- c(rep(1,50), rep(0,50))

E put together:

> train_data$y <- y

Done!

To finish the simulation use a for:

for (1 in 1:1000) {

      ## Generate traning data
     
      ## Fit a model on trainning data

      ## Generate test data

      ## Predict on test  data
 
      ## Compare prediction with y

      ## Save the result on a vector

      error_rate[i] <- ## result

}


In the end jus calculate the mean:

> mean(error_rate)

Good luck!>
223:<Agree that the answer was correct (I got it right on the first try :-).  My suggestion was that the explanation was unclear.  Not a big deal, just trying to improve the materials.>
224:<I have been searching and it seems the standard way is to translate the categorical variable (with $N$ different values) into $N-1$ boolean dummy variables. Well, it does make some kind of sense :-)>
225:<This is really helpful :) And it reminds me to always keep for loop in mind...>
226:<I have an issue with this approach. The fact that they didn't look at the class labels is OK, but they looked at the test data and that is problematic. Seems to me that you first should split the data into train/test parts and then to pick the 500 genes that have the highest variance within the training part. But then again, when you split to the train/test parts, why not look to pick 500 genes that are the best while looking at the class labels? Seems to me that we should be fine doing whatever we want with the training data, just we need to make sure that the test data stays untouched :)>
227:<I guess you can performance regression (on R for example), and the features with a p-value < 0.05 (non significants tho), could be removed from the model. So I guess the p-value could be a good criteria. Not sure if you were asking about that, but I hope that helps =)>
228:<Hi everyone! 

I'm working with metheorological data, and I want to improve some prediction model by performing linear regression. So I have the response ( real wind speed) and two different prediction variables (prediction wind speed and prediction wind direction). I also have some other real data (like direction wind speed, temperature, moisture...), but what i'd really like to predict is the real wind speed. 

Eventho when I just have two predictors as I said, (wind speed prediction and wind directin prediction), I made some new predictors in order to raise the R^2 adj of the model (with lag and post variables, I mean, the same variables but at different +1, +2, +3... -1, -2... hours), but obviously, that variables are correlated with each other. I also worked with wind direction as factors (by quadrants) and harmonics (sin and cos). 

It's quite complicated to explain, but my question is, which methods should be better to perform my regression model? To know which are the best predictors. Btw, I'm working with R. 

I'd really appreciate any help! 

Cheers =)>
229:<I got totally swamped by the questions at the end of the R-tutorial, which didn't have much relation at all to what was in the actual tutorial video.  I cobbled together this bit of code from the ISLR book (p39 to 362), as it seemed like what we'd need to do was work out a truth table and then the test error rate.  However, the instructions around the question I found hopelessly unclear, and the code in the book didn't actually work.  The points where it was unclear are in the breaks below, as is the point where I got an error.

If anyone else has attempted this and gotten further than me, I'd be grateful for some pointers.  I could follow what svm was supposed to do in the text, and in the tutorials, and in the R lectures, but the questions didn't seem to bear any real resemblance to anything in the lectures.

set.seed(1)
library(e1071)
x=matrix(rnorm(50*2),ncol=2)
y=c(rep(-1,25),rep(1,25))
x[y==1,]=x[y==1,]+1

the three lines above are generating some random vectors for x and y, but I have no clue as to whether they are the same as the ones in the question, and I don't really understand what this bit of code is doing anyway; x is a 50*2 matrix with two columns, but I'm not sure if it's picking up the pattern for x required by the question, and I am not sure what the description of y even means.  Finally, I don't know what the third line's purpose is at all.

dat=data.frame(x,y=as.factor(y))
svmfit1=svm(y~.,data=dat,cost=10,scale=FALSE)
summary(svmfit1)
set.seed(1)
tune.out=tune(svm ,y?.,data=dat,ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ))
summary(tune.out)
bestmod=tune.out$best.model
summary(bestmod)
xtest=matrix(rnorm(50*2),ncol=2)
ytest=sample(c(-1,1),50,rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,]+1
testdat=data.frame(x=xtest,y=as.factor(ytest))
ypred=predict(bestmod,testdat)

when I got to here, I got an error message "Error in eval(expr, envir, enclos) : object 'X1' not found".  I can't see anything missing from what was in the book, and all the rest of the code above worked.

table(predict =ypred,truth= testdat$y)>
230:<:-)>
231:<SVM splits on many variables, and as long as there are not checkerboard patterns, should do a good job with one vs. one or one vs. all. Thus the set of SVM models forms a set of rules. There are some research projects that have used SVMs to create decision boundaries for trees (BUTIA, SVMODT) but I don't know of an R package that does this.

Edit: Adding cubist<br>
R package "cubist" (Quinlan's C5.0 for regression trees):<br>
http://cran.r-project.org/web/packages/Cubist/index.html<br>
http://cran.r-project.org/web/packages/Cubist/vignettes/cubist.pdf

R package "oblique.tree":<br>
http://cran.r-project.org/web/packages/oblique.tree/index.html <br>
Here is a page with a graph of data and the tree created:<br>
http://statistical-research.com/a-brief-tour-of-the-trees-and-forests/


Steven Salzberg's OC1 decision tree:<br>
http://www.cbcb.umd.edu/~salzberg/announce-oc1.html  (old)<br>
http://ccb.jhu.edu/people/salzberg/Salzberg/Software.html  (current)

Murthy, S. K., Kasif, S., & Salzberg, S. (1994). <br>
A system for induction of oblique decision trees. <br>
Journal of Artificial Intelligence Research, 2(1), 1-32. <br>
http://www.jair.org/media/63/live-63-1401-jair.pdf <br>
http://arxiv.org/pdf/cs/<phoneRedac>.pdf

Sreerama Murthy1, Simon Kasif1, Steven Salzberg1, Richard Beigel2, <br>
OC1: Randomized induction of oblique decision trees <br>
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.1037&rep=rep1&type=pdf <br>
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.6304>
232:<I think that fitted values and standard errors transformation should precede the construction of se.bands (lines 77-88)

I used the following commands to produce the standard error bands and I got different results.

    preds2=predict(fit,list(age=age.grid),se=T,type="response")
    se.bands2=preds2$fit +
    cbind(fit=0,lower=-2*preds2$se,upper=2*preds2$se)>
233:<"#Courseware Question 9.R.1
"#Reference ISLR pages 359-362 with radial kernel substituted

"#Load SVM and Set up for loop to capture errors from 1000 train/test runs

library(e1071)

error = rep(0,1000)
for (i in 1:1000) {

  #Generate training data set with 50 observations from one class and 50 observations from the other
  x1=matrix(rnorm(50*10), ncol=10)
  y1=c(rep(0, 50))
  x2=matrix(rnorm(50*10), ncol=10)
  x2[,1:5]=x2[,1:5] + 1
  y2=c(rep(1,50))
  xdat1=rbind(x1,x2)
  ydat1=c(y1,y2)
  traindat=data.frame(x=xdat1, y=as.factor(ydat1))
  
  #Fit default support vector classifier
  svmfit=svm(y~., data=traindat)
    
  #Generate test data set
  x1test=matrix(rnorm(50*10), ncol=10)
  y1test=c(rep(0, 50))
  x2test=matrix(rnorm(50*10), ncol=10)
  x2test[,1:5]=x2test[,1:5] + 1
  y2test=c(rep(1,50))
  xdat1test=rbind(x1,x2)
  ydat1test=c(y1,y2)
  testdat=data.frame(x=xdat1test, y=as.factor(ydat1test))
  
  #Predict the class labels of these test observations using default svmfit and calculate the error
  ypred=predict(svmfit, testdat)
  error[i]=(sum(ypred != testdat$y))/100
}
hist(error)
mean(error)
sd(error)>
234:<When you watch the video there is on the top like a button/tab...
yes... I know... the user interface is awful (doesn't honor the very good course content ;)   )>
235:<I got ambitious and ran randomForest along the same line as indicated in chapter 9.R section. 
I then ran the code that Dr.Hastie used to generate the error plots with obvious modifications. Here is the piece of code I used:

oob.err=double(10)
test.err=double(10)
for(mtry in 1:10){
  
  rf.fit = randomForest(trainData,as.factor(trainResponse),mtry=mtry)
 
  oob.err[mtry] =(rf.fit$confusion[1,2] + rf.fit$confusion[2,1])/sum(rf.fit$confusion[,-3])
  rf.pred=predict(rf.fit, newdata = testData)
  
 
  test.err[mtry] = mean(rf.pred != testResponse)

  cat(mtry," ")

}


matplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c("red","blue"),type="b",ylab="")
legend("topleft",legend=c("OOB","Test"),pch=19,col=c("red","blue"))

since I was using classification, I found the OOB error rate the hard way (there must be other slicker way to find it that I don't know). 

![enter image description here][1]

However, it doesn't match the figure generated by Dr.Hastie in the file ch8.Rmd . Can anybody explain the difference here. Thanks a lot.
[1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>86.jpg>
236:<Hello all, 

I've followed all of the advice for doing 9.R.1, but my three submitted answers are still incorrect. Could someone please provide some advice? 

**I'm guessing I've made a mistake with the simulated dataset?** I've looked over the related threads and use the mvnorm() function, following the suggestions. (Thanks to everyone commenting on this problem!)

I've tried to create the 1) training dataset and fit the model, 2) create the larger test data set, calculate the test error --- and then repeat this process in the Monte Carlo storing the test error as an element in the test error vector.  

I don't know if it would violate the rules to dump the entire code, so here's the gist and some code mostly from the right side of the <- operator:

    initialize the error vector
    start the i in 1:1000 for loop {
          x1<-mvrnorm(n=50, mu=rep(0,10), Sigma=diag(10)) 
          # do the draws and store as object

          x2<-mvrnorm(n=50, mu=rep(c(1,0), c(5,5)), Sigma=diag(10)) 
          # thanks to you all on the other threads
    
       train<-rbind(x1,x2)
     classes<-rep(c(0,1),c(50,50))
         dat<-data.frame(train,classes=as.factor(classes))
      svmfit<-svm(classes~.,data=dat)
              predict(svmfit,dat)
    
          # is the problem in the test dataset mvnorm()?
    
              mvrnorm(n=500, mu=rep(0,10), Sigma=diag(10))
              mvrnorm(n=500, mu=rep(c(1,0), c(5,5)), Sigma=diag(10))
              rbind(x1a,x2a)
              rep(c(0,1),c(500,500))
              data.frame(test, classest=as.factor(classest))
              svm(classest~.,data=testdata )
              predict(svmfitt,testdata)
              sum(testdata$fit1 != testdata$classest)/1000 
          
              # calc error rate on element [i]
    } 
    
    mean(errorrate) # doesn't give me the correct answer :(


Could anyone provide some advice about where I'm not understanding how to correctly answer the problem? 

Thanks very much!>
237:<Remember that Stanford is in the Pacific time zone. Give them some time to wake up. :-)

They have consistently opened the new week at some point every Saturday, but not first thing in the morning.>
238:<Hello,
I think I am doing 9.R.1 okay but if I want to do
plot(svmfit.dat) I get the error below. Any idea?

set.seed(105)

x=matrix(rnorm(1000),100,10)

dat=data.frame(x,y=as.factor(y))

svmfit=svm(y~.,data=dat,kernel="radial",scale=FALSE)

plot(x,col=y+3,pch=19)

print(svmfit)

plot(svmfit,dat)

Error in plot.svm(svmfit, dat) : missing formula.>
239:<Minor note: you did not specify `y` in your code. 

Re: the error, when there are more than two variables you have to specify a formula indicating which two input variables you want to be displayed. For example, for your data, give this a try. 

    plot(svmfit, data=dat, formula=X1 ~ X2)>
240:<Why does my code in case of logistic regression results in mean error of 0.004?

library(e1071)
set.seed(1)
nvar=10
ntrain=50
ntest=100
n=1000
err=vector(mode="numeric", length=n)
modeltype="logistic"
kernal="linear"

for(i in 1:n) {
  if (i %% 100 ==0) {
    print(i)  
  }
  #Train
  dat1=data.frame(x=matrix(rnorm(ntrain*nvar), ncol=nvar), y=as.factor(rep(0,ntrain)))
  x2=cbind(matrix(rnorm(ntrain*(nvar/2),mean=1,sd=1), ncol=nvar/2),matrix(rnorm(ntrain*(nvar/2),mean=0,sd=1), ncol=nvar/2))
  dat2=data.frame(x=x2, y=as.factor(rep(1,ntrain)))
  dat=rbind(dat1,dat2)  
  
  if (modeltype=="svm") {
    fit=svm(y~., data=dat, kernal=kernal)
  }
  else if (modeltype=="logistic") {
    fit=glm(y~., data=dat, family=binomial()) # Should reproduce same result als linear SVM kernel but does not
  }
  
  #Test
  dat1=data.frame(x=matrix(rnorm(ntest*nvar), ncol=nvar), y=as.factor(rep(0,ntest)))
  x2=cbind(matrix(rnorm(ntest*(nvar/2),mean=1,sd=1), ncol=nvar/2),matrix(rnorm(ntest*(nvar/2),mean=0,sd=1), ncol=nvar/2))
  dat2=data.frame(x=x2, y=as.factor(rep(1,ntest)))
  dat=rbind(dat1,dat2)
  ypred=predict(fit, dat)
  t=as.matrix(table(predict=ypred, truth=dat$y))
  err[i] = (t[1,2]+t[2,1])/length(ypred)  
}

mean(err)>
241:<I'm not sure if it is ok to post code here, so admin please notify me if I need to remove it. 

Basically, I am generating the data (including train and test), than sampling 50 from them to train, finally testing the model and the rest data. The result I got for all three method (raidal svm, linear svm, logistic regression) all resulted in accuracy around 83%, which were seen as incorrect when I submit them. 

    set.seed(101)
    require(MASS)
    nobs = 550 # per class
    cv.ntrain = 50
    nfeat = 10 # number of features
    nsimu = 1000 # number of simulation (cross-validation)
    
    # Generate multi-normal distributions for each class
    # Given yi=0, xi?N10(0,I10).
    mu0    = rep(0, nfeat)
    sigma0 = diag(nfeat)
    x0     = mvrnorm(nobs, mu0, sigma0)
    y0     = rep(0, nobs)
    dat0   = data.frame(x0, y = factor(y0))
    
    # Given yi=1, xi?N10(?,I10) with ?=(1,1,1,1,1,0,0,0,0,0).
    mu1    = rep(c(1,0), each = nfeat/2)
    sigma1 = diag(nfeat)
    x1     = mvrnorm(nobs, mu1, sigma1)
    y1     = rep(1, nobs)
    dat1   = data.frame(x1, y = factor(y1))
    
    # start monte carlo simulation of svm classification
    # randomly sample 50 obs from each class, and test on the rest
    svm.accu = matrix(0, ncol=3, nrow=nsimu)
    nobs.test = 2 * (nobs - cv.ntrain)
    for (i in 1:nsimu) {
      idx.train = sample(1:nobs, cv.ntrain, replace = FALSE)
      dat.train = rbind(dat0[idx.train, ], dat1[idx.train, ])
      dat.test  = rbind(dat0[-idx.train, ], dat1[-idx.train, ])
      
      fit.radial = svm(y~., data=dat.train, kernel="radial")
      fit.linear = svm(y~., data=dat.train, kernel="linear")
      fit.logit  = glm(y~., data=dat.train, family="binomial")
      
      pred.radial= predict(fit.radial, dat.test[, 1:nfeat])
      pred.linear= predict(fit.linear, dat.test[, 1:nfeat]) 
      pred.logit = predict(fit.logit, dat.test[, 1:nfeat]) 
    
      svm.accu[i,1]  = sum(pred.radial == dat.test$y) / nobs.test
      svm.accu[i,2]  = sum(pred.linear == dat.test$y) / nobs.test
      svm.accu[i,3]  = sum(ifelse(pred.logit >0.5, 1, 0)  == dat.test$y) / nobs.test
    }>
242:<I am also struggling with this question.
At present, I have the following script:

train <- cbind(y,x)

train= as.data.frame(train)

fit<- lm(y~,data=train)

yhat=predict(fit,x.test)

mean(yhat-y.test)^2

and I am unable to obtain the correct answer.
Your feedback is appreciated.

Best wishes,>
243:<I think because the `predict` functions work slightly different between `svm` and `glm`. Here's the relevant parts. 

    if (modeltype=="svm") { 
      fit=svm(y~., data=dat, kernal=kernal) 
      ypred=predict(fit, testdat) 
    } else if (modeltype=="logistic") { 
      fit=glm(y~., data=dat, family="binomial")
      ypred=predict(fit, testdat, type="response")
      ypred=ifelse(ypred < 0.5, 0, 1) 
    }>
244:<Could someone please help me on this.

#create the train set for regression
xols<-pca.out$x[1:300,1:5] 

#run the regression regfit=y~xols

#create the test set with the remaining 1000 observations
xtest=pca.out$x[301:1300,1:5]

#use the predict function to predict y based on xtest (i.e. 1000 obs)
predict(regfit,data.frame(xtest))

But I get the following warning message:
****Warning message:
'newdata' had 1000 rows but variables found have 300 rows**** 

I can't figure out my mistake. As i have 1000 rows in xtest, I would expect to get the same number of predicted values, which I would use to find the MSE (as y.test also has 1000 obervations).

Thanks for the help!>
245:<For some reason the predict function for lm model (10.R.2) does not work as expected with the train set (i.e. 1000 obs). I keep on getting predictions for the first 300 obs (although when I check it the test data has 1000 lines).
Can you help with this?

xtest=pca.out$x[301:1300,1:5]

predict(reg,data.frame(xtest))

(reg is the regression fitted on training set (first 300 obs))

I a warning message (***Warning message: 'newdata' had 1000 rows but variables found have 300 rows***)

Thanks a lot!

Cheers!>
246:<Yes, that does the job and it struck me why. Logistic regression models the **probability** of an outcome and not the actual outcome itself. So the result of: 

    ypred=predict(fit, testdat, type="response")

is a vector of probabilities.>
247:<This is a subtle question.  We can think of the loadings in PCA as estimates of the "population loadings," which would be the eigenvectors of the population correlation matrix.  In the limit where the data set is very large and the number of variables is fixed, the loading $hat v_{j,k}$ (loading of variable $j$ for PC $k$) <nameRedac_anon_screen_name_redacted> be roughly normal, centered on $v_{j,k}$ with a small variance that is shrinking at rate $1/n$.

But suppose the first two eigenvalues are pretty close to one another.  Then, if we ran PCA on some finitely-sized data set, the eigenvalues might be "swapped" in that sample: the first set of sample PC loadings might be very close to the second set of population PC loadings, and the second sample PC loadings very close to the first population PC loadings.  Then in the literal sense of "standard error of $hat v_{j,1}$," you got very inaccurate estimates of the PC loadings because $hat v_{j,1} approx v_{j,2} 
eq v_{j,1}$.  But maybe it's more appropriate to think you actually got good estimates of the PC loadings, but with the order just swapped.

tl;dr: it's complicated and there's not a pat answer.>
248:<@imag: 

I'm hoping this is a minimal example to show how the `plot.svm` works. If there are more than two variables in the data frame you give it, you have to specify the column names in the data frame so it knows which two of them to select (as I guess the function only makes 2D plots). So, not the columns, just their *names*. That's because it's just a formula. (Typically in R, where a formula option is available, you are giving the relevant names in the formula and the data for where to look, or the actual vectors, but not both.) 

Please let me know if you have any further questions. 

    require('e1071')
    set.seed(105)
    x=matrix(rnorm(1000),100,10)
    y=sample(c(0,1), size=100, replace=TRUE)
    dat=data.frame(x,y=as.factor(y))
    head(dat)  # note the column names, we'll use these in the plotting formula
    names(dat)  # the column names explicitly
    svmfit=svm(y~.,data=dat,kernel="radial",scale=FALSE)
    # The input variables in the formula must match column names in 'dat'
    plot(svmfit, data=dat, formula=X1 ~ X2)>
249:<Thanks very much for all the responses; I'm not sure I learned a whole lot about SVM in answering this question, but by going through the lines of code everyone posted, I sure learned a lot about generating variables, putting them into vectors and matrices, and running simulations multiple times.  If the professors ever ran a course called "tricks for dummies in R", whereby they just had a dozen or so R projects which each exemplified a different trick (even if it was all in something simple, like OLS), or a series of projects where you start with a bunch of data and go through the process of looking at it, screening it and so on as a statistician would, I'd sign up in a second.

Two asides.  Firstly, when I tried DWaterson's code above, I got very low error rates, but when I used the other code for mvrnorm, I got error rates comparable with the right answers.  I'm not really sure why that should have been the case.

Secondly, I ran into problems with the logistic regression question because all I did was swap the svm line above for a glm one (glm.fit=glm(y~.,data=dat,family="binomial")).  It turns out that you need to change the error command as well, because predict in glm gives you the probabilities, not the actual classes the prediction falls into.  If others had the same problem, I found the solution here:
https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/531dfcf<phoneRedac><zipRedac>f<zipRedac>6<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>a

Note, however, that the suggested replacement error function gives you the total number of errors; you need to divide by the size of your tests set to get the rate.>
250:<I had a bad time with this item :-( only got OK at 3rd trial.

I was fooled by the phrase *"using the first five principal components ***(computed on rbind(x,x.test))*** instead as low-dimensional derived features"* and due to the bold comment I regressed an lm() in the PCA vectors obtained with all the data!

If the lm() model is trained in the 5 first PCA vectors coming for all the data (x and x.test merged with rbind()) and then it is used to predict y.test, we get a wrong answer. So beware, *you must train using only x to get the first 5 PCA,* then apply lm() to these reduced features in the test data and then predict in the (x.test,y.test) data set and calculate the MSE. 

To get the first 5 PCA vectors you do:

    pr.out = prcomp (x , scale =TRUE)
    rot = pr.out$rotation[,1:5]

To get the 5 dimension reduced features in train data

    xred=pr.out$x[,1:5]     # the reduced features

Or also works this

    xmat=data.matrix(x, rownames.force = NA)
    xred = xmat %*% rot

To get the 5 dimension reduced features in test data

    xtestmat=data.matrix(x.test, rownames.force = NA)
    xtestred = xtestmat %*% rot

And from here you fit the lm(), predict, etc... switching between maatrices and data frames...

Note that in the reduced data you can use simply 

    lmfit=lm(y~.,data=...)

without worrying with boundaries.

HTH

Tony>
251:<**Ch1 Introduction**

1.1 Opening Remarks
https://www.youtube.com/watch?v=2wLfFB_6SKI
 
1.2 Examples and Framework
https://www.youtube.com/watch?v=LvaTokhYnDw



**Ch2 Overview of Statistical Learning**
 
2.1 Introduction to Regression Model
https://www.youtube.com/watch?v=WjyuiK5taS8

2.2 Dimensionality and Structured Models
https://www.youtube.com/watch?v=UvxHOkYQl8g

2.3 Model Selection and Bias-Variance Tradeoff
https://www.youtube.com/watch?v=VusKAosxxyk

2.4 Classification
https://www.youtube.com/watch?v=vVj2itVNku4

2.R Introduction to R
https://www.youtube.com/watch?v=jwBgGS_4RQA

Interview with John Chambers
https://www.youtube.com/watch?v=jk9S3RTAl38



**Ch3 Linear Regression**

3.1 Simple Linear Regression
https://www.youtube.com/watch?v=PsE9UqoWtS4

3.2 Hypothesis Testing and Interval Confidence
https://www.youtube.com/watch?v=J6AdoiNUyWI

3.3 Multiple Linear Regression
https://www.youtube.com/watch?v=1hbCJyM9ccs

3.4 Some Important Questions
https://www.youtube.com/watch?v=3T6RXmIHbJ4

3.5 Extensions of the linear models
https://www.youtube.com/watch?v=IFzVxLv0TKQ

3.R Linear Regression in R
https://www.youtube.com/watch?v=5ONFqIk3RFg



**Ch4 Classification**

4.1 Introduction to Classification Problems
https://www.youtube.com/watch?v=sqq21-VIa1c

4.2 Logistic Regression
https://www.youtube.com/watch?v=31Q5FGRnxt4

4.3 Multivariate Logistic Regression
https://www.youtube.com/watch?v=MpX8rVv_u4E

4.4 Logistic Regression - Case Control Sampling and Multiclass
https://www.youtube.com/watch?v=GavRXXEHGqU

4.5 Discriminant Analysis
https://www.youtube.com/watch?v=RfrGiG1Hm3M

4.6 Gaussian Discriminant Analysis - One Variable
https://www.youtube.com/watch?v=QG0pVJXT6EU

4.7 Gaussian Discriminant Analysis - Many Variable
https://www.youtube.com/watch?v=X4VDZDp2vqw

4.8 Quadratic Discriminant Analysis and Naive Bayes
https://www.youtube.com/watch?v=6FiNGTYAOAA

4.R Classification in R.A
https://www.youtube.com/watch?v=TxvEVc8YNlU

4.R Classification in R.B
https://www.youtube.com/watch?v=2cl7JiPzkBY

4.R Classification in R.C
https://www.youtube.com/watch?v=9TVVF7CS3F4



**Ch5 Resampling Methods**

Interview with Bradley Efron
https://www.youtube.com/watch?v=6l9V1sINzhE

5.1 Cross-Validation
https://www.youtube.com/watch?v=_2ij6eaaSl0

5.2 K-fold Cross-Validation
https://www.youtube.com/watch?v=nZAM5OXrktY

5.3 Cross-Validation: the wong and right way
https://www.youtube.com/watch?v=S06JpVoNaA0

5.4 The Bootstrap
https://www.youtube.com/watch?v=p4B>
252:<On the bracket challenge, you have like a 1 in 9 quintillion chance of winning. (For those who don't know about it:(https://tournament.fantasysports.yahoo.com/quickenloansbracket/challenge/) 

 This guy has given it some serious thought: [https://www.youtube.com/watch?v=pdnxTr6hG14][1]

I don't have suggestions for data, but second the call for references on any stat/machine learning approaches that have proven successful --- for having the smallest error rate in the office pool! 


  [1]: https://www.youtube.com/watch?v=pdnxTr6hG14>
253:<Thank you, Dr. Hastie, Dr. Tibshirani and Dr. Witten for a top course in Statistical Learning. I've learned a lot.

Great explanations and a fantastic book. I got it! Now it's time to deep in "The Elements of Statistical Learning"... Maybe it will be another course. If so, count on me!

Thank you to students that helped me with some tricky questions. At last there's light!

:)>
254:<I had the exact same problem in my first attempt to put factorial in the denominator. The I read the question few more time, it emphasize "naive". So just omit the denominator and be really naive:)>
255:<Hello Tony, thanks for this tip!
> I was fooled by the phrase "using the first five principal components (computed on rbind(x,x.test)) instead as low-dimensional derived features" and due to the bold comment I regressed an lm() in the PCA vectors obtained with all the data!

Also, I believe that you made some errors in the code that you posted:

> `xmat=data.matrix(x, rownames.force = NA)`
> `xred = xmat %*% rot`

If you want to calculate to apply the projection of the test data on your just obtained principal components you need to run the following code (note that pca.out is the output from the prcomp(...) function):

    rot = pca.out$rotation[,1:5]
    center = pca.out$center
    center = matrix(rep(center,times=nrow(x)), nrow=nrow(x), ncol=ncol(x), byrow=TRUE)
    scale   = pca.out$scale
    scale   = matrix(rep(scale,times=nrow(x)), nrow=nrow(x), ncol=ncol(x), byrow=TRUE)
    
    center[1:10,1:5]
    x[1:10,1:5]
    (x-center)[1:10,1:5]
    scale[1:10,1:5]
    ((x-center)/scale)[1:10,1:5]
    
    xmat=data.matrix( (x-center)/scale , rownames.force = NA)
    xred = xmat %*% rot
    xred[1:10,1:5]
    x.train.pca5[1:10,1:5]

Anyway, thanks for your help!!>
256:<Christian and others,

Thanks for your suggestions and sharing your problems. It is good to see I'm not the only one struggling here :). Regarding Christian's solution, I get within the boundaries of the right answer, but I don't get the exact answer (differences are respectively (-0.0957 and -0.25). Somehow, the proposed solution thus underestimates the value of the MSE we are actually looking for. Anyone a clue on how to get closer to the exact solution?>
257:<Hi,
someone can help me with this sintaxis, I can't find the mistake but results seems to be wrong.

Thanks a lot

k=1000; #test sample size

M=1;    #mean

S=1;    #variance
samples=100

ER=rep(0,samples)

for (i in 1:samples){

x=matrix(rnorm(1000,mean=M,sd=S),100,10)

y=rep(c(0,1),c(50,50))

x[y==1,]=x[y==1,]-1

xtest=matrix(rnorm(10*k,mean=M,sd=S),k,10)

ytest=rep(c(0,1),c(k/2,k/2))

xtest[ytest==1,]=xtest[ytest==1,]-1

dat=data.frame(x,y=as.factor(y))

test=data.frame(xtest,ytest=as.factor(ytest))

fit=svm(y~.,data=dat)

ypred=predict(fit,xtest,data=test)

ER[i]=1-mean((ypred==ytest))

}; mean(ER)>
258:<Maybe you need something like this:

dat.lm=data.frame(x,y)>
259:<I also want to thank you for an excellent course. This course illuminated many of my questions in data analysis. The course is beautifully structured and therefore helped me see the big picture, but what impressed me most is how simple, compact and understandable professors explain such rather complicated stuff! Also before this course, I did not even know about the possibilities of R andhow to use it:-) (I used Matlab), but now I feel myself as confident user of R thanks to you!:-) Many-many times thanks again! You did a great job!>
260:<Any advice or suggestions will be valuable for me! 

My research for PhD relate mainly to  model selection approaches and feature selection methods (in regression tasks). I used something like Forward Stepwise Selection, Floating Forward Selection and some others my own modifications of some similar methods. Due to this online course, I discovered also ridge regression and Lasso algorithm that can also be helpful to me (at least for the comparison of models obtained by different methods).

But in my country (Ukraine) as well as in Russia it is very popular to use the Group Method of Data Handling (GMDH , http://en.wikipedia.org/wiki/Group_method_of_data_handling). There are a lot of information about this method in Russian language (at least because a creator of this method was the Ukrainian scientist Ivakhnenko and there are a scientific groups and schools that actively develop this method now). 


Nevertheless, reputable professors of this online course did not say a word about this method. I just need to know, this is because you maybe not even have heard about it? Or maybe you used it before but it has not proved helpful in the analysis of real data and it turned out to be the worst method?

It is important for me to understand whether or not to spend my limited time (and nerves) to study GMDH? On the one hand, Ukrainian group of scientists claim that it is "the best of the best techniques" (so they say), but on the other hand, world known scientists in the field of statistical learning say nothing about this method. Where is the truth? :-)

It seems to me quite doubtful and excessive (and, among other things, it is quite timeconsuming for software implementation).

I need authoritative advice or opinion, whether or not to include this method for comparison with other methods. If someone used it, please share your experience and impression of its use. 

Hope, the wording of my question is clear enough in my English. If not, then sorry for my English :-)>
261:<>First, can the methods presented be used to model feedback loops? For example, can I use regression when the response affects future values of the features?

Google for ARIMA, GARCH models. In this case, the simple linear model is not appropriate.

>Second, what if the phenomena being modeled is changing or being disturbed during the period of time during which data is being collected. In that case, even my best cross-validated model may be invalid.

Did you ask how frequently do you have to update the coefficients in the model? If yes, then the general answer to your question: it depends.

It seems like you want to predict the stock market prices :)>
262:<I'd like to a big thank you to Trevor Hastie and Rob Tibshirani! And everyone else involved with the course too! It's been great fun and an excellent opportunity!

I'm busy with my Masters degree here in South Africa. The topic of my dissertation is the Lasso and I'm learning so much working through research articles.

The Elements of Statistical Learning is my bible! I absolutely love it, I'm amazed over and over by how much knowledge is squeezed in between the covers of that book! The new book is great too, very easy to digest! A huge thank you for the books as well!!

I really enjoyed the lecture videos and especially the interviews. It's so great to put some personality to the authors of articles and books that I've been studying :-)

Overall it's been a wonderful experience... once again, many thanks!!!

~<nameRedac_anon_screen_name_redacted>>
263:<I am doing the lab of Chapter 5 and run the bootstrapping code. It returns:

> set.seed(1)
> alpha.fn(Portfolio,sample(100,100,replace=T))
[1] 0.<phoneRedac>
> boot( data=Portfolio , statistic=alpha.fn , R=1000)
Error in boot(data = Portfolio, statistic = alpha.fn, R = 1000) : 
  could not find function "isMatrix"

I don't know what's going on. Does it mean that I have to transfer the data to a matrix?>
264:<Hi Rob, Trevor and team!

Thanks a ton for conducting such a fantastic course and making it all so simple :) Looking forward to some more advanced courses covering unsupervised learning in more detail I hope!!>
265:<Not managed this on a windows machine.

Installed python then youtube-dl.  Created Videos folder, saved links.txt and download_vids.py into Videos folder.

Set working directory from python.exe using command: 

os.chdir('E:DropboxData AnalysisStatLearningVideos')

Tried to run:
python download_vids.py links.txt

Returns SyntaxError: invalid syntax on the 's' of download_vids.py>
266:<Have tried several other ways of doing this with a common error of the script failing at line:
c:Python34>python download_vids.py links.txt
  File "download_vids.py", line 14
    print "incorrect number of arguments"
Do I need to amend the script in order to use it?                                        ^>
267:<I recommend saving yourself the hassle of using python by instead doing the following:

1) create a folder called videos

2) save youtube-dl.exe to that folder

3) open notepad, copy & paste the content at the bottom of this post

4) save file to videos folder using 'save-as', dropdown to 'all files', and call the file: somename.cmd

5) double-click on somename.cmd

youtube-dl.exe		https://www.youtube.com/watch?v=6l9V1sINzhE

youtube-dl.exe		https://www.youtube.com/watch?v=jk9S3RTAl38

youtube-dl.exe		https://www.youtube.com/watch?v=uQBnDGu6TYU

youtube-dl.exe		https://www.youtube.com/watch?v=DCn83aXXuHc

youtube-dl.exe		https://www.youtube.com/watch?v=jwBgGS_4RQA

youtube-dl.exe		https://www.youtube.com/watch?v=5ONFqIk3RFg

youtube-dl.exe		https://www.youtube.com/watch?v=6ENTbK3yQUQ

youtube-dl.exe		https://www.youtube.com/watch?v=GfPR7Xhdokc

youtube-dl.exe		https://www.youtube.com/watch?v=hPEJoITBbQ4

youtube-dl.exe		https://www.youtube.com/watch?v=lq_xzBRIWm4

youtube-dl.exe		https://www.youtube.com/watch?v=U3MdBNysk9w

youtube-dl.exe		https://www.youtube.com/watch?v=2cl7JiPzkBY

youtube-dl.exe		https://www.youtube.com/watch?v=9TVVF7CS3F4

youtube-dl.exe		https://www.youtube.com/watch?v=TxvEVc8YNlU

youtube-dl.exe		https://www.youtube.com/watch?v=6dSXlqHAoMk

youtube-dl.exe		https://www.youtube.com/watch?v=YVSmsWoBKnA

youtube-dl.exe		https://www.youtube.com/watch?v=3kwdDGnV8MM

youtube-dl.exe		https://www.youtube.com/watch?v=mv-vdysZIb4

youtube-dl.exe		https://www.youtube.com/watch?v=F8MMHCCoALU

youtube-dl.exe		https://www.youtube.com/watch?v=1REe3qSotx8

youtube-dl.exe		https://www.youtube.com/watch?v=1REe3qSotx8

youtube-dl.exe		https://www.youtube.com/watch?v=0wZUXtvAtDc

youtube-dl.exe		https://www.youtube.com/watch?v=IY7oWGXb77o

youtube-dl.exe		https://www.youtube.com/watch?v=QpbynqiTCsY

youtube-dl.exe		https://www.youtube.com/watch?v=xKsTsGE7KpI

youtube-dl.exe		https://www.youtube.com/watch?v=xKsTsGE7KpI

youtube-dl.exe		https://www.youtube.com/watch?v=dm32QvCW7wE

youtube-dl.exe		https://www.youtube.com/watch?v=mI18GD4_ysE

youtube-dl.exe		https://www.youtube.com/watch?v=2wLfFB_6SKI

youtube-dl.exe		https://www.youtube.com/watch?v=LvaTokhYnDw

youtube-dl.exe		https://www.youtube.com/watch?v=L3n2VF7yKkk

youtube-dl.exe		https://www.youtube.com/watch?v=L3n2VF7yKkk

youtube-dl.exe		https://www.youtube.com/watch?v=WjyuiK5taS8

youtube-dl.exe		https://www.youtube.com/watch?v=UvxHOkYQl8g

youtube-dl.exe		https://www.youtube.com/watch?v=VusKAosxx>
268:<you can look at the python code, it's very simple. you don't need to specify a working directory, just run the script where you want the videos to download. you said you saved links.txt and download_vids.py into Videos folder but what you pasted indicates you are in c:Python34.>
269:<@Sithspawn:

> I believe that I have remembered this technique correctly,
> but I would certainly appreciate validation, if possible.

Looks OK.

At the end of the last line, `p-value: <zipRedac>.9575` seems to
correspond to the book:

> This F-test has a p-value of <zipRedac>.96

----
[To format the results of `summary(fit)` a little more clearly:]

    Call:
    lm(formula = Balance ~ Ethnicity, data = credit)
    
    Residuals:
        Min      1Q  Median      3Q     Max 
    -531.<zipRedac><zipRedac> -457.<zipRedac>8  -63.25  339.25 148<zipRedac>.5<zipRedac> 
    
    Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
    (Intercept)          531.<zipRedac><zipRedac>      46.32  11.464   <2e-16 ***
    EthnicityAsian       -18.69      65.<zipRedac>2  -<zipRedac>.287    <zipRedac>.774    
    EthnicityCaucasian   -12.5<zipRedac>      56.68  -<zipRedac>.221    <zipRedac>.826    
    ---
    Signif. codes:  <zipRedac> *** <zipRedac>.<zipRedac><zipRedac>1 ** <zipRedac>.<zipRedac>1 * <zipRedac>.<zipRedac>5 . <zipRedac>.1   1
    
    Residual standard error: 46<zipRedac>.9 on 397 degrees of freedom
    Multiple R-squared:  <zipRedac>.<zipRedac><zipRedac><zipRedac>2188,	Adjusted R-squared:  -<zipRedac>.<zipRedac><zipRedac>4818 
    F-statistic: <zipRedac>.<zipRedac>4344 on 2 and 397 DF,  p-value: <zipRedac>.9575>
270:<Thanks Jeff, another clever R tool :)>
271:<It sure helps me - that was exactly the ambiguity I needed resolved.  What was the mu for each of the 10 dimensions?  1 for half the units in the class, or 1 in half the dimensions?  Sorted!
 - looks like the last five dimensions aren't going to tell us anything useful about the classification variable, being the same in both classes.  But of course we only know that because we're looking at the True Distribution.
:-)  Now I shall go get on with the problem.>
272:<I have noticed that the following videos are missing from the above list:

https://www.youtube.com/watch?v=MEMGOlJxxz0

https://www.youtube.com/watch?v=79tR7BvYE6w

https://www.youtube.com/watch?v=qhyyufR0930

https://www.youtube.com/watch?v=ipyxSYXgzjQ

https://www.youtube.com/watch?v=dbuSGWCgdzw

https://www.youtube.com/watch?v=aIybuNt9ps4

https://www.youtube.com/watch?v=Tuuc9Y06tAc

https://www.youtube.com/watch?v=yUJcTpWNY_o

https://www.youtube.com/watch?v=lFHISDj_4EQ

https://www.youtube.com/watch?v=YDubYJsZ9iM

https://www.youtube.com/watch?v=4u3zvtfqb7w

Class titles for videos at above URL's:

(1) Ch 6:  Interviews with statistics graduate students.

(2) Ch 8:  Interview with Jerome Friedman.

(3) 1st R video tutorial of Ch 9:  9.R SVMs in R.

(4 - 11) Ch 10 Unsupervised Learning>
273:<Tom, your answer is quite close to the actual answer. What is the full MSE you get, without rounding it off? :)>
274:<Now it is over :( I really would like to thank you both for the excellent material presented and knowledge shared, which resulted extremely valuable to my position - Marketing analytics,I am totally convinced statistics is leading Marketing into another whole Era, greetings from Guadalajara, Mexico!!!

As simple as a day to day pleasure watching you both every morning. I already miss you!!

Warm regards,

Gina :)>
275:<It was a great pleasure to listen and practice with both of you and the discussion community. 
Enjoyed too youre great sense of humour ;-)
And actually, I hope see you soon perhaps on future courses :-)>
276:<I agree with veroneti. And for me it was a privilege having the possibility to take a course taught by people who I cite in my Ph.D. thesis! :)>
277:<Good point.  Yeah, I figured if I were to up the iterations by a couple of orders of magnitude, the minor distinctions between efficacy of SVM and LR methods would become more pronounced.  That said, it really did seem like they were very much in the same ballpark.

Thanks for taking the time to reply!  :)>
278:<Hi

I have a curious thing on my code. Before read this entry i had a function in order to generate the data it was

GenData = function(numSamples) {

  mean0 <- c(0,0,0,0,0,0,0,0,0,0)

  mean1 <- c(1,1,1,1,1,0,0,0,0,0)
  

  a2 = mvrnorm(n=numSamples, mu=mean0, Sigma=diag(10))

  b2 = mvrnorm(n=numSamples, mu=mean1, Sigma=diag(10))
  c2 = rbind (a2, b2)

  cframe = data.frame(c2)

  y = c(rep (0, numSamples), rep(1,numSamples))

  ## my nightmare command

  ##cframe$y= as.factor(y)

  cframe$y = y
  
  return (cframe)
}

Easy isn't it? Running my program with this function, i got the same error a constant 0.1 in every value so I thougth something was wrong... Until I read this entry.
After reading, I check my code and found my particular poltergeist if i change 

cframe$y = y 

by 

cframe$y= as.factor(y) 

it works and i get an aparent correct value. 

It is an R question, but what happens here? What is the difference?
Thanks in advance.


PD. Not important, but Numsamples is equals to 50 on first time and 500 on the second.>
279:<me too ana :)>
280:<Dear Prof. Hastie and Prof. Tibshirani,
Thank you for the wonderful course. I really enjoyed it. 
Though not the most important thing, I did want to have the certificate and put it one my resume, both as an encouragement for following through the full course and also as an acknowledgement for your work. :) 
I am wondering, when do we get the certificate, after the course ends? I seem to complete all the quiz questions by now, although not 100% correct,my grade doesn't look bad. :)
Again, thank you!
<nameRedac_anon_screen_name_redacted>>
281:<this paper says that a paired t-test on random train/test splits should "never be used" and cautions against a paired t-test on 10-fold CV results.  

[Approximate Statistical Tests for Comparing Supervised Classication Learning Algorithms][1]


  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&rep=rep1&type=pdf>
282:<nice! :D>
283:<Ch1 Introduction

1.1 Opening Remarks
https://www.youtube.com/watch?v=2wLfFB_6SKI
 
1.2 Examples and Framework
https://www.youtube.com/watch?v=LvaTokhYnDw


Ch2 Overview of Statistical Learning
 
2.1 Introduction to Regression Model
https://www.youtube.com/watch?v=WjyuiK5taS8

2.2 Dimensionality and Structured Models
https://www.youtube.com/watch?v=UvxHOkYQl8g

2.3 Model Selection and Bias-Variance Tradeoff
https://www.youtube.com/watch?v=VusKAosxxyk

2.4 Classification
https://www.youtube.com/watch?v=vVj2itVNku4

2.R Introduction to R
https://www.youtube.com/watch?v=jwBgGS_4RQA

Interview with John Chambers
https://www.youtube.com/watch?v=jk9S3RTAl38


Ch3 Linear Regression

3.1 Simple Linear Regression
https://www.youtube.com/watch?v=PsE9UqoWtS4

3.2 Hypothesis Testing and Interval Confidence
https://www.youtube.com/watch?v=J6AdoiNUyWI

3.3 Multiple Linear Regression
https://www.youtube.com/watch?v=1hbCJyM9ccs

3.4 Some Important Questions
https://www.youtube.com/watch?v=3T6RXmIHbJ4

3.5 Extensions of the linear models
https://www.youtube.com/watch?v=IFzVxLv0TKQ

3.R Linear Regression in R
https://www.youtube.com/watch?v=5ONFqIk3RFg


Ch4 Classification

4.1 Introduction to Classification Problems
https://www.youtube.com/watch?v=sqq21-VIa1c

4.2 Logistic Regression
https://www.youtube.com/watch?v=31Q5FGRnxt4

4.3 Multivariate Logistic Regression
https://www.youtube.com/watch?v=MpX8rVv_u4E

4.4 Logistic Regression - Case Control Sampling and Multiclass
https://www.youtube.com/watch?v=GavRXXEHGqU

4.5 Discriminant Analysis
https://www.youtube.com/watch?v=RfrGiG1Hm3M

4.6 Gaussian Discriminant Analysis - One Variable
https://www.youtube.com/watch?v=QG0pVJXT6EU

4.7 Gaussian Discriminant Analysis - Many Variable
https://www.youtube.com/watch?v=X4VDZDp2vqw

4.8 Quadratic Discriminant Analysis and Naive Bayes
https://www.youtube.com/watch?v=6FiNGTYAOAA

4.R Classification in R.A
https://www.youtube.com/watch?v=TxvEVc8YNlU

4.R Classification in R.B
https://www.youtube.com/watch?v=2cl7JiPzkBY

4.R Classification in R.C
https://www.youtube.com/watch?v=9TVVF7CS3F4


Ch5 Resampling Methods

Interview with Bradley Efron
https://www.youtube.com/watch?v=6l9V1sINzhE

5.1 Cross-Validation
https://www.youtube.com/watch?v=_2ij6eaaSl0

5.2 K-fold Cross-Validation
https://www.youtube.com/watch?v=nZAM5OXrktY

5.3 Cross-Validation: the wong and right way
https://www.youtube.com/watch?v=S06JpVoNaA0

5.4 The Bootstrap
https://www.youtube.com/watch?v=p4BYWX7PTBM

5.5 More on th>
284:<Thanks for that.  I've created this batch file which downloads videos.  Batch file uses youtube-dl which can be downloaded

    youtube-dl.exe -o "1.1 Opening Remarks.mp4" https://www.youtube.com/watch?v=2wLfFB_6SKI
    youtube-dl.exe -o "1.2 Examples and Framework.mp4" https://www.youtube.com/watch?v=LvaTokhYnDw
    youtube-dl.exe -o "2.1 Introduction to Regression Model.mp4" https://www.youtube.com/watch?v=WjyuiK5taS8
    youtube-dl.exe -o "2.2 Dimensionality and Structured Models.mp4" https://www.youtube.com/watch?v=UvxHOkYQl8g
    youtube-dl.exe -o "2.3 Model Selection and Bias-Variance Tradeoff.mp4" https://www.youtube.com/watch?v=VusKAosxxyk
    youtube-dl.exe -o "2.4 Classification.mp4" https://www.youtube.com/watch?v=vVj2itVNku4
    youtube-dl.exe -o "2.R Introduction to R.mp4" https://www.youtube.com/watch?v=jwBgGS_4RQA
    youtube-dl.exe -o "3.0 Interview with John Chambers.mp4" https://www.youtube.com/watch?v=jk9S3RTAl38
    youtube-dl.exe -o "3.1 Simple Linear Regression.mp4" https://www.youtube.com/watch?v=PsE9UqoWtS4
    youtube-dl.exe -o "3.2 Hypothesis Testing and Interval Confidence.mp4" https://www.youtube.com/watch?v=J6AdoiNUyWI
    youtube-dl.exe -o "3.3 Multiple Linear Regression.mp4" https://www.youtube.com/watch?v=1hbCJyM9ccs
    youtube-dl.exe -o "3.4 Some Important Questions.mp4" https://www.youtube.com/watch?v=3T6RXmIHbJ4
    youtube-dl.exe -o "3.5 Extensions of the linear models.mp4" https://www.youtube.com/watch?v=IFzVxLv0TKQ
    youtube-dl.exe -o "3.R Linear Regression in R.mp4" https://www.youtube.com/watch?v=5ONFqIk3RFg
    youtube-dl.exe -o "4.1 Introduction to Classification Problems.mp4" https://www.youtube.com/watch?v=sqq21-VIa1c
    youtube-dl.exe -o "4.2 Logistic Regression.mp4" https://www.youtube.com/watch?v=31Q5FGRnxt4
    youtube-dl.exe -o "4.3 Multivariate Logistic Regression.mp4" https://www.youtube.com/watch?v=MpX8rVv_u4E
    youtube-dl.exe -o "4.4 Logistic Regression - Case Control Sampling and Multiclass.mp4" https://www.youtube.com/watch?v=GavRXXEHGqU
    youtube-dl.exe -o "4.5 Discriminant Analysis.mp4" https://www.youtube.com/watch?v=RfrGiG1Hm3M
    youtube-dl.exe -o "4.6 Gaussian Discriminant Analysis - One Variable.mp4" https://www.youtube.com/watch?v=QG0pVJXT6EU
    youtube-dl.exe -o "4.7 Gaussian Discriminant Analysis - Many Variable.mp4" https://www.youtube.com/watch?v=X4VDZDp2vqw
    youtube-dl.exe -o "4.8 Quadratic Discriminant Analysis and Naive Bayes.mp4" https://www.youtube.com/watch?v=6FiNGTYAOAA
>
285:<Thank you! :)
The figure 5.1 on page 142, "The Elements of Statistical Learning" is useful too!>
286:<http://ch.linkedin.com/pub/<nameRedac_anon_screen_name_redacted>-<nameRedac_anon_screen_name_redacted>/3a/742/594?trk=pub-pbmap>
287:<You will need to download and install [Python][1] 2.6+ or 3.3+ and [youtube-dl][2].
Try out the different youtube-dl command line settings with one or two videos first. Then copy/paste MikeWilson's code into a bat/cmd file, adjust it to your liking and run it.

Nice work biplot and MikeWilson! It's tedious as hell to collect the individual YouTube file names from each video - I know!  :-)


  [1]: https://www.python.org/
  [2]: http://rg3.github.io/youtube-dl/>
288:<I'm sorry but I don't really understand your question... do you mean that you have always |data-predicted|=1? :) such a thing would be really nice!, isn't it?.
Most probably you've already solved your problem, but anyway... I think you should use glm with family=binomial (logit option) instead of lm which is used for linear regression. Once you fit the test data typing something like:
fit = predict(svmfit,test_dat), you'd need something like
fit01=ifelse(fit>0.5,1,0) due to logistic function gives you a probability instead of the response 0 or 1.

Cheers!>
289:<To Rob, Trevor and everyone else who made this course available, I'd like to say thank you!

It's definitely one of the more interesting courses I've taken. Thanks for introducing a variety of methods on statistical learning.  It's great to learn of the clever ways people came up with for solving problems.

Hope everyone who took the course enjoyed it as much as I did and hope everyone, especially myself ;D, will get a chance to apply the skills learned.>
290:<Try the followings:

dat=data.frame(x,y)

fit=lm(y~.,data=dat)>
291:<If you worry about the data structure, try the followings:

x=matrix(rnorm(1000),100,10)

y=rep(c(0,1),c(50,50))

x[y==1,1:5]=x[y==1,1:5]+1

dat=data.frame(x,y=as.factor(y))>
292:<I too want to thank Prof. Hastie and Prof. Tibshirani for this great course. This is really a great course. I really appreciate your effort. I am start reading the ESL2 book. It becomes much easy to understand after this course. It is really a good book. I'll keep a copy on my desk. Hopefully an advanced course will be provided later :) Thanks again.>
293:<Its explained nicely in the book on page 418.

    #variance
    pr.var =pca.out$sdev ^2
    #pct variance
    pve=pr.var/sum(pr.var )>
294:<# to calculate proportion of variance explained....from book page 418
    pr.var =pca.out$sdev ^2
    
    # variance
    pve=pr.var/sum(pr.var )>
295:<To Rob, Trevor, Daniela and everyone else who made this course available: thank you!!
It has been a pleasure to follow this course. And I think the book is excellent, much more accessible than Elements of Statistical Learning! (I immediately ordered 3 copies of the book, one for my daughter, one for my wife and one for myself:).
I particularly liked the R sessions by Trevor, as he showed many elegant tricks in R. I teach a course in basic Statistics with R for first year Informatics students, and I make all teaching materials with R Studio, markdown and some animations with Shiny. It is good to see that the experts use the same tools!

Thanks again, best regards, 

<nameRedac_anon_screen_name_redacted> <nameRedac_anon_screen_name_redacted> (Rotterdam, Netherlands)>
296:<Started late in the 4th week and had to rush through the material in order to catch up. Even so it would have been impossible to finish without the extension. Thank you very much for everything. I feel grateful for this fantastic course.
You are great teachers and persons!

Looking forward to see you soon in the followup course!:-)>
297:<Same For Me :)>
298:<The last lecture video (cluster analysis in R) currently has only 1370 views on YouTube--compared to over 16,000 views of the first video in the course.

Granted, some people will have downloaded it instead of streaming it directly. And others might've done the work without watching that particular video.

But still, I'd be surprised if more than few thousand of us made it to the end.

Which just makes us all the more awesome, right? :-)>
299:<I will violate the posting guidelines this one time, by not searching for posts on the same topic :) 
Thanks to the entire Statistical Learning crew and specially Trevor and Rob for a fantastic journey. I had some introduction to a few of the topics, but this course has definitely strengthened the foundation. Loved the camaraderie between the professors.  Considering the academic heavyweights that they are, they were amazingly easygoing. Very vibrant discussion forum (wish  I had the time to contribute more). Overall one of the best MOOCs I have done. Hoping for more from the professors on related topics.>
300:<:)>
301:<Great - all done! good luck guys :)>
302:<Now that the course is over. Here is the code for the exercise

    #10.R.2
    xxTest = rbind(x,x.test)
    pca.out=prcomp(xxTest, scale=TRUE)
    data.frame(pca.out$x[1:300,1:5],y=y)
    fit = lm(y~.,data.frame(pca.out$x[1:300,1:5],y=y))
    p = predict.lm(fit,data.frame(pca.out$x[301:1300,1:5]))
    mean((p-y.test)^2)

    #10.R.3
    dat=data.frame(x,y)
    fit=lm(y~.,data=dat)
    datTest = data.frame(x.test,y.test)
    pred = predict(fit,newdata=datTest, se.fit=TRUE)
    mean((pred$se.fit-y.test)^2)

I hope this helps. If you have any questions about how this works and I will answer just post.>
303:<For those of you wanting to download all the videos with a better resolution that the one provided in the "download video" functionality do the following:

1. Download youtube-dl http://rg3.github.io/youtube-dl/download.html

2. Create a list.txt file with the list of the videos urls. For this course all the videos are here below (copy/paste the list to the list.txt file without the '> ' prefix):


>     > https://www.youtube.com/watch?v=2wLfFB_6SKI
>     > https://www.youtube.com/watch?v=LvaTokhYnDw
>     > https://www.youtube.com/watch?v=WjyuiK5taS8
>     > https://www.youtube.com/watch?v=UvxHOkYQl8g
>     > https://www.youtube.com/watch?v=VusKAosxxyk
>     > https://www.youtube.com/watch?v=vVj2itVNku4
>     > https://www.youtube.com/watch?v=jwBgGS_4RQA
>     > https://www.youtube.com/watch?v=jk9S3RTAl38
>     > https://www.youtube.com/watch?v=PsE9UqoWtS4
>     > https://www.youtube.com/watch?v=J6AdoiNUyWI
>     > https://www.youtube.com/watch?v=1hbCJyM9ccs
>     > https://www.youtube.com/watch?v=3T6RXmIHbJ4
>     > https://www.youtube.com/watch?v=IFzVxLv0TKQ
>     > https://www.youtube.com/watch?v=5ONFqIk3RFg
>     > https://www.youtube.com/watch?v=sqq21-VIa1c
>     > https://www.youtube.com/watch?v=31Q5FGRnxt4
>     > https://www.youtube.com/watch?v=MpX8rVv_u4E
>     > https://www.youtube.com/watch?v=GavRXXEHGqU
>     > https://www.youtube.com/watch?v=RfrGiG1Hm3M
>     > https://www.youtube.com/watch?v=QG0pVJXT6EU
>     > https://www.youtube.com/watch?v=X4VDZDp2vqw
>     > https://www.youtube.com/watch?v=6FiNGTYAOAA
>     > https://www.youtube.com/watch?v=TxvEVc8YNlU
>     > https://www.youtube.com/watch?v=2cl7JiPzkBY
>     > https://www.youtube.com/watch?v=9TVVF7CS3F4
>     > https://www.youtube.com/watch?v=6l9V1sINzhE
>     > https://www.youtube.com/watch?v=_2ij6eaaSl0
>     > https://www.youtube.com/watch?v=nZAM5OXrktY
>     > https://www.youtube.com/watch?v=S06JpVoNaA0
>     > https://www.youtube.com/watch?v=p4BYWX7PTBM
>     > https://www.youtube.com/watch?v=BzHz0J9a6k0
>     > https://www.youtube.com/watch?v=6dSXlqHAoMk
>     > https://www.youtube.com/watch?v=YVSmsWoBKnA
>     > https://www.youtube.com/watch?v=MEMGOlJxxz0
>     > https://www.youtube.com/watch?v=91si52nk3LA
>     > https://www.youtube.com/watch?v=nLpJd_iKmrE
>     > https://www.youtube.com/watch?v=NJhMSpI2Uj8
>     > https://www.youtube.com/watch?v=LkifE44myLc
>     > https://www.youtube.com/watch?v=3p9JNaJCOb4
>     > https://www.youtube.com/watch?v=cSKzqb0EKS0
>     > https://ww>
304:<For those of you wanting to download all the videos with a better resolution that the one provided in the "download video" functionality do the following:

1. Download youtube-dl http://rg3.github.io/youtube-dl/download.html

2. Create a list.txt file with the list of the videos urls. For this course all the videos are here below (copy/paste the list to the list.txt file without the '> ' prefix):


>     > https://www.youtube.com/watch?v=2wLfFB_6SKI
>     > https://www.youtube.com/watch?v=LvaTokhYnDw
>     > https://www.youtube.com/watch?v=WjyuiK5taS8
>     > https://www.youtube.com/watch?v=UvxHOkYQl8g
>     > https://www.youtube.com/watch?v=VusKAosxxyk
>     > https://www.youtube.com/watch?v=vVj2itVNku4
>     > https://www.youtube.com/watch?v=jwBgGS_4RQA
>     > https://www.youtube.com/watch?v=jk9S3RTAl38
>     > https://www.youtube.com/watch?v=PsE9UqoWtS4
>     > https://www.youtube.com/watch?v=J6AdoiNUyWI
>     > https://www.youtube.com/watch?v=1hbCJyM9ccs
>     > https://www.youtube.com/watch?v=3T6RXmIHbJ4
>     > https://www.youtube.com/watch?v=IFzVxLv0TKQ
>     > https://www.youtube.com/watch?v=5ONFqIk3RFg
>     > https://www.youtube.com/watch?v=sqq21-VIa1c
>     > https://www.youtube.com/watch?v=31Q5FGRnxt4
>     > https://www.youtube.com/watch?v=MpX8rVv_u4E
>     > https://www.youtube.com/watch?v=GavRXXEHGqU
>     > https://www.youtube.com/watch?v=RfrGiG1Hm3M
>     > https://www.youtube.com/watch?v=QG0pVJXT6EU
>     > https://www.youtube.com/watch?v=X4VDZDp2vqw
>     > https://www.youtube.com/watch?v=6FiNGTYAOAA
>     > https://www.youtube.com/watch?v=TxvEVc8YNlU
>     > https://www.youtube.com/watch?v=2cl7JiPzkBY
>     > https://www.youtube.com/watch?v=9TVVF7CS3F4
>     > https://www.youtube.com/watch?v=6l9V1sINzhE
>     > https://www.youtube.com/watch?v=_2ij6eaaSl0
>     > https://www.youtube.com/watch?v=nZAM5OXrktY
>     > https://www.youtube.com/watch?v=S06JpVoNaA0
>     > https://www.youtube.com/watch?v=p4BYWX7PTBM
>     > https://www.youtube.com/watch?v=BzHz0J9a6k0
>     > https://www.youtube.com/watch?v=6dSXlqHAoMk
>     > https://www.youtube.com/watch?v=YVSmsWoBKnA
>     > https://www.youtube.com/watch?v=MEMGOlJxxz0
>     > https://www.youtube.com/watch?v=91si52nk3LA
>     > https://www.youtube.com/watch?v=nLpJd_iKmrE
>     > https://www.youtube.com/watch?v=NJhMSpI2Uj8
>     > https://www.youtube.com/watch?v=LkifE44myLc
>     > https://www.youtube.com/watch?v=3p9JNaJCOb4
>     > https://www.youtube.com/watch?v=cSKzqb0EKS0
>     > https://ww>
305:<Below is what I used; I relied heavily on the work of other students in the course!

Question 1: Default SVM

Based on work by DWaterson
(https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/<phoneRedac><zipRedac>2576f<phoneRedac>0)

Reference ISLR pages 359-362 with radial kernel substituted

    library(e1071)
    error = rep(0, 1000)
    for (i in 1:1000) {
         # Generate training data set with 50 observations from one 
         # class and 50 observations from the other
         x1 = matrix(rnorm(50*10), ncol=10)
         y1 = c(rep(0, 50))
  
         x2 = matrix(rnorm(50*10), ncol=10)
         x2[,1:5] = x2[,1:5] + 1
         y2 = c(rep(1,50))
  
         xdat1 = rbind(x1, x2)
         ydat1 = c(y1,y2)
  
         traindat = data.frame(x = xdat1, y = as.factor(ydat1))
         
         #Fit default support vector classifier
         svmfit = svm(y~., data=traindat, cost=1)
  
         #Generate test data set
         x1test = matrix(rnorm(50*10), ncol=10)
         y1test = c(rep(0, 50))
  
         x2test = matrix(rnorm(50*10), ncol=10)
         x2test[,1:5] = x2test[,1:5] + 1
         y2test = c(rep(1,50))
  
         xdattest = rbind(x1test,x2test)
         ydat1test = c(y1test,y2test)
  
         testdat = data.frame(x = xdattest, y = as.factor(ydat1test))

         # Predict the class labels of these test observations using
         # default svmfit and calculate the error 
         ypred=predict(svmfit, testdat)
         error[i] = sum(ypred != testdat$y)/100  
    }

    mean(error)

Question 2: SVM with a linear kernel (kernel = "linear")

    error = rep(0, 1000)
    for (i in 1:1000) {  
         # Generate training data set with 50 observations from one 
         # class and 50 observations from the other
         x1 = matrix(rnorm(50*10), ncol=10)
         y1 = c(rep(0, 50))

         x2 = matrix(rnorm(50*10), ncol=10)
         x2[,1:5] = x2[,1:5] + 1
         y2 = c(rep(1,50))
  
         xdat1 = rbind(x1, x2)
         ydat1 = c(y1,y2)
  
         traindat = data.frame(x = xdat1, y = as.factor(ydat1))
  
         # Fit linear support vector classifier
         svmfit = svm(y~., data=traindat, kernel="linear", cost=1)
  
         # Generate test data set
         x1test = matrix(rnorm(50*10), ncol=10)
         y1test = c(rep(0, 50))
 
         x2test = matrix(rnorm(50*10), ncol=10)
         x2test[,1:5] = x2test[>
306:<Thank you very much, Prof.Hastie, Prof.Tibshirani! 
I signed up in late March and just completed the whole course. I knew some basic algorithms before but never managed to link them together. Now I have a much clearer idea at what algorithms are doing and why they do what they do. Thank you for explaining the comprehensive context in a very clear and interconnected way! I really like the flow of the course,like putting bootstrap before tree models, and how graphs and simulated examples are used to facilitate understanding! 
Thank you again! It has been a fun and rewarding learning experience! 4 weekends well spent :)>
